<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.5 Nonparametric tests for two independent samples | index.knit</title>
  <meta name="description" content="MTH2006 Topic 5<br />
Nonparametric statistics</div>" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="5.5 Nonparametric tests for two independent samples | index.knit" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.5 Nonparametric tests for two independent samples | index.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="kde.html"/>
<link rel="next" href="permrand.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="assets/kePrint-0.0.1/kePrint.js"></script>
<link href="assets/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="5" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>5</b> Nonparametric statistics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="introduction.html"><a href="introduction.html#topic-webpage"><i class="fa fa-check"></i><b>5.1.1</b> Topic webpage</a></li>
<li class="chapter" data-level="5.1.2" data-path="introduction.html"><a href="introduction.html#proposed-learning-schedule"><i class="fa fa-check"></i><b>5.1.2</b> Proposed learning schedule</a></li>
<li class="chapter" data-level="5.1.3" data-path="introduction.html"><a href="introduction.html#office-hour"><i class="fa fa-check"></i><b>5.1.3</b> Office hour</a></li>
<li class="chapter" data-level="5.1.4" data-path="introduction.html"><a href="introduction.html#resources"><i class="fa fa-check"></i><b>5.1.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="pcdf.html"><a href="pcdf.html"><i class="fa fa-check"></i><b>5.2</b> Estimating cdfs and pdfs</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="pcdf.html"><a href="pcdf.html#teignmouth-rainfall"><i class="fa fa-check"></i><b>5.2.1</b> Teignmouth rainfall </a></li>
<li class="chapter" data-level="5.2.2" data-path="pcdf.html"><a href="pcdf.html#para"><i class="fa fa-check"></i><b>5.2.2</b> Parametric pdf estimation</a></li>
<li class="chapter" data-level="5.2.3" data-path="pcdf.html"><a href="pcdf.html#ord"><i class="fa fa-check"></i><b>5.2.3</b> Order statistics and ranks</a></li>
<li class="chapter" data-level="5.2.4" data-path="pcdf.html"><a href="pcdf.html#empirical-cdf-estimation"><i class="fa fa-check"></i><b>5.2.4</b> Empirical cdf estimation</a></li>
<li class="chapter" data-level="5.2.5" data-path="pcdf.html"><a href="pcdf.html#hist"><i class="fa fa-check"></i><b>5.2.5</b> Histograms</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="gof.html"><a href="gof.html"><i class="fa fa-check"></i><b>5.3</b> Goodness of fit</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="gof.html"><a href="gof.html#quantilequantile-plots"><i class="fa fa-check"></i><b>5.3.1</b> Quantile–quantile plots</a></li>
<li class="chapter" data-level="5.3.2" data-path="gof.html"><a href="gof.html#pearsons-chi-squared-test"><i class="fa fa-check"></i><b>5.3.2</b> Pearson’s chi-squared test</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="kde.html"><a href="kde.html"><i class="fa fa-check"></i><b>5.4</b> Kernel density estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="kde.html"><a href="kde.html#old-faithful-geyser-eruption-times"><i class="fa fa-check"></i><b>5.4.1</b> Old Faithful geyser eruption times</a></li>
<li class="chapter" data-level="5.4.2" data-path="kde.html"><a href="kde.html#kdedef"><i class="fa fa-check"></i><b>5.4.2</b> Definition</a></li>
<li class="chapter" data-level="5.4.3" data-path="kde.html"><a href="kde.html#kernel-functions"><i class="fa fa-check"></i><b>5.4.3</b> Kernel functions</a></li>
<li class="chapter" data-level="5.4.4" data-path="kde.html"><a href="kde.html#kernel-bandwidth"><i class="fa fa-check"></i><b>5.4.4</b> Kernel bandwidth</a></li>
<li class="chapter" data-level="5.4.5" data-path="kde.html"><a href="kde.html#kernel-density-estimation-in-r"><i class="fa fa-check"></i><b>5.4.5</b> Kernel density estimation in <code>R</code></a></li>
<li class="chapter" data-level="5.4.6" data-path="kde.html"><a href="kde.html#some-properties-of-hatf_hy"><i class="fa fa-check"></i><b>5.4.6</b> Some properties of <span class="math inline">\(\hat{f}_h(y)\)</span></a></li>
<li class="chapter" data-level="5.4.7" data-path="kde.html"><a href="kde.html#nkde"><i class="fa fa-check"></i><b>5.4.7</b> Optimal bandwidths by assuming normal <span class="math inline">\(f_Y(y)\)</span></a></li>
<li class="chapter" data-level="5.4.8" data-path="kde.html"><a href="kde.html#notnkde"><i class="fa fa-check"></i><b>5.4.8</b> Optimal bandwidths for any <span class="math inline">\(f_Y(y)\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="indepsamp.html"><a href="indepsamp.html"><i class="fa fa-check"></i><b>5.5</b> Nonparametric tests for two independent samples</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="indepsamp.html"><a href="indepsamp.html#welchs-t-test"><i class="fa fa-check"></i><b>5.5.1</b> Welch’s <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="5.5.2" data-path="indepsamp.html"><a href="indepsamp.html#kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>5.5.2</b> Kolmogorov-Smirnov test</a></li>
<li class="chapter" data-level="5.5.3" data-path="indepsamp.html"><a href="indepsamp.html#mann-whitney-u-test"><i class="fa fa-check"></i><b>5.5.3</b> Mann-Whitney <span class="math inline">\(U\)</span> test</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="permrand.html"><a href="permrand.html"><i class="fa fa-check"></i><b>5.6</b> Permutation and randomisation tests</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="permrand.html"><a href="permrand.html#permutation"><i class="fa fa-check"></i><b>5.6.1</b> Permutation tests</a></li>
<li class="chapter" data-level="5.6.2" data-path="permrand.html"><a href="permrand.html#randomisation-tests"><i class="fa fa-check"></i><b>5.6.2</b> Randomisation tests</a></li>
<li class="chapter" data-level="5.6.3" data-path="permrand.html"><a href="permrand.html#exspear"><i class="fa fa-check"></i><b>5.6.3</b> Pearson’s correlation coefficient example</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="depsamp.html"><a href="depsamp.html"><i class="fa fa-check"></i><b>5.7</b> Nonparametric measures and tests for two dependent samples</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="depsamp.html"><a href="depsamp.html#spear"><i class="fa fa-check"></i><b>5.7.1</b> Spearman’s rank correlation coefficient</a></li>
<li class="chapter" data-level="5.7.2" data-path="depsamp.html"><a href="depsamp.html#wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>5.7.2</b> Wilcoxon signed rank test</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5.8</b> Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">MTH2006 Topic 5<br />
Nonparametric statistics</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="indepsamp" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Nonparametric tests for two independent samples<a href="indepsamp.html#indepsamp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes we might want to compare one independent sample against another, or against something that we’ve specified, such as a distribution. For example, we collected heights of MTH2006 students at the start of the module, and we might want to test whether the heights of females are different from males. For such samples it is probably fair to assume that heights are independent both within and between groups, e.g. amongst females, and between males and females. There are various nonparametric tests that can sometimes improve our ability to make such comparisons.</p>
<div id="welchs-t-test" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Welch’s <span class="math inline">\(t\)</span>-test<a href="indepsamp.html#welchs-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <span class="math inline">\(t\)</span>-test comes in various forms, and is perhaps one of the most commonly used tests in statistics. For example, we encountered the one-sample <span class="math inline">\(t\)</span>-test with the normal linear model in Topic 3 when we considered whether a regression coefficient, denoted <span class="math inline">\(\beta_j\)</span>, was different from zero and performed a hypothesis test with null hypothesis <span class="math inline">\(H_0: \beta_j = 0\)</span>.</p>
<p>Welch’s <span class="math inline">\(t\)</span>-test extends this to two samples. Consider two random samples, <span class="math inline">\(X_1, \ldots, X_m\)</span> and <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, of size <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, respectively, such that <span class="math inline">\(X_i \sim N(\mu_X, \sigma_X^2)\)</span>, for <span class="math inline">\(i=1, \ldots, m\)</span> and <span class="math inline">\(Y_j \sim N(\mu_Y, \sigma_Y^2)\)</span>, for <span class="math inline">\(j=1, \ldots, n\)</span>, with independent <span class="math inline">\(X_i\)</span>s and <span class="math inline">\(Y_j\)</span>s. Then let <span class="math inline">\(\overline X = (X_1 + \ldots + X_m)/m\)</span> and <span class="math inline">\(\overline Y = (Y_1 + \ldots + Y_n)/n\)</span> and so <span class="math inline">\(\overline X \sim N(\mu_X, \sigma_X^2/m)\)</span> and <span class="math inline">\(\overline Y \sim N(\mu_Y, \sigma_Y^2/n)\)</span>. Hence <span class="math inline">\(\overline X - \overline Y \sim N(\mu_X - \mu_Y, \sigma_X^2 / m + \sigma_Y^2 / n)\)</span>.</p>
<p>Now consider the null hypothesis <span class="math inline">\(H_0:\mu_X = \mu_Y\)</span>. Our default alternative hypothesis is <span class="math inline">\(H_1:\mu_X \neq \mu_Y\)</span>, but sometimes we way have grounds for specifying <span class="math inline">\(H_1:\mu_X &lt; \mu_Y\)</span> or <span class="math inline">\(H_1:\mu_X &gt; \mu_Y\)</span>. For Welch’s <span class="math inline">\(t\)</span>-test we compare the <em>observed</em> test statistic</p>
<p><span class="math display">\[
t_\text{obs} = \dfrac{\overline x - \overline y}{s_{XY}}
\]</span></p>
<p>against Student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\nu\)</span> degrees of freedom, where</p>
<p><span class="math display">\[
s_{XY} = \sqrt{\dfrac{s_X^2}{m} + \dfrac{s_Y^2}{n}}, ~~\nu = s_{XY}^4 / \left(\dfrac{(s_X^2 / m)^2}{m - 1} + \dfrac{(s_Y^2 / n)^2}{n - 1}\right),
\]</span></p>
<p><span class="math inline">\(\overline x\)</span> and <span class="math inline">\(\overline y\)</span> are the usual sample means and <span class="math inline">\(s_X^2\)</span> and <span class="math inline">\(s_Y^2\)</span> are the usual sample variances. If <span class="math inline">\(t_\nu\)</span> denotes a Student-<span class="math inline">\(t\)</span> distributed random variable with <span class="math inline">\(\nu\)</span> degrees of freedom, then we reject <span class="math inline">\(H_0\)</span> in favour of two-sided <span class="math inline">\(H_1\)</span> if <span class="math inline">\(\text{Pr}(|t_\text{obs}| \leq |t_\nu|)\)</span> is small, e.g. below 0.05 if testing at the 5% significance level.</p>
<p>Note that <em>Student’s</em> two-sample <span class="math inline">\(t\)</span>-test refers to case when we also assume that <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span> are ‘similar’, e.g. with a ratio between 0.5 and 2. It is typically best to assume non-similar variance, which is <code>R</code>’s default.</p>
<p>The main point to note here is the various assumptions that have been made. First the <span class="math inline">\(X_i\)</span>s and <span class="math inline">\(Y_j\)</span>s, for <span class="math inline">\(i = 1, \ldots, m\)</span> and <span class="math inline">\(j = 1, \ldots, n\)</span>, are assumed normally distributed <em>and</em> the <span class="math inline">\(X_i\)</span>s are assumed to have the same variance, i.e. are homogeneous, and so too are the <span class="math inline">\(Y_j\)</span>s. If the <span class="math inline">\(X_i\)</span>s and <span class="math inline">\(Y_j\)</span>s are homogeneous, then for large <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> we can assume that <span class="math inline">\(\overline X\)</span> and <span class="math inline">\(\overline Y\)</span> are normally distributed by the central limit theorem, and hence Student’s <span class="math inline">\(t\)</span> sampling distribution results remain valid. However, if the <span class="math inline">\(X_i\)</span>s are not homogeneous, or the <span class="math inline">\(Y_j\)</span>s aren’t, or both aren’t, then the theoretical basis for Welch’s (or Student’s) two-sample <span class="math inline">\(t\)</span>-test is not valid.</p>
<p>By relying on assumptions of normality, we might brand Welch’s (or Student’s) two-sample <span class="math inline">\(t\)</span>-test as parametric. If we think these assumptions are not adequate, we might want to turn to more robust tests.
The following nonparametric tests give some options for comparing one sample against another, when the samples are independent of each other.</p>
</div>
<div id="kolmogorov-smirnov-test" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Kolmogorov-Smirnov test<a href="indepsamp.html#kolmogorov-smirnov-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One option for comparing one sample against another is the Kolmogorov-Smirnov (K-S) test<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. However, we’ll first consider the K-S test in its original form, which is to compare a sample against a known distribution.</p>
<div id="comparing-against-a-reference-distribution" class="section level4 hasAnchor" number="5.5.2.1">
<h4><span class="header-section-number">5.5.2.1</span> Comparing against a reference distribution<a href="indepsamp.html#comparing-against-a-reference-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consider an independent random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span> and some continuous reference cdf <span class="math inline">\(F_Y(y)\)</span>. Sometimes we might be interested in the hypotheses <span class="math display">\[\begin{align*} H_0: &amp;~ Y_1, \ldots, Y_n~\text{from}~F_Y(y),\\ H_1: &amp;~ Y_1, \ldots, Y_n~not\text{ from}~F_Y(y). \end{align*}\]</span> The K-S test lets us test this. The K-S test is motivated by formalising <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> above based on an edf estimate, <span class="math inline">\(\widehat F_n(y)\)</span>, so that <span class="math display">\[\begin{align*} H_0&amp;: \widehat F_n(y) = F_Y(y),\\
H_1&amp;: \widehat F_n(y) \neq F_Y(y). \end{align*}\]</span> Note that a composite <span class="math inline">\(H_1\)</span> is considered above, but the simple hypotheses <span class="math inline">\(H_1: \widehat F_n(y) &lt; F_Y(y)\)</span> or <span class="math inline">\(H_1: \widehat F_n(y) &gt; F_Y(y)\)</span> may also be considered, when appropriate.</p>
<p>It is important to note that here we are considering a <em>completely specified null</em> hypothesis. In particular, there are no parameters that we have to estimate, such as when we fitted a gamma distribution to rainfall amounts in Section <a href="pcdf.html#para">5.2.2</a>. So the K-S test is <em>not</em> a goodness-of-fit test for a fitted distribution if we’ve had to estimate parameters<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<p>The K-S test uses the test statistic <span class="math display">\[\begin{align*}D_n^\text{KS} &amp;= \sup_y |\widehat F_n(y) - F_Y(y)|,\\ &amp;=\max_{i = 1, \ldots, n}\{ \max(i/n - U_{(i)}, U_{(i)} - (i - 1)/n)\},\end{align*}\]</span> where <span class="math inline">\(U_{(1)} \leq U_{(2)} \leq \ldots \leq U_{(n)}\)</span> are the order statistics of <span class="math inline">\(\widehat F_n(Y_1), \ldots, \widehat F_n(Y_n)\)</span>. Put into words, the K-S test statistic takes the maximum absolute difference between <span class="math inline">\(F_Y(y)\)</span> and <span class="math inline">\(\widehat F_n(y)\)</span> over <span class="math inline">\(y\)</span>. Clearly if <span class="math inline">\(F_Y(y)\)</span> and <span class="math inline">\(\widehat F_n(y)\)</span> are different, we’ll expect the K-S test statistic to be large. For a given value of <span class="math inline">\(i\)</span>, for <span class="math inline">\(i = 1, \ldots, n\)</span>, the term <span class="math inline">\(\max(i/n - U_{(i)}, U_{(i)} - (i - 1)/n)\)</span> results from assuming a two-sided <span class="math inline">\(H_1\)</span>. Note that if we choose <span class="math inline">\(H_1: \widehat F_n(y) &lt; F_Y(y)\)</span> then we just use the first term, and so <span class="math inline">\(D_n^\text{KS} = \max_{i = 1, \ldots, n} (i/n - U_{(i)})\)</span>, and if we choose <span class="math inline">\(H_1: \widehat F_n(y) &gt; F_Y(y)\)</span> then we just use the second term, and so <span class="math inline">\(D_n^\text{KS} = \max_{i = 1, \ldots, n} (\{)U_{(i)} - (i - 1)/n))\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kolpdf"></span>
<img src="index_files/figure-html/kolpdf-1.png" alt="The pdf of the Kolmogorov distribution." width="480" />
<p class="caption">
Figure 5.20: The pdf of the Kolmogorov distribution.
</p>
</div>
<p>In order to implement a test based on <span class="math inline">\(D_n^\text{KS}\)</span> we need to know its sampling distribution. Kolmogorov’s theorem states that <span class="math inline">\(\sqrt{n} D_n \to K\)</span> as <span class="math inline">\(n \to \infty\)</span>, where <span class="math inline">\(K\)</span> denotes the Kolmogorov distribution; see Figure <a href="indepsamp.html#fig:kolpdf">5.20</a>.</p>
<p>In practice we will let <code>R</code>’s <code>ks.test()</code> function handle the sampling distribution for us. We’ll focus on three of its arguments: 1) <code>x</code>, which <code>R</code> states must be “a numeric vector of data values”; 2) <code>y</code>, which <code>R</code> states can be “either a numeric vector of data values, or a character string naming a cumulative distribution function or an actual cumulative distribution function such as <code>pnorm()</code>”; and 3) <code>alternative</code>, which specifies the form of <span class="math inline">\(H_1\)</span>, i.e. whether it’s two-sided, <code>"two.sided"</code> (the default) or one-sided where <code>"less"</code> means <span class="math inline">\(\widehat F_n(y) &lt; F_Y(y)\)</span> and <code>"greater"</code> means <span class="math inline">\(\widehat F_n(y) &gt; F_Y(y)\)</span>.</p>
<hr />
<p><strong>Example 7:</strong> Use the Kolmogorov–Smirnov test to assess whether the following independent sample of 15 values could have been drawn from a normal distribution with mean 6 and variance 5.</p>
<p><span class="math display">\[\begin{equation*}
\text{2.8, 2.9, 3.1, 3.8, 4.2, 4.5, 5.6, 5.8, 5.9, 6, 7.1, 7.4, 8.1, 9.1, 10.1.}
\end{equation*}\]</span></p>
<p>We are therefore testing <span class="math display">\[\begin{align*}
H_0: &amp;~ (y_1, \ldots, y_{15})~\text{from}~\Phi(y; 6, 5),\\
H_1: &amp;~ (y_1, \ldots, y_{15})~not\text{ from}~\Phi(y; 6, 5),
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\Phi(y; \mu, \sigma^2)\)</span> denotes the N<span class="math inline">\((\mu, \sigma^2)\)</span> cdf. The following performs this test in <code>R</code>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="indepsamp.html#cb52-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.8</span>, <span class="fl">2.9</span>, <span class="fl">3.1</span>, <span class="fl">3.8</span>, <span class="fl">4.2</span>, <span class="fl">4.5</span>, <span class="fl">5.6</span>, <span class="fl">5.8</span>, <span class="fl">5.9</span>, <span class="dv">6</span>, <span class="fl">7.1</span>, <span class="fl">7.4</span>, <span class="fl">8.1</span>, <span class="fl">9.1</span>, <span class="fl">10.1</span>)</span>
<span id="cb52-2"><a href="indepsamp.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ks.test</span>(y, <span class="cf">function</span>(z) <span class="fu">pnorm</span>(z, <span class="dv">6</span>, <span class="fu">sqrt</span>(<span class="dv">5</span>)))</span></code></pre></div>
<pre><code>## 
##  Exact one-sample Kolmogorov-Smirnov test
## 
## data:  y
## D = 0.16667, p-value = 0.7395
## alternative hypothesis: two-sided</code></pre>
<p>The output of <code>ks.test()</code> tells us that the observed test statistic, which we’ll denote by <span class="math inline">\(d_{n, \text{obs}}^\text{KS}\)</span>, is <span class="math inline">\(d_{n, \text{obs}}^\text{KS} =\)</span> 0.16667 and that the K-S test’s <span class="math inline">\(p\)</span>-value is 0.7395. Hence we have far too little evidence to reject <span class="math inline">\(H_0\)</span>. <span style="float:right;"> ⬛ </span> </p>
<hr />
<p>Alternatives to the K-S test are the Cramér-von Mises test, with test statistic</p>
<p><span class="math display">\[\begin{align*}
D_n^\text{CvM} &amp;= \int_{-\infty}^\infty [\widehat F_n(y) - F_Y(y)]^2 dF_Y(y)\\ &amp;= \dfrac{1}{12n^2} \sum_{i = 1}^n \left(U_{(j)} - \dfrac{2j - 1}{2n}\right)^2,
\end{align*}\]</span></p>
<p>and the Anderson-Darling test, with test statistic</p>
<p><span class="math display">\[\begin{align*}
D_n^\text{AD} &amp;= \int_{-\infty}^\infty \dfrac{[\widehat F_n(y) - F_Y(y)]^2}{F_Y(y)[1 - F_Y(y)]} dF_Y(y)\\ &amp;= -n - \sum_{i = 1}^n \dfrac{2j - 1}{n}\log(U_{(j)}[1 - U_{(n + 1 - j)}]).
\end{align*}\]</span></p>
<p>Note that, in comparison to the K–S test statistic, the Cramér-von Mises and Anderson-Darling test statistics put more weight on the tails, i.e. small or large values or <span class="math inline">\(y\)</span>. The Anderson-Darling test statistic also scales by the variance of <span class="math inline">\(\widehat F_n(y)\)</span>. The <code>cvm.test()</code> and <code>ad.test()</code> functions from the <code>goftest</code> package can be used in <code>R</code> to perform the Cramér-von Mises and Anderson-Darling tests, respectively.</p>
<p>Analogous results to theoretical distribution of <span class="math inline">\(D_n^\text{KS}\)</span> exist for the theoretical distributions of <span class="math inline">\(D_n^\text{CvM}\)</span> and <span class="math inline">\(D_n^\text{CvM}\)</span>, but we won’t go into them in MTH2006.</p>
</div>
<div id="comparing-against-another-sample" class="section level4 hasAnchor" number="5.5.2.2">
<h4><span class="header-section-number">5.5.2.2</span> Comparing against another sample<a href="indepsamp.html#comparing-against-another-sample" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A particularly useful application of the K-S test occurs when we take as our reference distribution another edf. Hence, now consider comparing the independent random sample <span class="math inline">\(X_1, \ldots, X_m\)</span> with edf <span class="math inline">\(\widehat F_{X, m}(z)\)</span> against the independent random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span> with edf <span class="math inline">\(\widehat F_{Y, n}(z)\)</span>. (Note that we’ve written these as functions of <span class="math inline">\(z\)</span>, but this is arbitrary.) We can compare these through the hypotheses <span class="math display">\[\begin{align*}
H_0: &amp;~ \widehat F_{X, m}(z) = \widehat F_{Y, n}(z),\\
H_1: &amp;~ \widehat F_{X, m}(z) \neq \widehat F_{Y, n}(z),
\end{align*}\]</span></p>
<p>where we’ve chosen <span class="math inline">\(H_1\)</span> to be two-sided, but can use the one-sided alternatives <span class="math inline">\(\widehat F_{X, m}(z) &lt; \widehat F_{Y, n}(z)\)</span> or <span class="math inline">\(\widehat F_{X, m}(z) &gt; \widehat F_{Y, n}(z)\)</span>, when appropriate. Put simply, the two-sample K-S test lets us test whether two samples are drawn from the same distribution.</p>
<p>The K-S test statistic for the two-sample case is given by</p>
<p><span class="math display">\[
D_{m,n}^\text{KS} = \sup_z|\widehat F_{X,m}(z) - \widehat F_{Y,n}(z)|.
\]</span></p>
<p>As with the one-sample case, the sampling distribution of <span class="math inline">\(D_{m,n}^\text{KS}\)</span> doesn’t have closed form, but can be approximated numerically, which is what <code>ks.test()</code> does. Implementing the two-sample K-S test in <code>R</code> simply requires that the two samples, <code>x</code> and <code>y</code>, say, are supplied with <code>ks.test(x, y)</code>, which performs a two-sided test. Specifying <code>ks.test(x, y, alternative = "greater")</code> would test <span class="math inline">\(H_1: \widehat F_{X, m}(z) &gt; \widehat F_{Y, n}(z)\)</span> and <code>ks.test(x, y, alternative = "less")</code> would test <span class="math inline">\(H_1: \widehat F_{X, m}(z) &lt; \widehat F_{Y, n}(z)\)</span>.</p>
<hr />
<p><strong>Example 8:</strong> When it rained in Teignmouth in 2019, did it rain more than in 1919?</p>
<p>Let <span class="math inline">\(X_1, \ldots, X_m\)</span> denote the daily rainfall amounts in Teignmouth in 1919 and let <span class="math inline">\(Y_1, \ldots, Y_n\)</span> denote the amounts in 2019. We first need to load the amounts, which we’ll store as <code>rain1919</code> and <code>rain2019</code>, respectively.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="indepsamp.html#cb54-1" aria-hidden="true" tabindex="-1"></a>teignmouth1919 <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;teignmouth1919.csv&quot;</span>)</span>
<span id="cb54-2"><a href="indepsamp.html#cb54-2" aria-hidden="true" tabindex="-1"></a>teignmouth2019 <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;teignmouth2019.csv&quot;</span>)</span>
<span id="cb54-3"><a href="indepsamp.html#cb54-3" aria-hidden="true" tabindex="-1"></a>rain1919 <span class="ot">&lt;-</span> <span class="fu">subset</span>(teignmouth1919, prcp <span class="sc">&gt;</span> <span class="dv">0</span>)<span class="sc">$</span>prcp</span>
<span id="cb54-4"><a href="indepsamp.html#cb54-4" aria-hidden="true" tabindex="-1"></a>rain2019 <span class="ot">&lt;-</span> <span class="fu">subset</span>(teignmouth2019, prcp <span class="sc">&gt;</span> <span class="dv">0</span>)<span class="sc">$</span>prcp</span></code></pre></div>
<p>Then we need to supply the data to <code>ks.test()</code>, remembering to correctly specify <span class="math inline">\(H_1\)</span>, and so we use <code>alternative = "less"</code> because we’re interested in <span class="math inline">\(H_1: \widehat F_{X, m}(z) &lt; \widehat F_{Y, n}(z)\)</span>.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="indepsamp.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ks.test</span>(rain1919, rain2019, <span class="at">alternative =</span> <span class="st">&quot;less&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in ks.test.default(rain1919, rain2019, alternative = &quot;less&quot;): p-value
## will be approximate in the presence of ties</code></pre>
<pre><code>## 
##  Asymptotic two-sample Kolmogorov-Smirnov test
## 
## data:  rain1919 and rain2019
## D^- = 0.13904, p-value = 0.03003
## alternative hypothesis: the CDF of x lies below that of y</code></pre>
<p>Before we draw conclusions from the K-S test, it’s worth noting the output that <code>R</code> has produced. Firstly <code>R</code> has drawn our attention to ties, which in this case corresponds to multiple occurrences of the same rainfall amount in a given year. Secondly <code>R</code> has called the test statistic <code>D^-</code> and stated that the alternative hypothesis is “<code>CDF of x lies below that of y</code>”. This is useful for ensuring that we’ve requested the right test, which we have.</p>
<p>We note that the observed test statistic is <span class="math inline">\(d_{m, n, \text{obs}}^{KS} =\)</span> 0.13904, which corresponds to a <span class="math inline">\(p\)</span>-value of 0.03003 and so we’d reject <span class="math inline">\(H_0\)</span> at the 5% significance level, that 1919 and 2019 rainfall amounts are the same, in favour <span class="math inline">\(H_1\)</span>, that amounts in 2019 are higher. However, we wouldn’t reject <span class="math inline">\(H_0\)</span> at the 1% significance level. So there is good evidence to favour <span class="math inline">\(H_1\)</span> over <span class="math inline">\(H_0\)</span>, but not overwhelming evidence. <span style="float:right;"> ⬛ </span> </p>
<hr />
</div>
</div>
<div id="mann-whitney-u-test" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Mann-Whitney <span class="math inline">\(U\)</span> test<a href="indepsamp.html#mann-whitney-u-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a random variable <span class="math inline">\(X\)</span> with cdf <span class="math inline">\(F_X(z)\)</span> and another random variable <span class="math inline">\(Y\)</span> with cdf <span class="math inline">\(F_Y(z)\)</span>, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent. Suppose that <span class="math inline">\(F_X(z)\)</span> and <span class="math inline">\(F_Y(z)\)</span> have similar shapes and scales. Typically we just assume that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have similar variance.</p>
<p>Suppose that we want to perform the hypothesis test</p>
<p><span class="math display">\[\begin{align*}
H_0: &amp; \text{ Pr}(X &gt; Y) = \text{Pr}(Y &gt; X),\\
H_1: &amp; \text{ Pr}(X &gt; Y) \neq \text{Pr}(Y &gt; X),
\end{align*}\]</span></p>
<p>thus performing a two-sided test of whether <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are similar, or whether <span class="math inline">\(Y\)</span> tends to be different from <span class="math inline">\(X\)</span>. The Mann-Whitney <span class="math inline">\(U\)</span> test might sometimes be described as testing whether <span class="math inline">\(X\)</span> is <em>stochastically</em> different from <span class="math inline">\(Y\)</span>. What this means is that, by considering that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random and how they are distributed, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> tend to take different values. We’ll discuss one-sided alternative hypotheses later. Let <span class="math inline">\(X_1, \ldots, X_m\)</span> be independent and each with cdf <span class="math inline">\(F_X(z)\)</span>, and let <span class="math inline">\(Y_1, \ldots, Y_n\)</span> also be independent and each with cdf <span class="math inline">\(F_Y(z)\)</span>. Because <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, these samples are independent of each other.</p>
<p>First let</p>
<p><span class="math display" id="eq:mann">\[\begin{equation}
U_X = \sum_{i = 1}^m \sum_{j = 1}^n S(X_i, Y_j),
\tag{5.11}
\end{equation}\]</span></p>
<p>where <span class="math display">\[S(X, Y) = \left\{\begin{array}{ll} 1 &amp; \text{if}~~ X &gt; Y,\\ 0.5 &amp; \text{if}~~ Y = X,\\ 0 &amp; \text{if}~~ X &lt; Y. \end{array}\right.\]</span> We have written <span class="math inline">\(U_X\)</span> in equation <a href="indepsamp.html#eq:mann">(5.11)</a> to make explicit that we’ve used <span class="math inline">\(S(X, Y) = 1\)</span> when <span class="math inline">\(X &gt; Y\)</span>. Large values of <span class="math inline">\(U_X\)</span> occur when <span class="math inline">\(X &gt; Y\)</span> and small values when <span class="math inline">\(X &lt; Y\)</span>. The test statistic for the Mann-Whitney <span class="math inline">\(U\)</span> test<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> is then
<span class="math display">\[\begin{equation}
U = \min(U_X, U_Y)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(U_Y = \sum_{i = 1}^m \sum_{j = 1}^n S(Y_j, X_i)\)</span>. If we then observe <span class="math inline">\(u_\text{obs}\)</span> our <span class="math inline">\(p\)</span>-value is <span class="math inline">\(2 \text{Pr}(U \leq u_\text{obs})\)</span>. However, after a bit of arithmetic<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> we find that <span class="math inline">\(U_X + U_Y = mn\)</span> and so we can obtain <span class="math inline">\(U_Y\)</span> simply as <span class="math inline">\(U_Y = mn - U_X\)</span>. We can use one-sided <span class="math inline">\(H_1\)</span>, when appropriate: for <span class="math inline">\(\text{Pr}(X &gt; Y) &gt; \text{Pr}(Y &gt; X)\)</span> we just use <span class="math inline">\(U_X\)</span> and calculate <span class="math inline">\(\text{Pr}(U_X \geq u_{X, \text{obs}})\)</span> and for <span class="math inline">\(\text{Pr}(X &gt; Y) &lt; \text{Pr}(Y &gt; X)\)</span> we just use <span class="math inline">\(U_Y\)</span> and calculate <span class="math inline">\(\text{Pr}(U_Y \geq u_{Y, \text{obs}})\)</span>.</p>
<p>The sampling distribution of the Mann-Whitney <span class="math inline">\(U\)</span> test statistic does not have closed form, as we saw for the K-S test statistic. Its sampling distribution can, however, be approximated very accurately in <code>R</code> with the function <code>pwilcox()</code>. If the samples <span class="math inline">\(x_1, \ldots, x_m\)</span> and <span class="math inline">\(y_1, \ldots, y_n\)</span> give test statistic <span class="math inline">\(u_\text{obs}\)</span>, then</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="indepsamp.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pwilcox</span>(u, m, n)</span></code></pre></div>
<p>evaluates <span class="math inline">\(\text{Pr}(U \leq u_\text{obs})\)</span>, if the test statistic is stored as <code>u</code> and the sample sizes stored as <code>m</code> and <code>n</code>. For large <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, the test statistic’s distribution is approximately normal with mean <span class="math inline">\(\mu = mn/2\)</span> and variance <span class="math inline">\(\sigma^2 = mn(m + n + 1) / 12\)</span>. Note that in Topic 5 we will consider sample sizes of 20 and above to be sufficiently large for the normal approximation to be okay to use. In the case of ties, a modification of <span class="math inline">\(\sigma^2\)</span> is used, which we won’t include here for brevity.</p>
<p>As with the K-S test, <code>R</code> will do all this for us, with the function <code>wilcox.test()</code>. For samples <code>x</code> and <code>y</code>, say, <code>wilcox.test(x, y)</code> performs the Mann-Whitney <span class="math inline">\(U\)</span> test with <span class="math inline">\(H_0\)</span> as above and the two-sided <span class="math inline">\(H_1\)</span> above. As with the K-S test, the argument <code>alternative</code> controls <span class="math inline">\(H_1\)</span>: <code>alternative = "greater"</code> implements <span class="math inline">\(H_1:\text{Pr}(X &gt; Y) &gt; \text{Pr}(Y &gt; X)\)</span> and <code>alternative = "less"</code> implements <span class="math inline">\(H_1:\text{Pr}(X &gt; Y) &lt; \text{Pr}(Y &gt; X)\)</span>.</p>
<hr />
<p><strong>Example 9:</strong> Red wood ants will forage in oak and sycamore trees. The amount of foraged material transported along the trunks of each, measured as biomass (mg) transported per ant per hour, was measured for 27 oak and 26 sycamore trees, which are loaded into <code>R</code> as <code>x</code> and <code>y</code> below.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="indepsamp.html#cb59-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">40.2</span>, <span class="fl">94.9</span>, <span class="fl">171.2</span>, <span class="fl">34.2</span>, <span class="fl">11.5</span>, <span class="fl">15.6</span>, <span class="fl">57.6</span>, <span class="fl">37.6</span>, <span class="fl">24.3</span>, <span class="fl">124.1</span>,</span>
<span id="cb59-2"><a href="indepsamp.html#cb59-2" aria-hidden="true" tabindex="-1"></a>  <span class="fl">9.8</span>, <span class="fl">114.2</span>, <span class="fl">40.8</span>, <span class="fl">58.2</span>, <span class="fl">16.4</span>, <span class="fl">15.1</span>, <span class="fl">16.3</span>, <span class="fl">38.1</span>, <span class="fl">13.5</span>, <span class="fl">33.2</span>, <span class="fl">105.6</span>,</span>
<span id="cb59-3"><a href="indepsamp.html#cb59-3" aria-hidden="true" tabindex="-1"></a>  <span class="fl">75.2</span>, <span class="fl">66.2</span>, <span class="dv">60</span>, <span class="fl">100.4</span>, <span class="dv">82</span>, <span class="fl">33.5</span>)</span>
<span id="cb59-4"><a href="indepsamp.html#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="indepsamp.html#cb59-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">70.5</span>, <span class="fl">11.2</span>, <span class="fl">14.4</span>, <span class="fl">27.6</span>, <span class="dv">51</span>, <span class="fl">17.7</span>, <span class="dv">9</span>, <span class="fl">18.9</span>, <span class="fl">70.4</span>, <span class="fl">88.7</span>, <span class="fl">23.4</span>,</span>
<span id="cb59-6"><a href="indepsamp.html#cb59-6" aria-hidden="true" tabindex="-1"></a>  <span class="fl">14.3</span>, <span class="fl">58.3</span>, <span class="fl">23.9</span>, <span class="fl">46.5</span>, <span class="fl">39.1</span>, <span class="fl">15.9</span>, <span class="fl">54.6</span>, <span class="fl">12.3</span>, <span class="dv">13</span>, <span class="fl">25.6</span>, <span class="dv">33</span>,</span>
<span id="cb59-7"><a href="indepsamp.html#cb59-7" aria-hidden="true" tabindex="-1"></a>  <span class="dv">40</span>, <span class="fl">65.8</span>, <span class="fl">28.7</span>, <span class="fl">32.5</span>)</span></code></pre></div>
<p>Use the Mann-Whitney <span class="math inline">\(U\)</span> test and a 5% significance level to assess whether the ants transport foraged material more efficiently along oak than sycamore trees. State your test hypotheses.</p>
<p>Let <span class="math inline">\(X\)</span> denote amounts foraged in oak trees and <span class="math inline">\(Y\)</span> in sycamore trees. Clearly more efficiently mean transporting more material. So our hypotheses are <span class="math inline">\(H_0: \text{Pr}(X &gt; Y) = \text{Pr}(Y &gt; X)\)</span> versus <span class="math inline">\(H_1: \text{Pr}(X &gt; Y) &gt; \text{Pr}(Y &gt; X)\)</span>. Let <span class="math inline">\(x_1, \ldots, x_m\)</span> denote the amounts transported in oak trees, with <span class="math inline">\(m = 27\)</span>, and <span class="math inline">\(y_1, \ldots, y_n\)</span> denote the amounts transported in sycamore trees, with <span class="math inline">\(n = 26\)</span>. As there are no ties, our observed Mann-Whitney <span class="math inline">\(U\)</span> test statistic is <span class="math inline">\(u_{X, \text{obs}} = \sum_{i = 1}^{27} \sum_{j = 1}^{26} I(x_i &gt; y_j) = 453\)</span>. Our sample sizes are sufficiently large that we can use a normal approximation to the test statistic’s sampling distribution, which is <span class="math inline">\(N(351.0, 56.20^2)\)</span>. We then compare <span class="math inline">\(z_\text{obs} = (453 - 351.0) / 56.20 = 1.815\)</span> against the standard normal distribution. As <span class="math inline">\(\text{Pr}(Z &gt; z_\text{obs}) \simeq 0.0348\)</span>, we reject <span class="math inline">\(H_0\)</span> at the 5% significance level, and conclude that the red ants forage more efficiently in oak than sycamore trees. <span style="float:right;"> ⬛ </span> </p>
<hr />
<p>The Mann-Whitney <span class="math inline">\(U\)</span> test statistic is rather pleasing for its simplicity. As a result it’s straightforward to construct a naïve function in <code>R</code>, which we’ll call <code>MannWhitney1()</code>, to calculate the test statistic.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="indepsamp.html#cb60-1" aria-hidden="true" tabindex="-1"></a>MannWhitney1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y) {</span>
<span id="cb60-2"><a href="indepsamp.html#cb60-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb60-3"><a href="indepsamp.html#cb60-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb60-4"><a href="indepsamp.html#cb60-4" aria-hidden="true" tabindex="-1"></a>U <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb60-5"><a href="indepsamp.html#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m) {</span>
<span id="cb60-6"><a href="indepsamp.html#cb60-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb60-7"><a href="indepsamp.html#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (x[i] <span class="sc">&gt;</span> y[j]) {</span>
<span id="cb60-8"><a href="indepsamp.html#cb60-8" aria-hidden="true" tabindex="-1"></a>      U <span class="ot">&lt;-</span> U <span class="sc">+</span> <span class="fl">1.0</span></span>
<span id="cb60-9"><a href="indepsamp.html#cb60-9" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb60-10"><a href="indepsamp.html#cb60-10" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> (x[i] <span class="sc">==</span> y[j]) {</span>
<span id="cb60-11"><a href="indepsamp.html#cb60-11" aria-hidden="true" tabindex="-1"></a>        U <span class="ot">&lt;-</span> U <span class="sc">+</span> <span class="fl">0.5</span></span>
<span id="cb60-12"><a href="indepsamp.html#cb60-12" aria-hidden="true" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb60-13"><a href="indepsamp.html#cb60-13" aria-hidden="true" tabindex="-1"></a>        U <span class="ot">&lt;-</span> U <span class="sc">+</span> <span class="fl">0.0</span></span>
<span id="cb60-14"><a href="indepsamp.html#cb60-14" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb60-15"><a href="indepsamp.html#cb60-15" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb60-16"><a href="indepsamp.html#cb60-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb60-17"><a href="indepsamp.html#cb60-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb60-18"><a href="indepsamp.html#cb60-18" aria-hidden="true" tabindex="-1"></a>U</span>
<span id="cb60-19"><a href="indepsamp.html#cb60-19" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The function <code>MannWhitney1()</code> is naïve because it could have been written with far fewer lines of code (and could actually be much quicker to evaluate too). The function <code>MannWhitney2()</code> below demonstrates this.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="indepsamp.html#cb61-1" aria-hidden="true" tabindex="-1"></a>MannWhitney2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y) {</span>
<span id="cb61-2"><a href="indepsamp.html#cb61-2" aria-hidden="true" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">outer</span>(x, y, <span class="st">&quot;-&quot;</span>)</span>
<span id="cb61-3"><a href="indepsamp.html#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(D <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">sum</span>(D <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb61-4"><a href="indepsamp.html#cb61-4" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><code>R</code>’s <code>outer()</code> function is remarkably useful: see its help file and the examples within for more details. Here we’ve used <code>outer()</code> to subtract every value of one vector from every value of another vector, which results in a matrix with numbers of rows and columns that are dictated by the lengths of the two vectors. Another clear difference between <code>MannWhitney2()</code> and <code>MannWhitney1()</code> is that the former is more concise because it uses fewer lines of code. This is useful to remember when programming, because more concise code is typically less prone to errors, or <em>bugs</em>.</p>
<p>We can clearly see that calculation of <span class="math inline">\(U\)</span> involves <span class="math inline">\(mn\)</span> comparisons of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_j\)</span>. For larger samples, this can soon become very costly. Fortunately, <span class="math inline">\(U_X\)</span> can be calculated differently and more efficiently. Now consider collecting <span class="math inline">\(X_1, \ldots, X_m\)</span> and <span class="math inline">\(Y_1, \ldots, Y_n\)</span> together in the <span class="math inline">\((m + n)\)</span>-vector <span class="math inline">\(\{X_1, \ldots, X_m, Y_1, \ldots, Y_n\}\)</span> with ranks <span class="math inline">\(\{R_1, \ldots, R_m, R_{m + 1}, \ldots, R_{m + n}\}\)</span>. We can also calculate <span class="math inline">\(U_X\)</span> for the Mann-Whitney <span class="math inline">\(U\)</span> statistic with</p>
<p><span class="math display">\[
U_X = \sum_{i = 1}^m R_i - \dfrac{m(m + 1)}{2}
\]</span></p>
<p>and here’s an <code>R</code> function, <code>MannWhitney3()</code>, that can perform the calculation.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="indepsamp.html#cb62-1" aria-hidden="true" tabindex="-1"></a>MannWhitney3 <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y) {</span>
<span id="cb62-2"><a href="indepsamp.html#cb62-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb62-3"><a href="indepsamp.html#cb62-3" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">rank</span>(<span class="fu">c</span>(x, y))</span>
<span id="cb62-4"><a href="indepsamp.html#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(R[<span class="dv">1</span><span class="sc">:</span>m]) <span class="sc">-</span> .<span class="dv">5</span> <span class="sc">*</span> m <span class="sc">*</span> (m <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb62-5"><a href="indepsamp.html#cb62-5" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Finally, we’ll check whether our three functions return the same value, which they should. To do this we’ll generate a sample of 20 values from the Gamma(5, 2) distribution, which we’ll call <code>x</code>, and another sample of 30 values from the Gamma(5, 1.5) distribution, which we’ll call <code>y</code>.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="indepsamp.html#cb63-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">20</span>, <span class="dv">5</span>, <span class="dv">2</span>)</span>
<span id="cb63-2"><a href="indepsamp.html#cb63-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">30</span>, <span class="dv">5</span>, <span class="fl">1.5</span>)</span>
<span id="cb63-3"><a href="indepsamp.html#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">MannWhitney1</span>(x, y), <span class="fu">MannWhitney2</span>(x, y), <span class="fu">MannWhitney3</span>(x, y))</span></code></pre></div>
<pre><code>## [1] 243 243 243</code></pre>
<p>Indeed, our three functions return the same value of the Mann-Whitney <span class="math inline">\(U\)</span> test statistic.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>Andrey Kolmogorov (1903–1987) made various contributions to probability theory during his career. But his mathematical ability started rather early: apparently he recognised that <span class="math inline">\(1=1^2\)</span>, <span class="math inline">\(1+3=2^2\)</span>, <span class="math inline">\(1+3+5=3^2\)</span>, <span class="math inline">\(\ldots\)</span> at just five years old!<a href="indepsamp.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Should you ever need to use the test statistics in this section to test goodness of fit of a parameter-based distribution, then this can be done through simulation. The Lillefor’s test (<span class="citation">Lilliefors (1967)</span>), for example, uses a modification to the sampling distribution when assuming a sample of data are normally distributed with unknown mean and variance. This test is available in <code>R</code> via the <code>lille.test()</code> function in the <code>nortest</code> package.}<a href="indepsamp.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>The Mann-Whitney <span class="math inline">\(U\)</span> test inherits <span class="math inline">\(U\)</span> in its name because its test statistic is typically denoted <span class="math inline">\(U\)</span>. The test has many aliases, in particular the Wilcoxon Rank-Sum test.<a href="indepsamp.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>You may want to prove that <span class="math inline">\(U_X + U_Y = mn\)</span> in your free time.<a href="indepsamp.html#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="kde.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="permrand.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": false,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
