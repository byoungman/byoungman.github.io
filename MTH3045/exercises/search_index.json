[["index.html", "MTH3045: Statistical Computing: Exercises 1 Chapter 1 exercises", " MTH3045: Statistical Computing: Exercises Dr. Ben Youngman b.youngman@exeter.ac.uk Laver 817; ext. 2314 16/01/2023 1 Chapter 1 exercises Generate a sample of \\(n = 20\\) \\(N(1, 3^2)\\) random variates, and without using mean(), var() or sd() write R functions to calculate the sample mean, \\(\\bar x\\), sample variance, \\(s^2\\), and sample standard deviation, \\(s\\), where \\[ \\bar x = \\dfrac{1}{n} \\sum_{i = 1}^n x_i \\text{ and } s^2 = \\dfrac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar x)^2. \\] Note than sum() may be used. Solution n &lt;- 20 y &lt;- rnorm(n, 1, 3) # mean mean2 &lt;- function(x) { # function to calculate mean of a vector # x is a vector # returns a scalar sum(x) / length(x) } mean2(y) ## [1] 0.8993296 # check it works all.equal(mean(y), mean2(y)) ## [1] TRUE # variance var2 &lt;- function(x) { # function to calculate variance of a vector # x is a vector # returns a scalar xbar &lt;- mean2(x) sum((x - xbar)^2) / (length(x) - 1) } var2(y) ## [1] 12.647 # check it works all.equal(var(y), var2(y)) ## [1] TRUE # standard deviation sd2 &lt;- function(x) { # function to calculate standard deviation of a vector # x is a vector # returns a scalar sqrt(var2(x)) } sd2(y) ## [1] 3.556263 # check it works all.equal(sd(y), sd2(y)) ## [1] TRUE Note that above for mean2() we’ve not used a for loop. A for loop can be used, but usually if on cna be avoided, then it should. Then for var2() we’ve re-used mean2() and for sd2() we’ve re-used var2(). It’s often a good idea to re-use functions. In this case, we break the function sd2() into various smaller functions, which can often be good practice, as whether the final function is correct can be assessed by whether the simpler functions it comprises are correct. Consider computing \\(\\text{Pr}(Z \\geq z) = 1 - \\Phi(z)\\) where \\(Z \\sim \\text{Normal}(0, 1)\\), or, for short, \\(Z \\sim N(0, 1)\\). For \\(z = 0, 0.5, 1, 1.5, 2, \\ldots\\) compute this in R in three different ways using the following three commands pnorm(z, lower.tail = FALSE) 1 - pnorm(z) pnorm(-z) and find the lowest value of \\(z\\) for which the three don’t give the same answer. Solution z &lt;- seq(0, 7, by = .5) cbind( z, pnorm(z, lower.tail = FALSE), 1 - pnorm(z), pnorm(-z) ) ## z ## [1,] 0.0 5.000000e-01 5.000000e-01 5.000000e-01 ## [2,] 0.5 3.085375e-01 3.085375e-01 3.085375e-01 ## [3,] 1.0 1.586553e-01 1.586553e-01 1.586553e-01 ## [4,] 1.5 6.680720e-02 6.680720e-02 6.680720e-02 ## [5,] 2.0 2.275013e-02 2.275013e-02 2.275013e-02 ## [6,] 2.5 6.209665e-03 6.209665e-03 6.209665e-03 ## [7,] 3.0 1.349898e-03 1.349898e-03 1.349898e-03 ## [8,] 3.5 2.326291e-04 2.326291e-04 2.326291e-04 ## [9,] 4.0 3.167124e-05 3.167124e-05 3.167124e-05 ## [10,] 4.5 3.397673e-06 3.397673e-06 3.397673e-06 ## [11,] 5.0 2.866516e-07 2.866516e-07 2.866516e-07 ## [12,] 5.5 1.898956e-08 1.898956e-08 1.898956e-08 ## [13,] 6.0 9.865876e-10 9.865877e-10 9.865876e-10 ## [14,] 6.5 4.016001e-11 4.015999e-11 4.016001e-11 ## [15,] 7.0 1.279813e-12 1.279865e-12 1.279813e-12 Above we see that for \\(z = 6.0\\), the second approximation to the standard normal tail probability doesn’t give the same answer as the other two approximations (although the discrepancy is tiny). The formula \\(\\text{Var}(Y) = \\text{E}(Y^2) - [\\text{E}(Y)]^2\\) is sometimes called the ‘short-cut’ variance formula, i.e. a short-cut for \\(\\text{Var}(Y) = \\text{E}[Y - \\text{E}(Y)]^2\\). Compare computing the biased version of \\(\\text{Var}(Y)\\) using the two formulae above for the samples y1 and y2 below. y1 &lt;- 1:10 y2 &lt;- y1 + 1e9 Solution We’ll start with a function to calculate the short-cut formula bvar1 &lt;- function(x) { # function to calculate short-cut variance # x is a vector # returns a scalar mean(x^2) - mean(x)^2 } and the we’ll write a function to calculate the other formula. bvar2 &lt;- function(x) { # function to calculate variance # x is a vector # returns a scalar mean((x - mean(x))^2) } For y1 we have bvar1(y1) ## [1] 8.25 bvar2(y1) ## [1] 8.25 and so both formulae give the same answer, but for y2 we have bvar1(y2) ## [1] 0 bvar2(y2) ## [1] 8.25 and see that they don’t. The short-cut formula is clearly wrong, bcause the variance of a sample doesn’t change if we add a constant, which is the only difference between y1 and y2. (We’ll learn about the cause of this in Chapter 2.) "],["chapter-2-exercises.html", "2 Chapter 2 exercises", " 2 Chapter 2 exercises Compute \\(\\pi + \\text{e}\\), where \\(\\pi = 3.1415927 \\times 10^0\\) and \\(\\text{e} = 2.7182818 \\times 10^0\\), using floating point addition in base 10, working to five decimal places. Solution As \\(\\pi\\) and \\(\\text{e}\\) have a common exponent, we sum their mantissas, i.e. \\[\\begin{align*} \\pi + \\text{e} &amp;= (3.14159 \\times 10^0) + (2.71828 \\times 10^0)\\\\ &amp;= 5.85987 \\times 10^0 \\end{align*}\\] Now compute \\(10^6\\pi + \\text{e}\\), using floating point addition in base 10, now working to seven decimal places. Solution We first need to put the numbers on a common exponent, which is that of \\(10^6 \\pi\\). \\[\\begin{align*} 10^6 \\pi &amp;= 3.1415927 \\times 10^6\\\\ \\text{e} &amp; = 2.7182818 \\times 10^0 = 0.0000027 \\times 10^6. \\end{align*}\\] Then summing their mantissas gives \\[\\begin{align*} 10^6 \\pi + \\text{e} &amp;= (3.1415927 \\times 10^6) + (2.7182818 \\times 10^0)\\\\ &amp;= (3.1415927 \\times 10^6) + (0.0000027 \\times 10^6)\\\\ &amp;= (3.1415927 + 0.0000027) \\times 10^6\\\\ &amp;= 3.1415954 \\times 10^6. \\end{align*}\\] What would happen if we computed \\(10^6\\pi + \\text{e}\\) using a base 10 floating point representation, but only worked with five decimal places? Solution If we put \\(2.7182818 \\times 10^0\\) onto the exponent \\(10^6\\) we get \\(0.00000 \\times 10^6\\) to five decimal places, and so \\(\\text{e}\\) becomes negligible alongside \\(10^6 \\pi\\) if we only work to five decimal places. Write \\(2\\pi\\) in binary form using single- and double precision arithmetic. [Hint: I recommend you consider the binary forms for \\(\\pi\\) given in the lecture notes, and you might also use bit2decimal() from the lecture notes to check your answer.] Solution The key here is to note that we want to calculate \\(\\pi \\times 2\\). As we have a number in the form \\(S \\times (1 + F) \\times 2^{E - e}\\), then we just need to raise \\(E\\) by one. For both single- and double-precision, this corresponds to changing the last zero in the 0s and 1s for the \\(E\\) term to a one. bit2decimal &lt;- function(x, e, dp = 20) { # function to convert bits to decimal form # x: the bits as a character string, with appropriate spaces # e: the excess # dp: the decimal places to report the answer to bl &lt;- strsplit(x, &#39; &#39;)[[1]] # split x into S, E and F components by spaces # and then into a list of three character vectors, each element one bit bl &lt;- lapply(bl, function(z) as.integer(strsplit(z, &#39;&#39;)[[1]])) names(bl) &lt;- c(&#39;S&#39;, &#39;E&#39;, &#39;F&#39;) # give names, to simplify next few lines S &lt;- (-1)^bl$S # calculate sign, S E &lt;- sum(bl$E * 2^c((length(bl$E) - 1):0)) # ditto for exponent, E F &lt;- sum(bl$F * 2^(-c(1:length(bl$F)))) # and ditto to fraction, F z &lt;- S * 2^(E - e) * (1 + F) # calculate z out &lt;- format(z, nsmall = dp) # use format() for specific dp # add (S, E, F) as attributes, for reference attr(out, &#39;(S,E,F)&#39;) &lt;- c(S = S, E = E, F = F) out } bit2decimal(&#39;0 10000001 10010010000111111011011&#39;, 127) ## [1] &quot;6.28318548202514648438&quot; ## attr(,&quot;(S,E,F)&quot;) ## S E F ## 1.0000000 129.0000000 0.5707964 bit2decimal(&#39;0 10000000001 1001001000011111101101010100010001000010110100011000&#39;, 1023) ## [1] &quot;6.28318530717958623200&quot; ## attr(,&quot;(S,E,F)&quot;) ## S E F ## 1.0000000 1025.0000000 0.5707963 Find the calculation error in R of \\(b - a\\) where \\(a = 10^{16}\\) and \\(b = 10^{16} + \\exp(0.5)\\). Solution a &lt;- 1e16 b &lt;- 1e16 + exp(.5) b - a ## [1] 2 Create the following: The row vector \\[ \\mathbf{a} = (2, 4, 6), \\] the \\(2 \\times 3\\) matrix \\[ \\mathbf{B} = \\left(\\begin{array}{ccc} 6 &amp; 5 &amp; 4\\\\ 3 &amp; 2 &amp; 1\\end{array}\\right) \\] and a list with first element \\(\\mathbf{a}\\) and second element \\(\\mathbf{B}\\), and an arbitrary (i.e. with whatever values you like) \\(5 \\times 3 \\times 2\\) array with a 'name' attribute that is 'array1'. \\(~\\) For each of the above, consider whether your code could be simpler. Solution a &lt;- t(c(2, 4, 6)) B &lt;- matrix(6:1, 2, byrow = TRUE) l &lt;- list(a, B) arr &lt;- array(rnorm(30), c(5, 3, 2)) attr(arr, &#39;name&#39;) &lt;- &#39;array1&#39; arr ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1.3090346 0.4943840 1.7484539 ## [2,] -1.0965115 0.1440487 -0.3423153 ## [3,] 0.6174184 0.9012992 0.3828870 ## [4,] -0.1678039 2.4260803 2.0760905 ## [5,] -2.3372520 0.1625087 0.2833898 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 0.7658111 -0.09688811 0.66431510 ## [2,] -0.7087925 -1.53558029 -0.34731395 ## [3,] -2.0978903 0.76281721 0.03672637 ## [4,] -0.2184404 -0.26423410 0.04565322 ## [5,] 0.1685809 0.56425057 0.88277290 ## ## attr(,&quot;name&quot;) ## [1] &quot;array1&quot; Produce a \\(3 \\times 4 \\times 4 \\times 2\\) array containing Uniform(0, 1) random variates and compute the mean over its second and third margins using apply(..., ..., means) and rowMeans() or colMeans(). Solution arr &lt;- array(NA, c(3, 4, 4, 2)) arr[] &lt;- runif(prod(dim(arr))) # saves names to get the number right apply(arr, 2:3, mean) ## [,1] [,2] [,3] [,4] ## [1,] 0.6001091 0.5317558 0.4624507 0.3457407 ## [2,] 0.4811870 0.5363801 0.3439255 0.2724603 ## [3,] 0.3066860 0.3744666 0.5088300 0.5389947 ## [4,] 0.3202585 0.8405218 0.2949815 0.4342804 rowMeans(aperm(arr, c(2, 3, 1, 4)), dims = 2) ## [,1] [,2] [,3] [,4] ## [1,] 0.6001091 0.5317558 0.4624507 0.3457407 ## [2,] 0.4811870 0.5363801 0.3439255 0.2724603 ## [3,] 0.3066860 0.3744666 0.5088300 0.5389947 ## [4,] 0.3202585 0.8405218 0.2949815 0.4342804 colMeans(aperm(arr, c(1, 4, 2, 3)), dims = 2) ## [,1] [,2] [,3] [,4] ## [1,] 0.6001091 0.5317558 0.4624507 0.3457407 ## [2,] 0.4811870 0.5363801 0.3439255 0.2724603 ## [3,] 0.3066860 0.3744666 0.5088300 0.5389947 ## [4,] 0.3202585 0.8405218 0.2949815 0.4342804 Create a 3-element list of vectors comprising Uniform(0, 1) variates of length 3, 5 and 7, respectively. Solution lst &lt;- list(runif(3), runif(5), runif(7)) Create another list in which the vectors above are sorted into descending order. Solution lst2 &lt;- lapply(lst, sort, decreasing = TRUE) Then create a vector comprising the minimum of each vector in the list, and another stating which element is the minimum. [Hint: for the latter you may want to consult the ‘See Also’ part of the min() function’s help file.] Solution sapply(lst, min) ## [1] 0.1165352 0.3616373 0.0386982 sapply(lst, which.min) ## [1] 3 5 3 Use a for() loop to produce the following. [Hint: the function paste() might be useful.] ## [1] &quot;iteration 1&quot; ## [1] &quot;iteration 2&quot; ## [1] &quot;iteration 3&quot; ## [1] &quot;iteration 4&quot; ## [1] &quot;iteration 5&quot; ## [1] &quot;iteration 6&quot; ## [1] &quot;iteration 7&quot; ## [1] &quot;iteration 8&quot; ## [1] &quot;iteration 9&quot; ## [1] &quot;iteration 10&quot; Solution for (i in 1:10) print(paste(&#39;iteration&#39;, i)) ## [1] &quot;iteration 1&quot; ## [1] &quot;iteration 2&quot; ## [1] &quot;iteration 3&quot; ## [1] &quot;iteration 4&quot; ## [1] &quot;iteration 5&quot; ## [1] &quot;iteration 6&quot; ## [1] &quot;iteration 7&quot; ## [1] &quot;iteration 8&quot; ## [1] &quot;iteration 9&quot; ## [1] &quot;iteration 10&quot; Consider the following two infinite series that represent \\(\\pi\\).\\[\\begin{align*} \\pi &amp;= 4 \\left[1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\frac{1}{9} - \\frac{1}{11} + \\frac{1}{13} -\\cdots\\right]\\\\ \\pi &amp;= 3 + \\frac{4}{2 \\times 3 \\times 4} - \\frac{4}{4 \\times 5 \\times 6} + \\frac{4}{6 \\times 7 \\times 8} - \\frac{4}{8 \\times 9 \\times 10} + \\cdots \\end{align*}\\] Use a while() loop to find which converges to pi in R to within \\(\\epsilon_m^{1/3}\\) using the fewest terms, where \\(\\epsilon_m\\) is R’s machine tolerance. Solution Here’s the first approximation… quarter_pi &lt;- 1 terms1 &lt;- 1 denom &lt;- 3 multiplier &lt;- -1 my_pi &lt;- 4 * quarter_pi while(abs(my_pi - pi) &gt; .Machine$double.eps^(1/3)) { terms1 &lt;- terms1 + 1 quarter_pi &lt;- quarter_pi + multiplier / denom my_pi &lt;- 4 * quarter_pi denom &lt;- denom + 2 multiplier &lt;- -1 * multiplier } terms1 ## [1] 165141 …and here’s the second approximation… my_pi &lt;- 3 terms2 &lt;- 2 denoms &lt;- c(2, 3, 4) multiplier &lt;- 1 while(abs(my_pi - pi) &gt; .Machine$double.eps^(1/3)) { my_pi &lt;- my_pi + multiplier * 4 / prod(denoms) denoms &lt;- denoms + 2 multiplier &lt;- -1 * multiplier terms2 &lt;- terms2 + 1 } terms2 ## [1] 36 Write a function based on a for() loop to calculate the cumulative sum of a vector, \\(\\bf y\\), i.e. so that its \\(i\\)th value, \\(y_i\\) say, is \\[y_i = \\sum_{j = 1}^i x_j.\\] Solution Either of the following two functions are options (although others exist). my_cumsum &lt;- function(x) { # function 1 to calculate cumulative sum of a vector # x is a vector # returns a vector of length length(x) out &lt;- numeric(length(x)) for (i in 1:length(x)) { out[i] &lt;- sum(x[1:i]) } out } my_cumsum2 &lt;- function(x) { # function 2 to calculate cumulative sum of a vector # x is a vector # returns a vector of length length(x) out &lt;- x for (i in 2:length(x)) { out[i] &lt;- out[i] + out[i - 1] } out } We see that both perform the same calculation. x &lt;- runif(10) cumsum(x) ## [1] 0.8929304 1.0321478 1.2746827 1.6546394 1.9798944 2.6502328 2.9667531 ## [8] 3.7727990 3.8675240 3.9051414 my_cumsum(x) ## [1] 0.8929304 1.0321478 1.2746827 1.6546394 1.9798944 2.6502328 2.9667531 ## [8] 3.7727990 3.8675240 3.9051414 my_cumsum2(x) ## [1] 0.8929304 1.0321478 1.2746827 1.6546394 1.9798944 2.6502328 2.9667531 ## [8] 3.7727990 3.8675240 3.9051414 Then benchmark its execution time against R’s vectorised function cumsum() for summing 1000 iid \\(Uniform[0, 1]\\) random variables. Solution x &lt;- runif(1e3) microbenchmark::microbenchmark( cumsum(x), my_cumsum(x), my_cumsum2(x) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## cumsum(x) 771 928.0 1740.15 1280.5 2287.5 5778 100 ## my_cumsum(x) 1789364 1894123.5 2519396.85 1947327.5 2102667.0 11705561 100 ## my_cumsum2(x) 60054 61569.5 64469.08 63538.5 65305.0 81365 100 To start a board game, a player must throw three sixes using a conventional die. Write a function to simulate this, which returns the total number of throws that the player has taken. Solution sixes1 &lt;- function() { # function to simulate number of throws needed # to reach three sixes # n is an integer # returns total number of throws taken out &lt;- sample(1:6, 3, replace = TRUE) while(sum(out == 6) &lt; 3) { out &lt;- c(out, sample(1:6, 1)) } return(length(out)) } Then use 1000 simulations to empirically estimate the distribution of the number of throws needed. Solution samp1 &lt;- replicate(1e3, sixes1()) table(samp1) ## samp1 ## 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## 7 8 17 24 27 41 48 48 50 46 52 51 40 49 41 49 44 38 33 24 32 24 18 30 18 16 ## 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 55 57 60 ## 15 10 6 13 8 2 5 3 10 4 5 9 8 3 2 4 5 1 2 2 1 1 2 1 1 1 ## 64 ## 1 hist(samp1, xlab = &#39;Number of throws&#39;, main = &#39;Histogram of number of throws&#39;) Figure 2.1: Histogram of empirical distribution of number of throws for starting method 1. I think you’ll agree that this would be a rather dull board game! It is therefore proposed that a player should instead throw two consecutive sixes. Write a function to simulate this new criterion, and estimate its distribution empirically. Solution sixes2 &lt;- function() { # function to simulate number of throws needed # for two consecutive sixes # n is an integer # returns total number of throws taken out &lt;- sample(1:6, 2, replace = TRUE) cond &lt;- TRUE while(cond) { if (sum(out[1:(length(out) - 1)] + out[2:length(out)] == 12) &gt; 0) { cond &lt;- FALSE } else { out &lt;- c(out, sample(1:6, 1)) } } return(length(out)) } sixes2() ## [1] 63 Then use 1000 simulations to empirically estimate the distribution of the number of throws needed. Solution samp2 &lt;- replicate(1e3, sixes2()) table(samp2) ## samp2 ## 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## 20 22 21 26 17 25 27 29 16 15 15 27 18 22 14 18 17 14 14 13 ## 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## 11 10 14 12 14 7 13 11 11 13 12 15 19 8 12 10 11 7 10 10 ## 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 ## 7 13 8 7 6 9 6 5 7 5 7 6 6 9 8 9 7 8 7 9 ## 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ## 4 9 4 7 3 3 5 2 5 8 3 2 4 7 2 5 5 3 6 7 ## 82 83 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 102 103 ## 2 7 3 2 5 2 4 4 2 3 5 1 4 2 1 2 1 3 1 1 ## 104 106 107 108 109 110 111 112 113 115 116 117 119 120 121 122 123 124 125 126 ## 1 2 2 1 6 1 1 1 3 2 3 1 1 3 1 3 2 2 1 1 ## 127 129 131 132 133 134 138 140 141 143 144 146 147 148 151 153 154 157 160 162 ## 2 4 1 1 1 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 ## 163 168 175 193 200 202 211 212 215 218 222 228 253 270 276 ## 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 hist(samp2, xlab = &#39;Number of throws&#39;, main = &#39;Histogram of number of throws&#39;) Figure 2.2: Histogram of empirical distribution of number of throws for starting method 2. By comparing sample mean starting numbers of throws, which starting criterion should get a player into the game quickest? Solution mean(samp1) - mean(samp2) ## [1] -24.378 So the second approach, by comparing means, takes more throws before the game can begin. Consider the following two functions for calculating \\[d_i = x_{i + 1} - x_i, \\hspace{2cm} i = 1, \\ldots, n - 1\\] where \\({\\bf x}&#39; = (x_1, \\ldots, x_n)\\). diff1 &lt;- function(x) { # function to calculate differences of a vector # based on a for loop # x is a vector # returns a vector of length (length(x) - 1) out &lt;- numeric(length(x) - 1) for (i in 1:(length(x) - 1)) { out[i] &lt;- x[i + 1] - x[i] } out } diff2 &lt;- function(x) { # function to calculate differences of a vector # based on vectorisation # x is a vector # returns a vector of length (length(x) - 1) id &lt;- 1:(length(x) - 1) x[id + 1] - x[id] } The first, diff1() uses a straightforward for() loop to calculate \\(d_i\\), for \\(i = 1, \\ldots, n - 1\\), whereas diff2() could be seen to be a vectorised alternative. Benchmark the two for a vector of \\(n = 1000\\) iid \\(N(0, 1)\\) random variates by comparing the median difference in execution time. Solution x &lt;- rnorm(1000) microbenchmark::microbenchmark( diff1(x), diff2(x) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## diff1(x) 59.424 60.128 101.21769 60.508 61.7205 4077.783 100 ## diff2(x) 8.360 8.781 32.62806 8.931 9.2080 2350.589 100 The following function assesses whether all elements is a logical vector are TRUE. all2 &lt;- function(x) { # function to calculate whether all elements are TRUE # returns a scalar # x is a logical vector sum(x) == length(x) } Calculate the following and benchmark all2() against R’s built-in function all(), which does the same. n &lt;- 1e4 x1 &lt;- !logical(n) Solution all2(x1) ## [1] TRUE all(x1) ## [1] TRUE microbenchmark::microbenchmark( all2(x1), all(x1) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## all2(x1) 4.781 4.888 19.21602 4.9450 5.011 1389.284 100 ## all(x1) 10.867 10.898 11.66841 10.9245 12.995 19.266 100 We see that both take a similar amount of time. Now swap the first element of x1 so that it’s FALSE and repeat the benchmarking. Solution x1[1] &lt;- FALSE all2(x1) ## [1] FALSE all(x1) ## [1] FALSE microbenchmark::microbenchmark( all2(x1), all(x1) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## all2(x1) 4720 4943 5243.76 5069.0 5125.5 20510 100 ## all(x1) 119 133 178.02 183.5 198.5 429 100 Now all2() is much slower. This is because it’s performed a calculation on the entire x1 vector, whereas all() has stopped as soon as it’s found a FALSE. Evaluate the function any2() below against R’s built-in function any() similarly. any2 &lt;- function(x) { # function to calculate whether any elements are TRUE # returns a scalar # x is a logical vector sum(x) &gt; 0 } Solution x2 &lt;- logical(n) any2(x2) ## [1] FALSE any(x2) ## [1] FALSE microbenchmark::microbenchmark( any2(x2), any(x2) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## any2(x2) 4.728 4.847 17.15918 4.9370 5.0715 1209.045 100 ## any(x2) 12.981 13.029 13.52635 13.5615 13.6160 23.506 100 We again see that both take a similar amount of time. Now swap the first element of x2 so that it’s FALSE and repeat the benchmarking. Solution x2[1] &lt;- TRUE microbenchmark::microbenchmark( any2(x2), any(x2) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## any2(x2) 4660 4950.5 5208.08 5032 5102.0 15234 100 ## any(x2) 115 130.0 207.86 199 209.5 1799 100 This time any2() is much slower, for similar reasoning to all2() being much slower than all(), except that any() is stopped when it reaches a TRUE. "],["chapter-3-exercises.html", "3 Chapter 3 exercises", " 3 Chapter 3 exercises Calculate \\(\\mathbf{A}^{\\text{T}}\\mathbf{B}\\) in R where \\(\\mathbf{A}\\) is a \\(n \\times p\\) matrix comprising N(0, 1) random variates and \\(\\mathbf{B}\\) is a \\(n \\times n\\) matrix comprising Uniform([0, 1]) random variates for \\(n = 1000\\) and \\(p = 500\\), using t(A) %*% B and crossprod(A, B). Confirm that both produce the same result. Solution A &lt;- matrix(rnorm(n * p), n) B &lt;- matrix(runif(n * n), n) all.equal(t(A) %*% B, crossprod(A, B)) ## [1] TRUE Then benchmark the time that two commands take to complete. Solution microbenchmark::microbenchmark( t(A) %*% B, crossprod(A, B) ) ## Unit: milliseconds ## expr min lq mean median uq max neval ## t(A) %*% B 5.855102 6.964834 11.285696 8.680233 14.77136 34.25668 100 ## crossprod(A, B) 3.473000 3.582057 6.919724 4.579746 10.91639 35.73526 100 Consider the calculation \\(\\mathbf{AD}\\) where \\[ \\mathbf{A} = \\begin{pmatrix}1.92&amp;0.63&amp;0.64&amp;-2.34 \\\\0.81&amp;-1.60&amp;-1.07&amp;1.51 \\\\0.71&amp;1.33&amp;1.43&amp;-0.82 \\\\-0.45&amp;0.17&amp;-0.57&amp;0.06 \\\\2.09&amp;1.19&amp;-1.78&amp;-0.63 \\\\-0.42&amp;0.73&amp;-1.43&amp;1.64 \\\\\\end{pmatrix} \\text{ and }\\mathbf{D} = \\begin{pmatrix}0.87&amp;0.00&amp;0.00&amp;0.00 \\\\0.00&amp;0.08&amp;0.00&amp;0.00 \\\\0.00&amp;0.00&amp;-0.78&amp;0.00 \\\\0.00&amp;0.00&amp;0.00&amp;1.65 \\\\\\end{pmatrix}. \\] Write a function in R that takes a matrix A and a vector d as its arguments and computes \\(\\mathbf{AD}\\), where \\(\\mathbf{A} =\\) A and \\(\\text{diag}(\\mathbf{D}) =\\) d, and where \\(\\text{diag}(\\mathbf{D})\\) denotes the vector comprising the diagonal elements of \\(\\mathbf{D}\\). Consider whether your function is performing redundant calculations and, if it is, try and avoid them. Solution We might start with AD &lt;- function(A, d) { # function to compute A * D # A is a matrix # D is a matrix with diagonal elements d # returns a matrix D &lt;- diag(d) A %*% D } but if we do this then we’re multiplying a lot of zeros unnecessarily. Instead, the following avoids this AdiagD &lt;- function(A, d) { # function to compute A * D slightly more efficiently # A is a matrix # D is a matrix with diagonal elements d # returns a matrix t(t(A) * d) } and the following confirms that both give the same result AD(A, diag(D)) ## [,1] [,2] [,3] [,4] ## [1,] 1.6704 0.0504 -0.4992 -3.8610 ## [2,] 0.7047 -0.1280 0.8346 2.4915 ## [3,] 0.6177 0.1064 -1.1154 -1.3530 ## [4,] -0.3915 0.0136 0.4446 0.0990 ## [5,] 1.8183 0.0952 1.3884 -1.0395 ## [6,] -0.3654 0.0584 1.1154 2.7060 AdiagD(A, diag(D)) ## [,1] [,2] [,3] [,4] ## [1,] 1.6704 0.0504 -0.4992 -3.8610 ## [2,] 0.7047 -0.1280 0.8346 2.4915 ## [3,] 0.6177 0.1064 -1.1154 -1.3530 ## [4,] -0.3915 0.0136 0.4446 0.0990 ## [5,] 1.8183 0.0952 1.3884 -1.0395 ## [6,] -0.3654 0.0584 1.1154 2.7060 The following function generates an arbitrary \\(n \\times n\\) positive definite matrix. pdmatrix &lt;- function(n) { # function to generate an arbitrary n x n positive definite matrix # n is an integer # returns a matrix L &lt;- matrix(0, n, n) L[!lower.tri(L)] &lt;- abs(rnorm(n * (n + 1) / 2)) tcrossprod(L) } By generating random \\(n\\)-vectors of independent N(0, 1) random variates, \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_m,\\), say, and random \\(n \\times n\\) positive definite matrices, \\(\\mathbf{A}_1, \\ldots, \\mathbf{A}_m\\), say, confirm that \\(\\mathbf{x}_i^{\\text{T}} \\mathbf{A}_i \\mathbf{x}_i &gt; 0\\) for \\(i = 1, \\ldots, m\\) with \\(m = 100\\) and \\(n = 10\\). [Note that this can be considered a simulation-based example of trying to prove a result by considering a large number of simulations. Such an approach can be very valuable when an analytical approach is not possible.] Solution There are a variety of ways we can tackle this. One of the tidier seems to be to use all() and replicate(). check_pd &lt;- function(A, x) { # function to check whether a matrix is positive definite # A is a matrix # returns a logical sum(x * (A %*% x)) &gt; 0 } m &lt;- 1e2 n &lt;- 10 all(replicate(1e2, check_pd(pdmatrix(n), rnorm(n)) )) ## [1] TRUE For the cement factory data of Example 3.25 compute \\(\\hat {\\boldsymbol \\beta}\\) by inverting \\({\\bf X}^{\\text{T}} {\\bf X}\\) and multiplying by \\({\\bf X}^{\\text{T}} {\\bf y}\\), i.e. \\(\\hat {\\boldsymbol \\beta} = ({\\bf X}^{\\text{T}} {\\bf X})^{-1} {\\bf X}^{\\text{T}} {\\bf y}\\), and by solving \\({\\bf X}^{\\text{T}} {\\bf X} \\hat {\\boldsymbol \\beta} = {\\bf X}^{\\text{T}} {\\bf y}\\). Solution X &lt;- cbind(1, prod$days, prod$temp) XtX &lt;- crossprod(X) y &lt;- prod$output Xty &lt;- crossprod(X, y) (betahat1 &lt;- solve(XtX) %*% Xty) ## [,1] ## [1,] 9.12688541 ## [2,] 0.20281539 ## [3,] -0.07239294 (betahat2 &lt;- solve(XtX, Xty)) ## [,1] ## [1,] 9.12688541 ## [2,] 0.20281539 ## [3,] -0.07239294 Show in R that \\(\\mathbf{L}\\) is a Cholesky decomposition of \\(\\mathbf{A}\\) for \\[ \\mathbf{A} = \\begin{pmatrix}547.56&amp;348.66&amp;306.54 \\\\348.66&amp;278.26&amp;199.69 \\\\306.54&amp;199.69&amp;660.38 \\\\\\end{pmatrix} \\text{ and }\\mathbf{L} = \\begin{pmatrix}23.4&amp;0.0&amp;0.0 \\\\14.9&amp;7.5&amp;0.0 \\\\13.1&amp;0.6&amp;22.1 \\\\\\end{pmatrix}. \\] Solution We’ll load \\(\\mathbf{A}\\) and \\(\\mathbf{L}\\) as A and L, respectively. A &lt;- matrix(c(547.56, 348.66, 306.54, 348.66, 278.26, 199.69, 306.54, 199.69, 660.38), 3, 3) L &lt;- matrix(c(23.4, 14.9, 13.1, 0, 7.5, 0.6, 0, 0, 22.1), 3, 3) Then we need to show that \\(\\mathbf{L}\\) is lower-triangular all(L[upper.tri(L)] == 0) ## [1] TRUE which it is, that all its diagonal elements are positive all(diag(L) &gt; 0) ## [1] TRUE which they are, and that \\(\\mathbf{A} = \\mathbf{LL}^{\\text{T}}\\) all.equal(A, tcrossprod(L)) ## [1] TRUE which it does. For the matrix \\(\\mathbf{A}\\) below, find its Cholesky decomposition, \\(\\mathbf{L}\\), where \\(\\mathbf{A} = \\mathbf{LL}^\\text{T}\\) and \\(\\mathbf{L}\\) is a lower triangular matrix, and confirm that \\(\\mathbf{L}\\) is a Cholesky decomposition of \\(\\mathbf{A}\\): \\[\\mathbf{A} = \\begin{pmatrix}0.797&amp;0.839&amp;0.547 \\\\0.839&amp;3.004&amp;0.855 \\\\0.547&amp;0.855&amp;3.934 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\) A &lt;- cbind( c(0.797, 0.839, 0.547), c(0.839, 3.004, 0.855), c(0.547, 0.855, 3.934) ) and then we’ll find \\(\\mathbf{L}\\) L &lt;- t(chol(A)) L ## [,1] [,2] [,3] ## [1,] 0.8927486 0.0000000 0.000000 ## [2,] 0.9397943 1.4562921 0.000000 ## [3,] 0.6127145 0.1917022 1.876654 Then we’ll check that it’s lower-triangular all(L[upper.tri(L)] == 0) ## [1] TRUE which it is, then we’ll check that its diagonal elements are positive all(diag(L) &gt; 0) ## [1] TRUE which they are, and finally we’ll check that \\(\\mathbf{LL}^\\text{T} = \\mathbf{A}\\) all.equal(A, tcrossprod(L)) ## [1] TRUE which it does. So we conclude that \\(\\mathbf{L}\\) is a lower-triangular Cholesky decomposition of \\(\\mathbf{A}\\). Write a function in R called solve_chol() to solve a system of linear equations \\(\\mathbf{Ax} = \\mathbf{b}\\) based on the Cholesky decomposition \\(\\mathbf{A} = \\mathbf{LL}^{\\text{T}}\\). Solution The following is one option for solve_chol(). solve_chol &lt;- function(L, b) { # Function to solve LL^Tx = b for x # L is a lower-triangular matrix # b is a vector of length nrow(L) # return vector of same length as b y &lt;- forwardsolve(L, b) backsolve(t(L), y) } We’ll quickly check that we get the same result as solve() using the data from Example 3.2. y &lt;- c(.7, 1.3, 2.6) mu &lt;- 1:3 Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) res1 &lt;- solve(Sigma, y - mu) L &lt;- t(chol(Sigma)) all.equal(res1, solve_chol(L, y - mu)) ## [1] TRUE Note above that we could use backsolve(L, y, upper.tri = FALSE, transpose = TRUE) instead of backsolve(t(L), y), which avoids transposing L. Both give the same result, though. Show that solving \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) is equivalent to solving \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\) and then \\(\\mathbf{L}^{\\text{T}}\\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\) if \\(\\mathbf{A}\\) has Cholesky decomposition \\(\\mathbf{A} = \\mathbf{L} \\mathbf{L}^{\\text{T}}\\). Confirm this based on the cement factory data of Example 3.25 by taking \\(\\mathbf{A} = \\mathbf{X}^{\\text{T}}\\mathbf{X}\\), where \\(\\mathbf{X}\\) is the linear model’s design matrix. Solution Let \\(\\mathbf{L}^{\\text{T}} \\mathbf{x} = \\mathbf{y}\\). To solve \\(\\mathbf{LL}^{\\text{T}} \\mathbf{x} = \\mathbf{b}\\) for \\(\\mathbf{x}\\), we want to first solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\), and then \\(\\mathbf{L}^{\\text{T}} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\). We can confirm this numerically in R. We already have X from Question \\(\\ref{cement}\\), so we’ll re-use that X. We’ll set \\(\\mathbf{A} = \\mathbf{X}^{\\text{T}} \\mathbf{X}\\), and call this A. We’ll then use chol() to calculate its Cholesky decomposition in upper-triangular form, U, and lower-triangular form, L. A &lt;- crossprod(X) U &lt;- chol(A) L &lt;- t(U) The following two commands then solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\), and then \\(\\mathbf{L}^{\\text{T}} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\) cbind( solve(t(L), solve(L, Xty)), backsolve(t(L), forwardsolve(L, Xty)) ) ## [,1] [,2] ## [1,] 9.12688541 9.12688541 ## [2,] 0.20281539 0.20281539 ## [3,] -0.07239294 -0.07239294 although double use of solve() is inefficient compared to using forwardsolve() and then backsolve(). Alternatively, if we have \\(\\mathbf{A} = \\mathbf{U}^{\\text{T}} \\mathbf{U}\\), for upper-triangular \\(\\mathbf{U}\\), then we have the following two options cbind( backsolve(U, forwardsolve(t(U), Xty)), backsolve(U, forwardsolve(U, Xty, upper.tri = TRUE, transpose = TRUE)) ) ## [,1] [,2] ## [1,] 9.12688541 9.12688541 ## [2,] 0.20281539 0.20281539 ## [3,] -0.07239294 -0.07239294 the latter of which is ever so slightly more efficient for its avoidance of t(U). ## structure(c(0.185, 0.555, 3.615, 1.205, 8.7, -2.9, 8.7, -26.1, ## -0.553, -1.659, -2.697, -0.899), dim = 4:3) Show that \\(\\mathbf{U}\\) and \\(\\boldsymbol{\\Lambda}\\) form an eigen-decomposition of \\(\\mathbf{A}\\) for \\[\\mathbf{A} = \\begin{pmatrix}3.40&amp;0.00&amp;0.00 \\\\0.00&amp;0.15&amp;-2.06 \\\\0.00&amp;-2.06&amp;-1.05 \\\\\\end{pmatrix},~~\\mathbf{U} = \\begin{pmatrix}1.0&amp;0.0&amp;0.0 \\\\0.0&amp;0.6&amp;-0.8 \\\\0.0&amp;0.8&amp;0.6 \\\\\\end{pmatrix} \\text{ and } \\boldsymbol{\\Lambda} = \\begin{pmatrix}3.4&amp;0.0&amp;0.0 \\\\0.0&amp;-2.6&amp;0.0 \\\\0.0&amp;0.0&amp;1.7 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\), \\(\\boldsymbol{\\Lambda}\\) and \\(\\mathbf{U}\\) as A, Lambda and U, respectively. A &lt;- cbind( c(3.4, 0, 0), c(0, .152, -2.064), c(0, -2.064, -1.052) ) Lambda &lt;- diag(c(3.4, -2.6, 1.7)) U &lt;- matrix(c(1, 0, 0, 0, .6, .8, 0, -.8, .6), 3, 3) Then we need to show that \\(\\mathbf{U}\\) is orthogonal, all.equal(crossprod(U), diag(nrow(U))) ## [1] TRUE which it is, that \\(\\boldsymbol{\\Lambda}\\) is diagonal, all(Lambda - diag(diag(Lambda)) == 0) ## [1] TRUE and that \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\) all.equal(A, U %*% tcrossprod(Lambda, U)) ## [1] TRUE which it does. A &lt;- cbind( c(0.797, 0.839, 0.547), c(0.839, 3.004, 0.855), c(0.547, 0.855, 3.934) ) For the matrix \\(\\mathbf{A}\\) below, find \\(\\mathbf{U}\\) and \\(\\boldsymbol{\\Lambda}\\) in its eigen-decomposition of the form \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T}\\)m where \\(\\mathbf{U}\\) is orthogonal and \\(\\boldsymbol{\\Lambda}\\) is diagonal: \\[\\mathbf{A} = \\begin{pmatrix}0.797&amp;0.839&amp;0.547 \\\\0.839&amp;3.004&amp;0.855 \\\\0.547&amp;0.855&amp;3.934 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\) A &lt;- cbind( c(0.797, 0.839, 0.547), c(0.839, 3.004, 0.855), c(0.547, 0.855, 3.934) ) and then find \\(\\mathbf{U}\\) and \\(\\boldsymbol{\\Lambda}\\) eA &lt;- eigen(A, symmetric = TRUE) U &lt;- eA$vectors U ## [,1] [,2] [,3] ## [1,] -0.2317139 -0.1941590 0.95321085 ## [2,] -0.5372074 -0.7913728 -0.29178287 ## [3,] -0.8109975 0.5796821 -0.07906848 Lambda &lt;- diag(eA$values) Lambda ## [,1] [,2] [,3] ## [1,] 4.656641 0.000000 0.0000000 ## [2,] 0.000000 2.583555 0.0000000 ## [3,] 0.000000 0.000000 0.4948042 Then we need to show that \\(\\mathbf{U}\\) is orthogonal, all.equal(crossprod(U), diag(nrow(U))) ## [1] TRUE which it is, that \\(\\boldsymbol{\\Lambda}\\) is diagonal, all(Lambda - diag(diag(Lambda)) == 0) ## [1] TRUE which it is, and that \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\) all.equal(A, U %*% tcrossprod(Lambda, U)) ## [1] TRUE which it does. Show that \\(\\mathbf{U}\\), \\(\\mathbf{D}\\) and \\(\\mathbf{V}\\) form a singular value decomposition of \\(\\mathbf{A}\\) for \\[\\mathbf{A} = \\begin{pmatrix}0.185&amp;8.700&amp;-0.553 \\\\0.555&amp;-2.900&amp;-1.659 \\\\3.615&amp;8.700&amp;-2.697 \\\\1.205&amp;-26.100&amp;-0.899 \\\\\\end{pmatrix},~~\\mathbf{U} = \\begin{pmatrix}-0.2&amp;-0.2&amp;1.0 \\\\-0.5&amp;-0.8&amp;-0.3 \\\\-0.8&amp;0.6&amp;-0.1 \\\\\\end{pmatrix},\\] \\[\\mathbf{D} = \\begin{pmatrix}29&amp;0&amp;0 \\\\0&amp;5&amp;0 \\\\0&amp;0&amp;1 \\\\\\end{pmatrix} \\text{ and } \\mathbf{V} = \\begin{pmatrix}0.00&amp;0.76&amp;0.65 \\\\-1.00&amp;0.00&amp;0.00 \\\\0.00&amp;-0.65&amp;0.76 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\), \\(\\mathbf{U}\\), \\(\\mathbf{D}\\) and \\(\\mathbf{V}\\) as A, U, D and V, respectively. A &lt;- matrix(c(0.185, 0.555, 3.615, 1.205, 8.7, -2.9, 8.7, -26.1, -0.553, -1.659, -2.697, -0.899), 4, 3) U &lt;- matrix(c(-0.3, 0.1, -0.3, 0.9, 0.1, 0.3, 0.9, 0.3, -0.3, -0.9, 0.3, 0.1), 4, 3) D &lt;- diag(c(29, 5, 1)) V &lt;- matrix(c(0, -1, 0, 0.76, 0, -0.65, 0.65, 0, 0.76), 3, 3) We want to check that \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal all.equal(crossprod(U), diag(ncol(U))) ## [1] TRUE all.equal(crossprod(V), diag(nrow(V))) ## [1] &quot;Mean relative difference: 9.999e-05&quot; which they both are, that \\(\\mathbf{D}\\) is diagonal all(D - diag(diag(D)) == 0) ## [1] TRUE all.equal(crossprod(V), diag(nrow(V))) ## [1] &quot;Mean relative difference: 9.999e-05&quot; which it is, and finally that \\(\\mathbf{A} = \\mathbf{UDV}^{\\text{T}}\\) all.equal(A, U %*% tcrossprod(D, V)) ## [1] TRUE which is true. By considering \\(\\sqrt{\\mathbf{A}}\\) as \\(\\mathbf{A}^{1/2}\\), i.e. as a matrix power, show how an eigen-decomposition can be used to general multivariate Normal random vectors and then write a function to implement this in R. Solution From Example 3.14, to generate a multivariate Normal random vector, \\(\\mathbf{Y}\\), say, from the \\(MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) distribution we need to compute \\(\\mathbf{Y} = \\boldsymbol{\\mu} + \\mathbf{L} \\mathbf{Z}\\), where \\(\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^{\\text{T}}\\) and \\(\\mathbf{Z} = (Z_1, \\ldots, Z_p)^{\\text{T}}\\), where \\(Z_i\\), \\(i = 1, \\ldots, p\\), are independent \\(N(0, 1)\\) random variables. Given an eigen-decomposition of \\({\\boldsymbol \\Sigma} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\), we can write this as \\({\\boldsymbol \\Sigma} = \\mathbf{U} \\boldsymbol{\\Lambda}^{1/2} \\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^{\\text{T}}\\). As \\(\\boldsymbol{\\Lambda}\\) is diagonal, \\(\\boldsymbol{\\Lambda} = \\boldsymbol{\\Lambda}^{\\text{T}}\\) and hence \\(\\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^{\\text{T}} = (\\mathbf{U} \\boldsymbol{\\Lambda}^{1/2})^{\\text{T}}\\) and so \\({\\boldsymbol \\Sigma} = \\mathbf{LL}^{\\text{T}}\\) if \\(\\mathbf{L} = \\mathbf{U} \\boldsymbol{\\Lambda}^{1/2}\\). Therefore we can generate \\(MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) random variables with \\(\\mathbf{Y} = \\boldsymbol{\\mu} + \\mathbf{U} \\boldsymbol{\\Lambda}^{\\text{1/2}} \\mathbf{Z}\\). The following R function generates n multivariate Normal random vectors with mean mu and variance-covariance matrix Sigma. rmvn_eigen &lt;- function(n, mu, Sigma) { # Function to generate MVN random vectors with # n is a integer, giving the number of independent vectors to simulate # mu is a p-vector of the MVN mean # Sigma is a p x p matrix of the MVN variance-covariance matrix # returns a p times n matrix eS &lt;- eigen(Sigma, symmetric = TRUE) p &lt;- nrow(Sigma) Z &lt;- matrix(rnorm(p * n), p) mu + eS$vectors %*% diag(sqrt(eS$values)) %*% Z } rmvn_eigen(5, mu, Sigma) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3.812779 1.328476 4.819841 -1.4787812 -0.6061167 ## [2,] 5.302633 3.159313 3.723752 -0.1035905 -0.5120484 ## [3,] 5.310861 4.700222 3.865721 2.8249234 0.7614778 Show how an eigen-decomposition can be used to solve a system of linear equations \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) by matrix multiplications and vector divisions, only. Confirm this in R by solving \\(\\boldsymbol{\\Sigma} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\) for \\(\\mathbf{z}\\), with \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2. Solution To solve \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\), if \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\), then we want to solve \\(\\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}} \\mathbf{x} = \\mathbf{b}\\) for \\(\\mathbf{x}\\). The following manipulations can be used \\[\\begin{align} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}} \\mathbf{x} &amp;= \\mathbf{U}^{-1} \\mathbf{b} \\tag{3.1}\\\\ \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}} \\mathbf{x} &amp;= \\mathbf{U}^{\\text{T}} \\mathbf{b} \\tag{3.2} \\\\ \\mathbf{U}^{\\text{T}} \\mathbf{x} &amp;= (\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\boldsymbol{\\Lambda}) \\tag{3.3} \\\\ \\mathbf{x} &amp;= \\mathbf{U}^{-\\text{T}}(\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\boldsymbol{\\Lambda}) \\tag{3.4} \\\\ \\mathbf{x} &amp;= \\mathbf{U} [(\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\boldsymbol{\\Lambda})] \\tag{3.5} \\end{align}\\] in which (3.1) results from premultiplying by \\(\\mathbf{U}^{\\text{-1}}\\), (3.2) from orthogonality of \\(\\mathbf{U}\\), i.e. \\(\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}\\), (3.3) from elementwise division, given diagonal \\(\\boldsymbol{\\Lambda}\\), (3.4) from premultiplying by \\(\\mathbf{U}^{-\\text{T}}\\) and (3.5) from orthogonality of \\(\\mathbf{U}\\), again. Next we’ll load the data from Example 3.2 y &lt;- c(.7, 1.3, 2.6) mu &lt;- 1:3 Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) res1 &lt;- solve(Sigma, y - mu) Then we’ll go through the calculations given above eS &lt;- eigen(Sigma, symmetric = TRUE) lambda &lt;- eS$values U &lt;- eS$vectors res2 &lt;- U %*% (crossprod(U, y - mu) / lambda) which we see gives the same result as solve(), once we use as.vector() to convert res2 from a one-column matrix to a vector. all.equal(res1, as.vector(res2)) ## [1] TRUE Show in R that if \\(\\mathbf{H}_{4} = \\mathbf{UDV}^{\\text{T}}\\) is the SVD of \\(\\mathbf{H}_4\\), the \\(4 \\times 4\\) Hilbert matrix, then solving \\(\\mathbf{H}_4\\mathbf{x} = (1, 1, 1, 1)^{\\text{T}}\\) for \\(\\mathbf{x}\\) reduces to solving \\[ \\mathbf{V}^{\\text{T}} \\mathbf{x} \\simeq \\begin{pmatrix}-1.21257 \\\\-4.80104 \\\\-26.08668 \\\\234.33089 \\\\\\end{pmatrix} \\] and then use this to solve \\(\\mathbf{H}_4\\mathbf{x} = (1, 1, 1, 1)^{\\text{T}}\\) for \\(\\mathbf{x}\\). Solution We ultimately need to solve \\(\\mathbf{UDV}^{\\text{T}} \\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = (1, 1, 1, 1)^{\\text{T}}\\). Pre-multiplying by \\(\\mathbf{U}^{-1} = \\mathbf{U}^{\\text{T}}\\), we then need to solve \\(\\mathbf{DV}^{\\text{T}} \\mathbf{x} = \\mathbf{U}^{\\text{T}} \\mathbf{b}\\), which is equivalent to solving \\(\\mathbf{V}^{\\text{T}} \\mathbf{x} = (\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\mathbf{D})\\). The following calculates the SVD of \\(\\mathbf{H}_4\\) and then computes \\((\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\mathbf{D})\\). hilbert &lt;- function(n) { # Function to evaluate n by n Hilbert matrix. # n is an integer # Returns n by n matrix. ind &lt;- 1:n 1 / (outer(ind, ind, FUN = &#39;+&#39;) - 1) } H4 &lt;- hilbert(4) b &lt;- rep(1, 4) svdH &lt;- svd(H4) V &lt;- svdH$v U &lt;- svdH$u b2 &lt;- crossprod(U, b) z &lt;- b2 / svdH$d z ## [,1] ## [1,] -1.212566 ## [2,] -4.801038 ## [3,] -26.086677 ## [4,] 234.330888 which is as given in the question, subject to rounding. Finally we want to solve \\(\\mathbf{V}^{\\text{T}} \\mathbf{x} = (\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\mathbf{D})\\) for \\(\\mathbf{x}\\), which we can do with either of the following cbind( solve(t(V), z), V %*% z ) ## [,1] [,2] ## [1,] -4 -4 ## [2,] 60 60 ## [3,] -180 -180 ## [4,] 140 140 and we see that the latter gives the same result as solve() all.equal(solve(H4, b), as.vector(V %*% z)) ## [1] TRUE once we ensure that both are vectors. Note that solving systems of linear equations via the SVD is only a sensible option if we already have the SVD. Otherwise, solving via other decompositions is more efficient. Show that \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) form a QR decomposition of \\(\\mathbf{A}\\) for \\[\\mathbf{A} = \\begin{pmatrix}0.337&amp;0.890&amp;-1.035 \\\\0.889&amp;6.070&amp;-1.547 \\\\-1.028&amp;-1.545&amp;4.723 \\\\\\end{pmatrix},~~\\mathbf{Q} = \\begin{pmatrix}-0.241&amp;0.101&amp;0.965 \\\\-0.635&amp;-0.769&amp;-0.078 \\\\0.734&amp;-0.631&amp;0.249 \\\\\\end{pmatrix}\\] and \\[\\mathbf{R} = \\begin{pmatrix}-1.4&amp;-5.2&amp;4.7 \\\\0.0&amp;-3.6&amp;-1.9 \\\\0.0&amp;0.0&amp;0.3 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\), \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) as A, Q and R, respectively. A &lt;- matrix(c(0.3374, 0.889, -1.0276, 0.8896, 6.0704, -1.5452, -1.0351, -1.5468, 4.7234), 3, 3) Q &lt;- matrix(c(-0.241, -0.635, 0.734, 0.101, -0.769, -0.631, 0.965, -0.078, 0.249), 3, 3) R &lt;- matrix(c(-1.4, 0, 0, -5.2, -3.6, 0, 4.7, -1.9, 0.3), 3, 3) We want to show that \\(\\mathbf{Q}\\) is orthogonal all.equal(crossprod(Q), diag(nrow(Q))) ## [1] &quot;Mean relative difference: 0.001286839&quot; which it is (after allowing for a bit of error), that \\(\\mathbf{R}\\) is upper-triangular all(R[lower.tri(R)] == 0) ## [1] TRUE which it is, and that \\(\\mathbf{A} = \\mathbf{QR}\\) all.equal(A, Q %*% R) ## [1] TRUE which it does. The determinant of the \\(n \\times n\\) Hilbert matrix is given by \\[ |\\mathbf{H}_n| = \\dfrac{c_n^4}{c_{2n}} \\] where \\[ c_n = \\prod_{i}^{n - 1} = i! \\] is the Cauchy determinant. Write a function in R, det_hilbert(n, log = FALSE) that evaluates \\(|\\mathbf{H}_n|\\) and \\(\\log(|\\mathbf{H}_n|)\\) if log = FALSE or log = TRUE, respectively. Your function should compute \\(\\log(|\\mathbf{H}_n|)\\), and then return \\(|\\mathbf{H}_n|\\) if log = FALSE, as with dmvn1() in Example 3.2. Solution It will perhaps be tidier to write a function to calculate the Cauchy determinant det_cauchy &lt;- function(n, log = FALSE) { # function to calculate Cauchy determinant # n in an integer # log is a logical; defaults to FALSE # returns a scalar out &lt;- sum(lfactorial(seq_len(n - 1))) if (!log) out &lt;- exp(out) out } and then to use that to calculate the determinant of \\(\\mathbf{H}_n\\). det_hilbert &lt;- function(n, log = FALSE) { # function to calculate determinant of Hilbert matrix # n in an integer # log is a logical; defaults to FALSE # returns a scalar out &lt;- 4 * det_cauchy(n, TRUE) - det_cauchy(2 * n, TRUE) if (!log) out &lt;- exp(out) out } Calculate \\(|\\mathbf{H}_n|\\) and \\(\\log(|\\mathbf{H}_n|)\\) through the QR decomposition of \\(\\mathbf{H}_n\\) for \\(n = 5\\) and confirm that both give the same result as det_hilbert() above. Solution We’ll start with a function det_QR(), which calculates the determinant of a matrix via its QR decomposition det_QR &lt;- function(A) { # function to calculate determinant of a matrix via QR decomposition # A is a matrix # returns a scalar qrA &lt;- qr(A) R &lt;- qr.R(qrA) prod(abs(diag(R))) } and we see that this gives the same answer as det_hilbert() for \\(n = 5\\). det_QR(hilbert(5)) ## [1] 3.749295e-12 det_hilbert(5) ## [1] 3.749295e-12 Then we’ll write a function to calculate the logarithm of the determinant of a matrix through its QR decomposition. logdet_QR &lt;- function(A) { # function to calculate log determinant of a matrix via QR decomposition # A is a matrix # returns a scalar qrA &lt;- qr(A) R &lt;- qr.R(qrA) sum(log(abs(diag(R)))) } which we also see gives the same answer as det_hilbert() with log = TRUE. logdet_QR(hilbert(5)) ## [1] -26.30945 det_hilbert(5, log = TRUE) ## [1] -26.30945 Compute \\(\\mathbf{H}_4^{-1}\\) via its QR decomposition, and confirm your result with solve() and qr.solve(). Solution If \\(\\mathbf{H}_4 = \\mathbf{QR}\\) then \\(\\mathbf{H}_4^{-1} = (\\mathbf{QR})^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^{\\text{T}}\\), since \\(\\mathbf{Q}\\) is orthogonal. Therefore we want to solve \\(\\mathbf{R} \\mathbf{X} = \\mathbf{Q}^{\\text{T}}\\) for \\(\\mathbf{X}\\). The following calculates the QR decomposition of \\(\\mathbf{H}_4\\) with qr() and then extracts \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) as Q and R, respectively. H4 &lt;- hilbert(4) qrH &lt;- qr(H4) Q &lt;- qr.Q(qrH) R &lt;- qr.R(qrH) Then we want to solve \\(\\mathbf{R} \\mathbf{X} = \\mathbf{Q}^{\\text{T}}\\) for \\(\\mathbf{X}\\), which we do with backsolve(), since \\(\\mathbf{R}\\), stored as R, is upper-triangular. X1 &lt;- backsolve(R, t(Q)) The following use solve() and qr.solve(). Note that if we already have the QR decomposition from qr(), then qr.solve() uses far fewer calculations to obtain the inverse. X2 &lt;- solve(H4) X3 &lt;- qr.solve(qrH) We see that both give the same answer all.equal(X2, X3) ## [1] TRUE and also the same answer as with backsolve() above all.equal(X1, X2) ## [1] TRUE Benchmark Cholesky, eigen (with symmetric = TRUE and symmetric = FALSE), singular value and QR decompositions of \\(\\mathbf{A} = \\mathbf{I}_{100} + \\mathbf{H}_{100}\\), where \\(\\mathbf{H}_{100}\\) is the \\(100 \\times 100\\) Hilbert matrix. (If you’re feeling impatient, consider reducing the value of argument times for function microbenchmark::microbenchmark().) Solution hilbert &lt;- function(n) { # Function to evaluate n by n Hilbert matrix. # n is an integer # Returns n by n matrix. ind &lt;- 1:n 1 / (outer(ind, ind, FUN = &#39;+&#39;) - 1) } H100 &lt;- hilbert(1e2) + diag(1, 1e2) microbenchmark::microbenchmark( chol(H100), eigen(H100, symmetric = TRUE), eigen(H100), svd(H100), qr(H100), times = 1e2 ) ## Unit: microseconds ## expr min lq mean median uq ## chol(H100) 131.902 169.724 201.6285 186.5585 200.8825 ## eigen(H100, symmetric = TRUE) 1759.716 2124.160 2335.1459 2183.9480 2257.8235 ## eigen(H100) 2520.745 2719.698 2910.0417 2776.2990 2856.5460 ## svd(H100) 1761.567 1888.135 2120.4812 1942.0690 2003.8705 ## qr(H100) 266.409 398.699 466.0155 413.1450 424.2145 ## max neval ## 728.733 100 ## 4769.881 100 ## 6585.226 100 ## 4852.035 100 ## 3265.292 100 If we consider median computation times, we see that the Cholesky decomposition is quickest, at nearly six times quicker than the QR decomposition, which is next quickest. The QR decomposition is then about just under three times quicker than the symmetric eigen-decomposition, which takes about the same amount of time as the singular value decomposition. The asymmetric eigen-decomposition is slowest, and demonstrates that if we know the matrix we want an eigen-decomposition of is symmetric, then we should pass this information to R. Remark. The following shows us that if we only want the eigenvalues of a symmetric matrix, then we can further save times by specifying only.values = TRUE. microbenchmark::microbenchmark( eigen(H100, symmetric = TRUE), eigen(H100, symmetric = TRUE, only.values = TRUE), times = 1e2 ) ## Unit: microseconds ## expr min lq mean ## eigen(H100, symmetric = TRUE) 2014.998 2066.200 2224.633 ## eigen(H100, symmetric = TRUE, only.values = TRUE) 914.626 944.736 1038.733 ## median uq max neval ## 2095.7450 2162.7725 4745.816 100 ## 964.0725 989.1285 3631.472 100 Given that \\[\\mathbf{A} = \\begin{pmatrix}1.23&amp;0.30&amp;2.58 \\\\0.30&amp;0.43&amp;1.92 \\\\2.58&amp;1.92&amp;10.33 \\\\\\end{pmatrix},~\\mathbf{A}^{-1} = \\begin{pmatrix}6.9012&amp;16.9412&amp;-4.8724 \\\\16.9412&amp;55.2602&amp;-14.5022 \\\\-4.8724&amp;-14.5022&amp;4.0092 \\\\\\end{pmatrix},\\] \\[\\mathbf{B} = \\begin{pmatrix}1.49&amp;0.40&amp;2.76 \\\\0.40&amp;0.53&amp;2.16 \\\\2.76&amp;2.16&amp;11.20 \\\\\\end{pmatrix},\\] and that \\(\\mathbf{B} = \\mathbf{A} + \\mathbf{LL}^{\\text{T}}\\), where \\(\\mathbf{L}\\) is a lower-triangular matrix, find \\(\\mathbf{B}^{-1}\\) using Woodbury’s formula, i.e. using \\[(\\mathbf{A} + \\mathbf{UV}^{\\text{T}})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_n + \\mathbf{V}^{\\text{T}} \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^{\\text{T}} \\mathbf{A}^{-1}. \\] Solution We note that we can write \\(\\mathbf{U} = \\mathbf{V} = \\mathbf{L}\\) and that \\(\\mathbf{LL}^{\\text{T}} = \\mathbf{B} - \\mathbf{A}\\), and so we can obtain \\(\\mathbf{L}\\) via the Cholesky decomposition of \\(\\mathbf{B} - \\mathbf{A}\\). We’ll load \\(\\mathbf{A}\\), \\(\\mathbf{A}^{-1}\\) and \\(\\mathbf{B}\\) below as A, iA and B, and then compute the lower-triangular Cholesky decomposition of \\(\\mathbf{B} - \\mathbf{A}\\) and store this as L. A &lt;- matrix( c(1.23, 0.3, 2.58, 0.3, 0.43, 1.92, 2.58, 1.92, 10.33), 3, 3) iA &lt;- matrix( c(6.9012, 16.9412, -4.8724, 16.9412, 55.2602, -14.5022, -4.8724, -14.5022, 4.0092), 3, 3) B &lt;- matrix( c(1.49, 0.4, 2.76, 0.4, 0.53, 2.16, 2.76, 2.16, 11.2), 3, 3) L &lt;- t(chol(B - A)) Then we’ll write a function to implement Woodbury’s formula, woodbury &lt;- function(iA, U, V) { # function to implement Woodbury&#39;s formula # iA, U and V are matrices # returns a matrix I_n &lt;- diag(nrow(iA)) iAU &lt;- iA %*% U iA - iAU %*% solve(I_n + crossprod(V, iAU), crossprod(V, iA)) } and then use this to find \\(\\mathbf{B}^{-1}\\), which we’ll call iB, iB &lt;- woodbury(iA, L, L) and see that, subject to rounding, this is equal to \\(\\mathbf{B}^{-1}\\) all.equal(iB, solve(B)) ## [1] &quot;Mean relative difference: 9.113679e-06&quot; Recall the cement factory data of Example 3.25. Now suppose that a further observation has been obtained based on the factory operating at 20 degrees for 14 days. For given \\(\\sigma^2\\), the sampling distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) is \\(MVN_3(\\hat{\\boldsymbol{\\beta}}, \\sigma^{-2} (\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1})\\). Use the Sherman-Morrison formula to give an expression for the estimated standard errors of \\(\\hat{\\boldsymbol{\\beta}}\\) in terms of \\(\\sigma\\) given that \\[(\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1} = \\begin{pmatrix} 2.78 \\times 10^0 &amp; -1.12 \\times 10^{-2} &amp; -1.06 \\times 10^{-1}\\\\ -1.12 \\times 10^{-2} &amp; 1.46 \\times 10^{-4} &amp; 1.75 \\times 10^{-4}\\\\ -1.0 \\times 10^{-1} &amp; 1.75 \\times 10^{-4} &amp; 4.79 \\times 10^{-3} \\end{pmatrix}. \\] Solution If we refer to the Sherman-Morrison formula, we take \\(\\mathbf{A}^{-1} = (\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1}\\) and \\(\\mathbf{u}^{\\text{T}} = \\mathbf{v} = (1, 20, 14)\\). Then we can calculate the updated variance-covariance matrix of the sampling distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) as \\[\\sigma^{-2} (\\mathbf{A} + \\mathbf{u} \\mathbf{v}^{\\text{T}})^{-1} = \\sigma^{-1} \\left[ \\mathbf{A}^{-1} - \\dfrac{\\mathbf{A}^{-1} \\mathbf{u} \\mathbf{v}^{\\text{T}} \\mathbf{A}^{-1}}{1 + \\mathbf{v}^{\\text{T}}\\mathbf{A}^{-1}\\mathbf{u}} \\right] \\] which can be calculated in R with V &lt;- matrix( c(2.78, -0.0112, -0.106, -0.0112, 0.000146, 0.000175, -0.106, 0.000175, 0.00479), 3, 3) u &lt;- c(1, 20, 14) V2 &lt;- V - (V %*% tcrossprod(u) %*% V) / (1 + crossprod(u, V %*% u))[1, 1] V2 ## [,1] [,2] [,3] ## [1,] 1.992477728 -6.917113e-03 -7.996475e-02 ## [2,] -0.006917113 1.227078e-04 3.340903e-05 ## [3,] -0.079964749 3.340903e-05 3.929282e-03 Taking the diagonal elements of V2 we get \\[ \\left(\\text{e.s.e.}(\\hat \\beta_0), \\text{e.s.e.}(\\hat \\beta_1), \\text{e.s.e.}(\\hat \\beta_2)\\right) = \\sigma^{-1} \\left( 1.412~,0.011~,0.063 \\right). \\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
