[["index.html", "MTH3045: Statistical Computing: Exercises 1 Chapter 1 exercises", " MTH3045: Statistical Computing: Exercises Dr. Ben Youngman b.youngman@exeter.ac.uk Laver 817; ext. 2314 16/01/2023 1 Chapter 1 exercises Generate a sample of \\(n = 20\\) \\(N(1, 3^2)\\) random variates, and without using mean(), var() or sd() write R functions to calculate the sample mean, \\(\\bar x\\), sample variance, \\(s^2\\), and sample standard deviation, \\(s\\), where \\[ \\bar x = \\dfrac{1}{n} \\sum_{i = 1}^n x_i \\text{ and } s^2 = \\dfrac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar x)^2. \\] Note than sum() may be used. Solution n &lt;- 20 y &lt;- rnorm(n, 1, 3) # mean mean2 &lt;- function(x) { # function to calculate mean of a vector # x is a vector # returns a scalar sum(x) / length(x) } mean2(y) ## [1] 0.3961438 # check it works all.equal(mean(y), mean2(y)) ## [1] TRUE # variance var2 &lt;- function(x) { # function to calculate variance of a vector # x is a vector # returns a scalar xbar &lt;- mean2(x) sum((x - xbar)^2) / (length(x) - 1) } var2(y) ## [1] 14.14087 # check it works all.equal(var(y), var2(y)) ## [1] TRUE # standard deviation sd2 &lt;- function(x) { # function to calculate standard deviation of a vector # x is a vector # returns a scalar sqrt(var2(x)) } sd2(y) ## [1] 3.760435 # check it works all.equal(sd(y), sd2(y)) ## [1] TRUE Note that above for mean2() we’ve not used a for loop. A for loop can be used, but usually if on cna be avoided, then it should. Then for var2() we’ve re-used mean2() and for sd2() we’ve re-used var2(). It’s often a good idea to re-use functions. In this case, we break the function sd2() into various smaller functions, which can often be good practice, as whether the final function is correct can be assessed by whether the simpler functions it comprises are correct. Consider computing \\(\\text{Pr}(Z \\geq z) = 1 - \\Phi(z)\\) where \\(Z \\sim \\text{Normal}(0, 1)\\), or, for short, \\(Z \\sim N(0, 1)\\). For \\(z = 0, 0.5, 1, 1.5, 2, \\ldots\\) compute this in R in three different ways using the following three commands pnorm(z, lower.tail = FALSE) 1 - pnorm(z) pnorm(-z) and find the lowest value of \\(z\\) for which the three don’t give the same answer. Solution z &lt;- seq(0, 7, by = .5) cbind( z, pnorm(z, lower.tail = FALSE), 1 - pnorm(z), pnorm(-z) ) ## z ## [1,] 0.0 5.000000e-01 5.000000e-01 5.000000e-01 ## [2,] 0.5 3.085375e-01 3.085375e-01 3.085375e-01 ## [3,] 1.0 1.586553e-01 1.586553e-01 1.586553e-01 ## [4,] 1.5 6.680720e-02 6.680720e-02 6.680720e-02 ## [5,] 2.0 2.275013e-02 2.275013e-02 2.275013e-02 ## [6,] 2.5 6.209665e-03 6.209665e-03 6.209665e-03 ## [7,] 3.0 1.349898e-03 1.349898e-03 1.349898e-03 ## [8,] 3.5 2.326291e-04 2.326291e-04 2.326291e-04 ## [9,] 4.0 3.167124e-05 3.167124e-05 3.167124e-05 ## [10,] 4.5 3.397673e-06 3.397673e-06 3.397673e-06 ## [11,] 5.0 2.866516e-07 2.866516e-07 2.866516e-07 ## [12,] 5.5 1.898956e-08 1.898956e-08 1.898956e-08 ## [13,] 6.0 9.865876e-10 9.865877e-10 9.865876e-10 ## [14,] 6.5 4.016001e-11 4.015999e-11 4.016001e-11 ## [15,] 7.0 1.279813e-12 1.279865e-12 1.279813e-12 Above we see that for \\(z = 6.0\\), the second approximation to the standard normal tail probability doesn’t give the same answer as the other two approximations (although the discrepancy is tiny). The formula \\(\\text{Var}(Y) = \\text{E}(Y^2) - [\\text{E}(Y)]^2\\) is sometimes called the ‘short-cut’ variance formula, i.e. a short-cut for \\(\\text{Var}(Y) = \\text{E}[Y - \\text{E}(Y)]^2\\). Compare computing the biased version of \\(\\text{Var}(Y)\\) using the two formulae above for the samples y1 and y2 below. y1 &lt;- 1:10 y2 &lt;- y1 + 1e9 Solution We’ll start with a function to calculate the short-cut formula bvar1 &lt;- function(x) { # function to calculate short-cut variance # x is a vector # returns a scalar mean(x^2) - mean(x)^2 } and the we’ll write a function to calculate the other formula. bvar2 &lt;- function(x) { # function to calculate variance # x is a vector # returns a scalar mean((x - mean(x))^2) } For y1 we have bvar1(y1) ## [1] 8.25 bvar2(y1) ## [1] 8.25 and so both formulae give the same answer, but for y2 we have bvar1(y2) ## [1] 0 bvar2(y2) ## [1] 8.25 and see that they don’t. The short-cut formula is clearly wrong, bcause the variance of a sample doesn’t change if we add a constant, which is the only difference between y1 and y2. (We’ll learn about the cause of this in Chapter 2.) "],["chapter-2-exercises.html", "2 Chapter 2 exercises", " 2 Chapter 2 exercises Compute \\(\\pi + \\text{e}\\), where \\(\\pi = 3.1415927 \\times 10^0\\) and \\(\\text{e} = 2.7182818 \\times 10^0\\), using floating point addition in base 10, working to five decimal places. Solution As \\(\\pi\\) and \\(\\text{e}\\) have a common exponent, we sum their mantissas, i.e. \\[\\begin{align*} \\pi + \\text{e} &amp;= (3.14159 \\times 10^0) + (2.71828 \\times 10^0)\\\\ &amp;= 5.85987 \\times 10^0 \\end{align*}\\] Now compute \\(10^6\\pi + \\text{e}\\), using floating point addition in base 10, now working to seven decimal places. Solution We first need to put the numbers on a common exponent, which is that of \\(10^6 \\pi\\). \\[\\begin{align*} 10^6 \\pi &amp;= 3.1415927 \\times 10^6\\\\ \\text{e} &amp; = 2.7182818 \\times 10^0 = 0.0000027 \\times 10^6. \\end{align*}\\] Then summing their mantissas gives \\[\\begin{align*} 10^6 \\pi + \\text{e} &amp;= (3.1415927 \\times 10^6) + (2.7182818 \\times 10^0)\\\\ &amp;= (3.1415927 \\times 10^6) + (0.0000027 \\times 10^6)\\\\ &amp;= (3.1415927 + 0.0000027) \\times 10^6\\\\ &amp;= 3.1415954 \\times 10^6. \\end{align*}\\] What would happen if we computed \\(10^6\\pi + \\text{e}\\) using a base 10 floating point representation, but only worked with five decimal places? Solution If we put \\(2.7182818 \\times 10^0\\) onto the exponent \\(10^6\\) we get \\(0.00000 \\times 10^6\\) to five decimal places, and so \\(\\text{e}\\) becomes negligible alongside \\(10^6 \\pi\\) if we only work to five decimal places. Write \\(2\\pi\\) in binary form using single- and double precision arithmetic. [Hint: I recommend you consider the binary forms for \\(\\pi\\) given in the lecture notes, and you might also use bit2decimal() from the lecture notes to check your answer.] Solution The key here is to note that we want to calculate \\(\\pi \\times 2\\). As we have a number in the form \\(S \\times (1 + F) \\times 2^{E - e}\\), then we just need to raise \\(E\\) by one. For both single- and double-precision, this corresponds to changing the last zero in the 0s and 1s for the \\(E\\) term to a one. bit2decimal &lt;- function(x, e, dp = 20) { # function to convert bits to decimal form # x: the bits as a character string, with appropriate spaces # e: the excess # dp: the decimal places to report the answer to bl &lt;- strsplit(x, &#39; &#39;)[[1]] # split x into S, E and F components by spaces # and then into a list of three character vectors, each element one bit bl &lt;- lapply(bl, function(z) as.integer(strsplit(z, &#39;&#39;)[[1]])) names(bl) &lt;- c(&#39;S&#39;, &#39;E&#39;, &#39;F&#39;) # give names, to simplify next few lines S &lt;- (-1)^bl$S # calculate sign, S E &lt;- sum(bl$E * 2^c((length(bl$E) - 1):0)) # ditto for exponent, E F &lt;- sum(bl$F * 2^(-c(1:length(bl$F)))) # and ditto to fraction, F z &lt;- S * 2^(E - e) * (1 + F) # calculate z out &lt;- format(z, nsmall = dp) # use format() for specific dp # add (S, E, F) as attributes, for reference attr(out, &#39;(S,E,F)&#39;) &lt;- c(S = S, E = E, F = F) out } bit2decimal(&#39;0 10000001 10010010000111111011011&#39;, 127) ## [1] &quot;6.28318548202514648438&quot; ## attr(,&quot;(S,E,F)&quot;) ## S E F ## 1.0000000 129.0000000 0.5707964 bit2decimal(&#39;0 10000000001 1001001000011111101101010100010001000010110100011000&#39;, 1023) ## [1] &quot;6.28318530717958623200&quot; ## attr(,&quot;(S,E,F)&quot;) ## S E F ## 1.0000000 1025.0000000 0.5707963 Find the calculation error in R of \\(b - a\\) where \\(a = 10^{16}\\) and \\(b = 10^{16} + \\exp(0.5)\\). Solution a &lt;- 1e16 b &lt;- 1e16 + exp(.5) b - a ## [1] 2 Create the following: The row vector \\[ \\mathbf{a} = (2, 4, 6), \\] the \\(2 \\times 3\\) matrix \\[ \\mathbf{B} = \\left(\\begin{array}{ccc} 6 &amp; 5 &amp; 4\\\\ 3 &amp; 2 &amp; 1\\end{array}\\right) \\] and a list with first element \\(\\mathbf{a}\\) and second element \\(\\mathbf{B}\\), and an arbitrary (i.e. with whatever values you like) \\(5 \\times 3 \\times 2\\) array with a 'name' attribute that is 'array1'. \\(~\\) For each of the above, consider whether your code could be simpler. Solution a &lt;- t(c(2, 4, 6)) B &lt;- matrix(6:1, 2, byrow = TRUE) l &lt;- list(a, B) arr &lt;- array(rnorm(30), c(5, 3, 2)) attr(arr, &#39;name&#39;) &lt;- &#39;array1&#39; arr ## , , 1 ## ## [,1] [,2] [,3] ## [1,] -0.1698401 -0.5362694 -1.3468028 ## [2,] 0.9098240 -3.0997186 0.4959363 ## [3,] 0.6588275 0.4357260 -0.6007355 ## [4,] -0.1423156 -1.7257134 0.5183544 ## [5,] 0.4835803 -2.0039373 -0.5295640 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 0.23024534 -0.3957261 -1.7707146 ## [2,] -0.02148646 -0.6347192 1.8277414 ## [3,] 0.08173060 0.8668145 2.3671458 ## [4,] -0.66329711 1.0771460 -0.6908651 ## [5,] 0.11385164 -0.4261344 0.6500139 ## ## attr(,&quot;name&quot;) ## [1] &quot;array1&quot; Produce a \\(3 \\times 4 \\times 4 \\times 2\\) array containing Uniform(0, 1) random variates and compute the mean over its second and third margins using apply(..., ..., means) and rowMeans() or colMeans(). Solution arr &lt;- array(NA, c(3, 4, 4, 2)) arr[] &lt;- runif(prod(dim(arr))) # saves names to get the number right apply(arr, 2:3, mean) ## [,1] [,2] [,3] [,4] ## [1,] 0.6907286 0.6153756 0.3865067 0.5363558 ## [2,] 0.7104660 0.5126316 0.7054345 0.6194567 ## [3,] 0.4275345 0.5965150 0.5312223 0.4960691 ## [4,] 0.7214823 0.5644073 0.4722087 0.5001974 rowMeans(aperm(arr, c(2, 3, 1, 4)), dims = 2) ## [,1] [,2] [,3] [,4] ## [1,] 0.6907286 0.6153756 0.3865067 0.5363558 ## [2,] 0.7104660 0.5126316 0.7054345 0.6194567 ## [3,] 0.4275345 0.5965150 0.5312223 0.4960691 ## [4,] 0.7214823 0.5644073 0.4722087 0.5001974 colMeans(aperm(arr, c(1, 4, 2, 3)), dims = 2) ## [,1] [,2] [,3] [,4] ## [1,] 0.6907286 0.6153756 0.3865067 0.5363558 ## [2,] 0.7104660 0.5126316 0.7054345 0.6194567 ## [3,] 0.4275345 0.5965150 0.5312223 0.4960691 ## [4,] 0.7214823 0.5644073 0.4722087 0.5001974 Create a 3-element list of vectors comprising Uniform(0, 1) variates of length 3, 5 and 7, respectively. Solution lst &lt;- list(runif(3), runif(5), runif(7)) Create another list in which the vectors above are sorted into descending order. Solution lst2 &lt;- lapply(lst, sort, decreasing = TRUE) Then create a vector comprising the minimum of each vector in the list, and another stating which element is the minimum. [Hint: for the latter you may want to consult the ‘See Also’ part of the min() function’s help file.] Solution sapply(lst, min) ## [1] 0.1402660981 0.0005073147 0.0358424829 sapply(lst, which.min) ## [1] 3 4 1 Use a for() loop to produce the following. [Hint: the function paste() might be useful.] ## [1] &quot;iteration 1&quot; ## [1] &quot;iteration 2&quot; ## [1] &quot;iteration 3&quot; ## [1] &quot;iteration 4&quot; ## [1] &quot;iteration 5&quot; ## [1] &quot;iteration 6&quot; ## [1] &quot;iteration 7&quot; ## [1] &quot;iteration 8&quot; ## [1] &quot;iteration 9&quot; ## [1] &quot;iteration 10&quot; Solution for (i in 1:10) print(paste(&#39;iteration&#39;, i)) ## [1] &quot;iteration 1&quot; ## [1] &quot;iteration 2&quot; ## [1] &quot;iteration 3&quot; ## [1] &quot;iteration 4&quot; ## [1] &quot;iteration 5&quot; ## [1] &quot;iteration 6&quot; ## [1] &quot;iteration 7&quot; ## [1] &quot;iteration 8&quot; ## [1] &quot;iteration 9&quot; ## [1] &quot;iteration 10&quot; Consider the following two infinite series that represent \\(\\pi\\).\\[\\begin{align*} \\pi &amp;= 4 \\left[1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\frac{1}{9} - \\frac{1}{11} + \\frac{1}{13} -\\cdots\\right]\\\\ \\pi &amp;= 3 + \\frac{4}{2 \\times 3 \\times 4} - \\frac{4}{4 \\times 5 \\times 6} + \\frac{4}{6 \\times 7 \\times 8} - \\frac{4}{8 \\times 9 \\times 10} + \\cdots \\end{align*}\\] Use a while() loop to find which converges to pi in R to within \\(\\epsilon_m^{1/3}\\) using the fewest terms, where \\(\\epsilon_m\\) is R’s machine tolerance. Solution Here’s the first approximation… quarter_pi &lt;- 1 terms1 &lt;- 1 denom &lt;- 3 multiplier &lt;- -1 my_pi &lt;- 4 * quarter_pi while(abs(my_pi - pi) &gt; .Machine$double.eps^(1/3)) { terms1 &lt;- terms1 + 1 quarter_pi &lt;- quarter_pi + multiplier / denom my_pi &lt;- 4 * quarter_pi denom &lt;- denom + 2 multiplier &lt;- -1 * multiplier } terms1 ## [1] 165141 …and here’s the second approximation… my_pi &lt;- 3 terms2 &lt;- 2 denoms &lt;- c(2, 3, 4) multiplier &lt;- 1 while(abs(my_pi - pi) &gt; .Machine$double.eps^(1/3)) { my_pi &lt;- my_pi + multiplier * 4 / prod(denoms) denoms &lt;- denoms + 2 multiplier &lt;- -1 * multiplier terms2 &lt;- terms2 + 1 } terms2 ## [1] 36 Write a function based on a for() loop to calculate the cumulative sum of a vector, \\(\\bf y\\), i.e. so that its \\(i\\)th value, \\(y_i\\) say, is \\[y_i = \\sum_{j = 1}^i x_j.\\] Solution Either of the following two functions are options (although others exist). my_cumsum &lt;- function(x) { # function 1 to calculate cumulative sum of a vector # x is a vector # returns a vector of length length(x) out &lt;- numeric(length(x)) for (i in 1:length(x)) { out[i] &lt;- sum(x[1:i]) } out } my_cumsum2 &lt;- function(x) { # function 2 to calculate cumulative sum of a vector # x is a vector # returns a vector of length length(x) out &lt;- x for (i in 2:length(x)) { out[i] &lt;- out[i] + out[i - 1] } out } We see that both perform the same calculation. x &lt;- runif(10) cumsum(x) ## [1] 0.02687266 0.89647532 1.63953215 1.80682152 2.47257741 2.70524939 ## [7] 3.44352866 3.89313316 4.13715064 4.83476706 my_cumsum(x) ## [1] 0.02687266 0.89647532 1.63953215 1.80682152 2.47257741 2.70524939 ## [7] 3.44352866 3.89313316 4.13715064 4.83476706 my_cumsum2(x) ## [1] 0.02687266 0.89647532 1.63953215 1.80682152 2.47257741 2.70524939 ## [7] 3.44352866 3.89313316 4.13715064 4.83476706 Then benchmark its execution time against R’s vectorised function cumsum() for summing 1000 iid \\(Uniform[0, 1]\\) random variables. Solution x &lt;- runif(1e3) microbenchmark::microbenchmark( cumsum(x), my_cumsum(x), my_cumsum2(x) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## cumsum(x) 752 877.0 1780.36 1140.5 2150.5 17339 100 ## my_cumsum(x) 1528018 1574073.5 2127775.09 1617887.0 1908017.0 8328514 100 ## my_cumsum2(x) 43169 44523.5 48141.41 45717.5 49784.5 74044 100 To start a board game, a player must throw three sixes using a conventional die. Write a function to simulate this, which returns the total number of throws that the player has taken. Solution sixes1 &lt;- function() { # function to simulate number of throws needed # to reach three sixes # n is an integer # returns total number of throws taken out &lt;- sample(1:6, 3, replace = TRUE) while(sum(out == 6) &lt; 3) { out &lt;- c(out, sample(1:6, 1)) } return(length(out)) } Then use 1000 simulations to empirically estimate the distribution of the number of throws needed. Solution samp1 &lt;- replicate(1e3, sixes1()) table(samp1) ## samp1 ## 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## 1 15 28 13 40 30 36 52 53 43 56 40 46 47 36 38 31 33 36 27 29 18 26 26 19 24 ## 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 53 55 56 ## 17 17 11 9 12 12 6 7 7 11 4 8 5 6 4 1 4 2 1 1 1 2 1 1 1 1 ## 57 58 59 62 ## 2 1 1 1 hist(samp1, xlab = &#39;Number of throws&#39;, main = &#39;Histogram of number of throws&#39;) Figure 2.1: Histogram of empirical distribution of number of throws for starting method 1. I think you’ll agree that this would be a rather dull board game! It is therefore proposed that a player should instead throw two consecutive sixes. Write a function to simulate this new criterion, and estimate its distribution empirically. Solution sixes2 &lt;- function() { # function to simulate number of throws needed # for two consecutive sixes # n is an integer # returns total number of throws taken out &lt;- sample(1:6, 2, replace = TRUE) cond &lt;- TRUE while(cond) { if (sum(out[1:(length(out) - 1)] + out[2:length(out)] == 12) &gt; 0) { cond &lt;- FALSE } else { out &lt;- c(out, sample(1:6, 1)) } } return(length(out)) } sixes2() ## [1] 11 Then use 1000 simulations to empirically estimate the distribution of the number of throws needed. Solution samp2 &lt;- replicate(1e3, sixes2()) table(samp2) ## samp2 ## 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ## 35 27 24 19 10 21 18 20 18 17 27 15 20 19 17 19 20 20 12 16 ## 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ## 16 19 11 9 12 9 11 9 16 9 13 12 12 11 13 10 8 8 10 15 ## 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 ## 7 10 11 8 6 3 7 7 11 4 13 7 9 6 6 7 6 5 4 5 ## 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ## 7 5 11 3 6 10 5 4 2 1 2 2 3 5 2 5 7 6 4 6 ## 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 102 ## 7 7 4 1 3 1 1 1 2 2 2 3 1 2 3 2 5 1 1 4 ## 103 104 105 106 108 109 110 111 112 113 115 116 117 119 120 121 124 127 128 129 ## 3 2 1 1 1 2 2 2 3 5 2 1 2 1 1 1 1 1 1 2 ## 130 131 132 133 135 138 140 143 146 147 149 152 153 154 155 158 159 160 165 166 ## 1 3 2 1 1 2 3 1 3 1 1 1 1 1 2 1 2 1 1 1 ## 172 175 182 185 198 202 206 207 228 240 246 266 292 318 345 ## 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 hist(samp2, xlab = &#39;Number of throws&#39;, main = &#39;Histogram of number of throws&#39;) Figure 2.2: Histogram of empirical distribution of number of throws for starting method 2. By comparing sample mean starting numbers of throws, which starting criterion should get a player into the game quickest? Solution mean(samp1) - mean(samp2) ## [1] -23.681 So the second approach, by comparing means, takes more throws before the game can begin. Consider the following two functions for calculating \\[d_i = x_{i + 1} - x_i, \\hspace{2cm} i = 1, \\ldots, n - 1\\] where \\({\\bf x}&#39; = (x_1, \\ldots, x_n)\\). diff1 &lt;- function(x) { # function to calculate differences of a vector # based on a for loop # x is a vector # returns a vector of length (length(x) - 1) out &lt;- numeric(length(x) - 1) for (i in 1:(length(x) - 1)) { out[i] &lt;- x[i + 1] - x[i] } out } diff2 &lt;- function(x) { # function to calculate differences of a vector # based on vectorisation # x is a vector # returns a vector of length (length(x) - 1) id &lt;- 1:(length(x) - 1) x[id + 1] - x[id] } The first, diff1() uses a straightforward for() loop to calculate \\(d_i\\), for \\(i = 1, \\ldots, n - 1\\), whereas diff2() could be seen to be a vectorised alternative. Benchmark the two for a vector of \\(n = 1000\\) iid \\(N(0, 1)\\) random variates by comparing the median difference in execution time. Solution x &lt;- rnorm(1000) microbenchmark::microbenchmark( diff1(x), diff2(x) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## diff1(x) 44.083 45.1445 87.45188 46.1290 48.7905 3976.741 100 ## diff2(x) 6.919 7.3005 24.36390 7.5885 7.9920 1617.930 100 The following function assesses whether all elements is a logical vector are TRUE. all2 &lt;- function(x) { # function to calculate whether all elements are TRUE # returns a scalar # x is a logical vector sum(x) == length(x) } Calculate the following and benchmark all2() against R’s built-in function all(), which does the same. n &lt;- 1e4 x1 &lt;- !logical(n) Solution all2(x1) ## [1] TRUE all(x1) ## [1] TRUE microbenchmark::microbenchmark( all2(x1), all(x1) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## all2(x1) 4.445 4.7475 15.45914 4.7815 4.7980 1076.426 100 ## all(x1) 6.501 6.7120 6.91093 6.9790 7.0145 8.661 100 We see that both take a similar amount of time. Now swap the first element of x1 so that it’s FALSE and repeat the benchmarking. Solution x1[1] &lt;- FALSE all2(x1) ## [1] FALSE all(x1) ## [1] FALSE microbenchmark::microbenchmark( all2(x1), all(x1) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## all2(x1) 4394 4418 4524.48 4430 4457.0 10178 100 ## all(x1) 95 109 133.22 116 124.5 1043 100 Now all2() is much slower. This is because it’s performed a calculation on the entire x1 vector, whereas all() has stopped as soon as it’s found a FALSE. Evaluate the function any2() below against R’s built-in function any() similarly. any2 &lt;- function(x) { # function to calculate whether any elements are TRUE # returns a scalar # x is a logical vector sum(x) &gt; 0 } Solution x2 &lt;- logical(n) any2(x2) ## [1] FALSE any(x2) ## [1] FALSE microbenchmark::microbenchmark( any2(x2), any(x2) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## any2(x2) 4.758 4.9995 18.76494 5.8260 6.9285 1270.349 100 ## any(x2) 7.110 7.4640 9.01157 8.3625 9.3095 23.786 100 We again see that both take a similar amount of time. Now swap the first element of x2 so that it’s FALSE and repeat the benchmarking. Solution x2[1] &lt;- TRUE microbenchmark::microbenchmark( any2(x2), any(x2) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## any2(x2) 4461 5108.5 5603.99 5720.5 6110.5 8951 100 ## any(x2) 94 131.0 229.84 201.5 239.0 3637 100 This time any2() is much slower, for similar reasoning to all2() being much slower than all(), except that any() is stopped when it reaches a TRUE. "],["chapter-3-exercises.html", "3 Chapter 3 exercises", " 3 Chapter 3 exercises Calculate \\(\\mathbf{A}^{\\text{T}}\\mathbf{B}\\) in R where \\(\\mathbf{A}\\) is a \\(n \\times p\\) matrix comprising N(0, 1) random variates and \\(\\mathbf{B}\\) is a \\(n \\times n\\) matrix comprising Uniform([0, 1]) random variates for \\(n = 1000\\) and \\(p = 500\\), using t(A) %*% B and crossprod(A, B). Confirm that both produce the same result. Solution A &lt;- matrix(rnorm(n * p), n) B &lt;- matrix(runif(n * n), n) all.equal(t(A) %*% B, crossprod(A, B)) ## [1] TRUE Then benchmark the time that two commands take to complete. Solution microbenchmark::microbenchmark( t(A) %*% B, crossprod(A, B) ) ## Unit: milliseconds ## expr min lq mean median uq max neval ## t(A) %*% B 25.01682 27.64878 29.89909 28.47620 31.34298 75.16782 100 ## crossprod(A, B) 22.84685 25.38421 28.51215 26.73809 28.94669 102.07641 100 Consider the calculation \\(\\mathbf{AD}\\) where \\[ \\mathbf{A} = \\begin{pmatrix}0.25&amp;-1.65&amp;0.74&amp;1.07 \\\\-0.23&amp;0.84&amp;0.30&amp;-1.01 \\\\-0.47&amp;1.34&amp;-0.66&amp;0.01 \\\\1.02&amp;-0.16&amp;-0.67&amp;0.23 \\\\1.56&amp;1.04&amp;0.28&amp;0.00 \\\\0.26&amp;-1.47&amp;0.43&amp;-1.39 \\\\\\end{pmatrix} \\text{ and }\\mathbf{D} = \\begin{pmatrix}1.07&amp;0.00&amp;0.00&amp;0.00 \\\\0.00&amp;-0.28&amp;0.00&amp;0.00 \\\\0.00&amp;0.00&amp;-0.96&amp;0.00 \\\\0.00&amp;0.00&amp;0.00&amp;-1.80 \\\\\\end{pmatrix}. \\] Write a function in R that takes a matrix A and a vector d as its arguments and computes \\(\\mathbf{AD}\\), where \\(\\mathbf{A} =\\) A and \\(\\text{diag}(\\mathbf{D}) =\\) d, and where \\(\\text{diag}(\\mathbf{D})\\) denotes the vector comprising the diagonal elements of \\(\\mathbf{D}\\). Consider whether your function is performing redundant calculations and, if it is, try and avoid them. Solution We might start with AD &lt;- function(A, d) { # function to compute A * D # A is a matrix # D is a matrix with diagonal elements d # returns a matrix D &lt;- diag(d) A %*% D } but if we do this then we’re multiplying a lot of zeros unnecessarily. Instead, the following avoids this AdiagD &lt;- function(A, d) { # function to compute A * D slightly more efficiently # A is a matrix # D is a matrix with diagonal elements d # returns a matrix t(t(A) * d) } and the following confirms that both give the same result AD(A, diag(D)) ## [,1] [,2] [,3] [,4] ## [1,] 0.2675 0.4620 -0.7104 -1.926 ## [2,] -0.2461 -0.2352 -0.2880 1.818 ## [3,] -0.5029 -0.3752 0.6336 -0.018 ## [4,] 1.0914 0.0448 0.6432 -0.414 ## [5,] 1.6692 -0.2912 -0.2688 0.000 ## [6,] 0.2782 0.4116 -0.4128 2.502 AdiagD(A, diag(D)) ## [,1] [,2] [,3] [,4] ## [1,] 0.2675 0.4620 -0.7104 -1.926 ## [2,] -0.2461 -0.2352 -0.2880 1.818 ## [3,] -0.5029 -0.3752 0.6336 -0.018 ## [4,] 1.0914 0.0448 0.6432 -0.414 ## [5,] 1.6692 -0.2912 -0.2688 0.000 ## [6,] 0.2782 0.4116 -0.4128 2.502 The following function generates an arbitrary \\(n \\times n\\) positive definite matrix. pdmatrix &lt;- function(n) { # function to generate an arbitrary n x n positive definite matrix # n is an integer # returns a matrix L &lt;- matrix(0, n, n) L[!lower.tri(L)] &lt;- abs(rnorm(n * (n + 1) / 2)) tcrossprod(L) } By generating random \\(n\\)-vectors of independent N(0, 1) random variates, \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_m,\\), say, and random \\(n \\times n\\) positive definite matrices, \\(\\mathbf{A}_1, \\ldots, \\mathbf{A}_m\\), say, confirm that \\(\\mathbf{x}_i^{\\text{T}} \\mathbf{A}_i \\mathbf{x}_i &gt; 0\\) for \\(i = 1, \\ldots, m\\) with \\(m = 100\\) and \\(n = 10\\). [Note that this can be considered a simulation-based example of trying to prove a result by considering a large number of simulations. Such an approach can be very valuable when an analytical approach is not possible.] Solution There are a variety of ways we can tackle this. One of the tidier seems to be to use all() and replicate(). check_pd &lt;- function(A, x) { # function to check whether a matrix is positive definite # A is a matrix # returns a logical sum(x * (A %*% x)) &gt; 0 } m &lt;- 1e2 n &lt;- 10 all(replicate(1e2, check_pd(pdmatrix(n), rnorm(n)) )) ## [1] TRUE For the cement factory data of Example 3.25 compute \\(\\hat {\\boldsymbol \\beta}\\) by inverting \\({\\bf X}^{\\text{T}} {\\bf X}\\) and multiplying by \\({\\bf X}^{\\text{T}} {\\bf y}\\), i.e. \\(\\hat {\\boldsymbol \\beta} = ({\\bf X}^{\\text{T}} {\\bf X})^{-1} {\\bf X}^{\\text{T}} {\\bf y}\\), and by solving \\({\\bf X}^{\\text{T}} {\\bf X} \\hat {\\boldsymbol \\beta} = {\\bf X}^{\\text{T}} {\\bf y}\\). Solution X &lt;- cbind(1, prod$days, prod$temp) XtX &lt;- crossprod(X) y &lt;- prod$output Xty &lt;- crossprod(X, y) (betahat1 &lt;- solve(XtX) %*% Xty) ## [,1] ## [1,] 9.12688541 ## [2,] 0.20281539 ## [3,] -0.07239294 (betahat2 &lt;- solve(XtX, Xty)) ## [,1] ## [1,] 9.12688541 ## [2,] 0.20281539 ## [3,] -0.07239294 Show in R that \\(\\mathbf{L}\\) is a Cholesky decomposition of \\(\\mathbf{A}\\) for \\[ \\mathbf{A} = \\begin{pmatrix}547.56&amp;348.66&amp;306.54 \\\\348.66&amp;278.26&amp;199.69 \\\\306.54&amp;199.69&amp;660.38 \\\\\\end{pmatrix} \\text{ and }\\mathbf{L} = \\begin{pmatrix}23.4&amp;0.0&amp;0.0 \\\\14.9&amp;7.5&amp;0.0 \\\\13.1&amp;0.6&amp;22.1 \\\\\\end{pmatrix}. \\] Solution We’ll load \\(\\mathbf{A}\\) and \\(\\mathbf{L}\\) as A and L, respectively. A &lt;- matrix(c(547.56, 348.66, 306.54, 348.66, 278.26, 199.69, 306.54, 199.69, 660.38), 3, 3) L &lt;- matrix(c(23.4, 14.9, 13.1, 0, 7.5, 0.6, 0, 0, 22.1), 3, 3) Then we need to show that \\(\\mathbf{L}\\) is lower-triangular all(L[upper.tri(L)] == 0) ## [1] TRUE which it is, that all its diagonal elements are positive all(diag(L) &gt; 0) ## [1] TRUE which they are, and that \\(\\mathbf{A} = \\mathbf{LL}^{\\text{T}}\\) all.equal(A, tcrossprod(L)) ## [1] TRUE which it does. For the matrix \\(\\mathbf{A}\\) below, find its Cholesky decomposition, \\(\\mathbf{L}\\), where \\(\\mathbf{A} = \\mathbf{LL}^\\text{T}\\) and \\(\\mathbf{L}\\) is a lower triangular matrix, and confirm that \\(\\mathbf{L}\\) is a Cholesky decomposition of \\(\\mathbf{A}\\): \\[\\mathbf{A} = \\begin{pmatrix}0.797&amp;0.839&amp;0.547 \\\\0.839&amp;3.004&amp;0.855 \\\\0.547&amp;0.855&amp;3.934 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\) A &lt;- cbind( c(0.797, 0.839, 0.547), c(0.839, 3.004, 0.855), c(0.547, 0.855, 3.934) ) and then we’ll find \\(\\mathbf{L}\\) L &lt;- t(chol(A)) L ## [,1] [,2] [,3] ## [1,] 0.8927486 0.0000000 0.000000 ## [2,] 0.9397943 1.4562921 0.000000 ## [3,] 0.6127145 0.1917022 1.876654 Then we’ll check that it’s lower-triangular all(L[upper.tri(L)] == 0) ## [1] TRUE which it is, then we’ll check that its diagonal elements are positive all(diag(L) &gt; 0) ## [1] TRUE which they are, and finally we’ll check that \\(\\mathbf{LL}^\\text{T} = \\mathbf{A}\\) all.equal(A, tcrossprod(L)) ## [1] TRUE which it does. So we conclude that \\(\\mathbf{L}\\) is a lower-triangular Cholesky decomposition of \\(\\mathbf{A}\\). Write a function in R called solve_chol() to solve a system of linear equations \\(\\mathbf{Ax} = \\mathbf{b}\\) based on the Cholesky decomposition \\(\\mathbf{A} = \\mathbf{LL}^{\\text{T}}\\). Solution The following is one option for solve_chol(). solve_chol &lt;- function(L, b) { # Function to solve LL^Tx = b for x # L is a lower-triangular matrix # b is a vector of length nrow(L) # return vector of same length as b y &lt;- forwardsolve(L, b) backsolve(t(L), y) } We’ll quickly check that we get the same result as solve() using the data from Example 3.2. y &lt;- c(.7, 1.3, 2.6) mu &lt;- 1:3 Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) res1 &lt;- solve(Sigma, y - mu) L &lt;- t(chol(Sigma)) all.equal(res1, solve_chol(L, y - mu)) ## [1] TRUE Note above that we could use backsolve(L, y, upper.tri = FALSE, transpose = TRUE) instead of backsolve(t(L), y), which avoids transposing L. Both give the same result, though. Show that solving \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) is equivalent to solving \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\) and then \\(\\mathbf{L}^{\\text{T}}\\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\) if \\(\\mathbf{A}\\) has Cholesky decomposition \\(\\mathbf{A} = \\mathbf{L} \\mathbf{L}^{\\text{T}}\\). Confirm this based on the cement factory data of Example 3.25 by taking \\(\\mathbf{A} = \\mathbf{X}^{\\text{T}}\\mathbf{X}\\), where \\(\\mathbf{X}\\) is the linear model’s design matrix. Solution Let \\(\\mathbf{L}^{\\text{T}} \\mathbf{x} = \\mathbf{y}\\). To solve \\(\\mathbf{LL}^{\\text{T}} \\mathbf{x} = \\mathbf{b}\\) for \\(\\mathbf{x}\\), we want to first solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\), and then \\(\\mathbf{L}^{\\text{T}} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\). We can confirm this numerically in R. We already have X from Question \\(\\ref{cement}\\), so we’ll re-use that X. We’ll set \\(\\mathbf{A} = \\mathbf{X}^{\\text{T}} \\mathbf{X}\\), and call this A. We’ll then use chol() to calculate its Cholesky decomposition in upper-triangular form, U, and lower-triangular form, L. A &lt;- crossprod(X) U &lt;- chol(A) L &lt;- t(U) The following two commands then solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\), and then \\(\\mathbf{L}^{\\text{T}} \\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\) cbind( solve(t(L), solve(L, Xty)), backsolve(t(L), forwardsolve(L, Xty)) ) ## [,1] [,2] ## [1,] 9.12688541 9.12688541 ## [2,] 0.20281539 0.20281539 ## [3,] -0.07239294 -0.07239294 although double use of solve() is inefficient compared to using forwardsolve() and then backsolve(). Alternatively, if we have \\(\\mathbf{A} = \\mathbf{U}^{\\text{T}} \\mathbf{U}\\), for upper-triangular \\(\\mathbf{U}\\), then we have the following two options cbind( backsolve(U, forwardsolve(t(U), Xty)), backsolve(U, forwardsolve(U, Xty, upper.tri = TRUE, transpose = TRUE)) ) ## [,1] [,2] ## [1,] 9.12688541 9.12688541 ## [2,] 0.20281539 0.20281539 ## [3,] -0.07239294 -0.07239294 the latter of which is ever so slightly more efficient for its avoidance of t(U). Show that \\(\\mathbf{U}\\) and \\(\\boldsymbol{\\Lambda}\\) form an eigen-decomposition of \\(\\mathbf{A}\\) for \\[\\mathbf{A} = \\begin{pmatrix}3.40&amp;0.00&amp;0.00 \\\\0.00&amp;0.15&amp;-2.06 \\\\0.00&amp;-2.06&amp;-1.05 \\\\\\end{pmatrix},~~\\mathbf{U} = \\begin{pmatrix}1.0&amp;0.0&amp;0.0 \\\\0.0&amp;0.6&amp;-0.8 \\\\0.0&amp;0.8&amp;0.6 \\\\\\end{pmatrix} \\text{ and } \\boldsymbol{\\Lambda} = \\begin{pmatrix}3.4&amp;0.0&amp;0.0 \\\\0.0&amp;-2.6&amp;0.0 \\\\0.0&amp;0.0&amp;1.7 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\), \\(\\boldsymbol{\\Lambda}\\) and \\(\\mathbf{U}\\) as A, Lambda and U, respectively. A &lt;- cbind( c(3.4, 0, 0), c(0, .152, -2.064), c(0, -2.064, -1.052) ) Lambda &lt;- diag(c(3.4, -2.6, 1.7)) U &lt;- matrix(c(1, 0, 0, 0, .6, .8, 0, -.8, .6), 3, 3) Then we need to show that \\(\\mathbf{U}\\) is orthogonal, all.equal(crossprod(U), diag(nrow(U))) ## [1] TRUE which it is, that \\(\\boldsymbol{\\Lambda}\\) is diagonal, all(Lambda - diag(diag(Lambda)) == 0) ## [1] TRUE and that \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\) all.equal(A, U %*% tcrossprod(Lambda, U)) ## [1] TRUE which it does. For the matrix \\(\\mathbf{A}\\) below, find \\(\\mathbf{U}\\) and \\(\\boldsymbol{\\Lambda}\\) in its eigen-decomposition of the form \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T}\\)m where \\(\\mathbf{U}\\) is orthogonal and \\(\\boldsymbol{\\Lambda}\\) is diagonal: \\[\\mathbf{A} = \\begin{pmatrix}0.797&amp;0.839&amp;0.547 \\\\0.839&amp;3.004&amp;0.855 \\\\0.547&amp;0.855&amp;3.934 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\) A &lt;- cbind( c(0.797, 0.839, 0.547), c(0.839, 3.004, 0.855), c(0.547, 0.855, 3.934) ) and then find \\(\\mathbf{U}\\) and \\(\\boldsymbol{\\Lambda}\\) eA &lt;- eigen(A, symmetric = TRUE) U &lt;- eA$vectors U ## [,1] [,2] [,3] ## [1,] -0.2317139 -0.1941590 0.95321085 ## [2,] -0.5372074 -0.7913728 -0.29178287 ## [3,] -0.8109975 0.5796821 -0.07906848 Lambda &lt;- diag(eA$values) Lambda ## [,1] [,2] [,3] ## [1,] 4.656641 0.000000 0.0000000 ## [2,] 0.000000 2.583555 0.0000000 ## [3,] 0.000000 0.000000 0.4948042 Then we need to show that \\(\\mathbf{U}\\) is orthogonal, all.equal(crossprod(U), diag(nrow(U))) ## [1] TRUE which it is, that \\(\\boldsymbol{\\Lambda}\\) is diagonal, all(Lambda - diag(diag(Lambda)) == 0) ## [1] TRUE which it is, and that \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\) all.equal(A, U %*% tcrossprod(Lambda, U)) ## [1] TRUE which it does. Show that \\(\\mathbf{U}\\), \\(\\mathbf{D}\\) and \\(\\mathbf{V}\\) form a singular value decomposition of \\(\\mathbf{A}\\) for \\[\\mathbf{A} = \\begin{pmatrix}0.185&amp;8.700&amp;-0.553 \\\\0.555&amp;-2.900&amp;-1.659 \\\\3.615&amp;8.700&amp;-2.697 \\\\1.205&amp;-26.100&amp;-0.899 \\\\\\end{pmatrix},~~\\mathbf{U} = \\begin{pmatrix}-0.2&amp;-0.2&amp;1.0 \\\\-0.5&amp;-0.8&amp;-0.3 \\\\-0.8&amp;0.6&amp;-0.1 \\\\\\end{pmatrix},\\] \\[\\mathbf{D} = \\begin{pmatrix}29&amp;0&amp;0 \\\\0&amp;5&amp;0 \\\\0&amp;0&amp;1 \\\\\\end{pmatrix} \\text{ and } \\mathbf{V} = \\begin{pmatrix}0.00&amp;0.76&amp;0.65 \\\\-1.00&amp;0.00&amp;0.00 \\\\0.00&amp;-0.65&amp;0.76 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\), \\(\\mathbf{U}\\), \\(\\mathbf{D}\\) and \\(\\mathbf{V}\\) as A, U, D and V, respectively. A &lt;- matrix(c(0.185, 0.555, 3.615, 1.205, 8.7, -2.9, 8.7, -26.1, -0.553, -1.659, -2.697, -0.899), 4, 3) U &lt;- matrix(c(-0.3, 0.1, -0.3, 0.9, 0.1, 0.3, 0.9, 0.3, -0.3, -0.9, 0.3, 0.1), 4, 3) D &lt;- diag(c(29, 5, 1)) V &lt;- matrix(c(0, -1, 0, 0.76, 0, -0.65, 0.65, 0, 0.76), 3, 3) We want to check that \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal all.equal(crossprod(U), diag(ncol(U))) ## [1] TRUE all.equal(crossprod(V), diag(nrow(V))) ## [1] &quot;Mean relative difference: 9.999e-05&quot; which they both are, that \\(\\mathbf{D}\\) is diagonal all(D - diag(diag(D)) == 0) ## [1] TRUE all.equal(crossprod(V), diag(nrow(V))) ## [1] &quot;Mean relative difference: 9.999e-05&quot; which it is, and finally that \\(\\mathbf{A} = \\mathbf{UDV}^{\\text{T}}\\) all.equal(A, U %*% tcrossprod(D, V)) ## [1] TRUE which is true. By considering \\(\\sqrt{\\mathbf{A}}\\) as \\(\\mathbf{A}^{1/2}\\), i.e. as a matrix power, show how an eigen-decomposition can be used to general multivariate Normal random vectors and then write a function to implement this in R. Solution From Example 3.14, to generate a multivariate Normal random vector, \\(\\mathbf{Y}\\), say, from the \\(MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) distribution we need to compute \\(\\mathbf{Y} = \\boldsymbol{\\mu} + \\mathbf{L} \\mathbf{Z}\\), where \\(\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^{\\text{T}}\\) and \\(\\mathbf{Z} = (Z_1, \\ldots, Z_p)^{\\text{T}}\\), where \\(Z_i\\), \\(i = 1, \\ldots, p\\), are independent \\(N(0, 1)\\) random variables. Given an eigen-decomposition of \\({\\boldsymbol \\Sigma} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\), we can write this as \\({\\boldsymbol \\Sigma} = \\mathbf{U} \\boldsymbol{\\Lambda}^{1/2} \\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^{\\text{T}}\\). As \\(\\boldsymbol{\\Lambda}\\) is diagonal, \\(\\boldsymbol{\\Lambda} = \\boldsymbol{\\Lambda}^{\\text{T}}\\) and hence \\(\\boldsymbol{\\Lambda}^{1/2} \\mathbf{U}^{\\text{T}} = (\\mathbf{U} \\boldsymbol{\\Lambda}^{1/2})^{\\text{T}}\\) and so \\({\\boldsymbol \\Sigma} = \\mathbf{LL}^{\\text{T}}\\) if \\(\\mathbf{L} = \\mathbf{U} \\boldsymbol{\\Lambda}^{1/2}\\). Therefore we can generate \\(MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) random variables with \\(\\mathbf{Y} = \\boldsymbol{\\mu} + \\mathbf{U} \\boldsymbol{\\Lambda}^{\\text{1/2}} \\mathbf{Z}\\). The following R function generates n multivariate Normal random vectors with mean mu and variance-covariance matrix Sigma. rmvn_eigen &lt;- function(n, mu, Sigma) { # Function to generate MVN random vectors with # n is a integer, giving the number of independent vectors to simulate # mu is a p-vector of the MVN mean # Sigma is a p x p matrix of the MVN variance-covariance matrix # returns a p times n matrix eS &lt;- eigen(Sigma, symmetric = TRUE) p &lt;- nrow(Sigma) Z &lt;- matrix(rnorm(p * n), p) mu + eS$vectors %*% diag(sqrt(eS$values)) %*% Z } rmvn_eigen(5, mu, Sigma) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3.812779 1.328476 4.819841 -1.4787812 -0.6061167 ## [2,] 5.302633 3.159313 3.723752 -0.1035905 -0.5120484 ## [3,] 5.310861 4.700222 3.865721 2.8249234 0.7614778 Show how an eigen-decomposition can be used to solve a system of linear equations \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) by matrix multiplications and vector divisions, only. Confirm this in R by solving \\(\\boldsymbol{\\Sigma} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\) for \\(\\mathbf{z}\\), with \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2. Solution To solve \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\), if \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}}\\), then we want to solve \\(\\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}} \\mathbf{x} = \\mathbf{b}\\) for \\(\\mathbf{x}\\). The following manipulations can be used \\[\\begin{align} \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}} \\mathbf{x} &amp;= \\mathbf{U}^{-1} \\mathbf{b} \\tag{3.1}\\\\ \\boldsymbol{\\Lambda} \\mathbf{U}^{\\text{T}} \\mathbf{x} &amp;= \\mathbf{U}^{\\text{T}} \\mathbf{b} \\tag{3.2} \\\\ \\mathbf{U}^{\\text{T}} \\mathbf{x} &amp;= (\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\boldsymbol{\\Lambda}) \\tag{3.3} \\\\ \\mathbf{x} &amp;= \\mathbf{U}^{-\\text{T}}(\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\boldsymbol{\\Lambda}) \\tag{3.4} \\\\ \\mathbf{x} &amp;= \\mathbf{U} [(\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\boldsymbol{\\Lambda})] \\tag{3.5} \\end{align}\\] in which (3.1) results from premultiplying by \\(\\mathbf{U}^{\\text{-1}}\\), (3.2) from orthogonality of \\(\\mathbf{U}\\), i.e. \\(\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}\\), (3.3) from elementwise division, given diagonal \\(\\boldsymbol{\\Lambda}\\), (3.4) from premultiplying by \\(\\mathbf{U}^{-\\text{T}}\\) and (3.5) from orthogonality of \\(\\mathbf{U}\\), again. Next we’ll load the data from Example 3.2 y &lt;- c(.7, 1.3, 2.6) mu &lt;- 1:3 Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) res1 &lt;- solve(Sigma, y - mu) Then we’ll go through the calculations given above eS &lt;- eigen(Sigma, symmetric = TRUE) lambda &lt;- eS$values U &lt;- eS$vectors res2 &lt;- U %*% (crossprod(U, y - mu) / lambda) which we see gives the same result as solve(), once we use as.vector() to convert res2 from a one-column matrix to a vector. all.equal(res1, as.vector(res2)) ## [1] TRUE Show in R that if \\(\\mathbf{H}_{4} = \\mathbf{UDV}^{\\text{T}}\\) is the SVD of \\(\\mathbf{H}_4\\), the \\(4 \\times 4\\) Hilbert matrix, then solving \\(\\mathbf{H}_4\\mathbf{x} = (1, 1, 1, 1)^{\\text{T}}\\) for \\(\\mathbf{x}\\) reduces to solving \\[ \\mathbf{V}^{\\text{T}} \\mathbf{x} \\simeq \\begin{pmatrix}-1.21257 \\\\-4.80104 \\\\-26.08668 \\\\234.33089 \\\\\\end{pmatrix} \\] and then use this to solve \\(\\mathbf{H}_4\\mathbf{x} = (1, 1, 1, 1)^{\\text{T}}\\) for \\(\\mathbf{x}\\). Solution We ultimately need to solve \\(\\mathbf{UDV}^{\\text{T}} \\mathbf{x} = \\mathbf{b}\\), where \\(\\mathbf{b} = (1, 1, 1, 1)^{\\text{T}}\\). Pre-multiplying by \\(\\mathbf{U}^{-1} = \\mathbf{U}^{\\text{T}}\\), we then need to solve \\(\\mathbf{DV}^{\\text{T}} \\mathbf{x} = \\mathbf{U}^{\\text{T}} \\mathbf{b}\\), which is equivalent to solving \\(\\mathbf{V}^{\\text{T}} \\mathbf{x} = (\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\mathbf{D})\\). The following calculates the SVD of \\(\\mathbf{H}_4\\) and then computes \\((\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\mathbf{D})\\). hilbert &lt;- function(n) { # Function to evaluate n by n Hilbert matrix. # n is an integer # Returns n by n matrix. ind &lt;- 1:n 1 / (outer(ind, ind, FUN = &#39;+&#39;) - 1) } H4 &lt;- hilbert(4) b &lt;- rep(1, 4) svdH &lt;- svd(H4) V &lt;- svdH$v U &lt;- svdH$u b2 &lt;- crossprod(U, b) z &lt;- b2 / svdH$d z ## [,1] ## [1,] -1.212566 ## [2,] -4.801038 ## [3,] -26.086677 ## [4,] 234.330888 which is as given in the question, subject to rounding. Finally we want to solve \\(\\mathbf{V}^{\\text{T}} \\mathbf{x} = (\\mathbf{U}^{\\text{T}} \\mathbf{b}) / \\text{diag}(\\mathbf{D})\\) for \\(\\mathbf{x}\\), which we can do with either of the following cbind( solve(t(V), z), V %*% z ) ## [,1] [,2] ## [1,] -4 -4 ## [2,] 60 60 ## [3,] -180 -180 ## [4,] 140 140 and we see that the latter gives the same result as solve() all.equal(solve(H4, b), as.vector(V %*% z)) ## [1] TRUE once we ensure that both are vectors. Note that solving systems of linear equations via the SVD is only a sensible option if we already have the SVD. Otherwise, solving via other decompositions is more efficient. Show that \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) form a QR decomposition of \\(\\mathbf{A}\\) for \\[\\mathbf{A} = \\begin{pmatrix}0.337&amp;0.890&amp;-1.035 \\\\0.889&amp;6.070&amp;-1.547 \\\\-1.028&amp;-1.545&amp;4.723 \\\\\\end{pmatrix},~~\\mathbf{Q} = \\begin{pmatrix}-0.241&amp;0.101&amp;0.965 \\\\-0.635&amp;-0.769&amp;-0.078 \\\\0.734&amp;-0.631&amp;0.249 \\\\\\end{pmatrix}\\] and \\[\\mathbf{R} = \\begin{pmatrix}-1.4&amp;-5.2&amp;4.7 \\\\0.0&amp;-3.6&amp;-1.9 \\\\0.0&amp;0.0&amp;0.3 \\\\\\end{pmatrix}.\\] Solution We’ll load \\(\\mathbf{A}\\), \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) as A, Q and R, respectively. A &lt;- matrix(c(0.3374, 0.889, -1.0276, 0.8896, 6.0704, -1.5452, -1.0351, -1.5468, 4.7234), 3, 3) Q &lt;- matrix(c(-0.241, -0.635, 0.734, 0.101, -0.769, -0.631, 0.965, -0.078, 0.249), 3, 3) R &lt;- matrix(c(-1.4, 0, 0, -5.2, -3.6, 0, 4.7, -1.9, 0.3), 3, 3) We want to show that \\(\\mathbf{Q}\\) is orthogonal all.equal(crossprod(Q), diag(nrow(Q))) ## [1] &quot;Mean relative difference: 0.001286839&quot; which it is (after allowing for a bit of error), that \\(\\mathbf{R}\\) is upper-triangular all(R[lower.tri(R)] == 0) ## [1] TRUE which it is, and that \\(\\mathbf{A} = \\mathbf{QR}\\) all.equal(A, Q %*% R) ## [1] TRUE which it does. The determinant of the \\(n \\times n\\) Hilbert matrix is given by \\[ |\\mathbf{H}_n| = \\dfrac{c_n^4}{c_{2n}} \\] where \\[ c_n = \\prod_{i}^{n - 1} = i! \\] is the Cauchy determinant. Write a function in R, det_hilbert(n, log = FALSE) that evaluates \\(|\\mathbf{H}_n|\\) and \\(\\log(|\\mathbf{H}_n|)\\) if log = FALSE or log = TRUE, respectively. Your function should compute \\(\\log(|\\mathbf{H}_n|)\\), and then return \\(|\\mathbf{H}_n|\\) if log = FALSE, as with dmvn1() in Example 3.2. Solution It will perhaps be tidier to write a function to calculate the Cauchy determinant det_cauchy &lt;- function(n, log = FALSE) { # function to calculate Cauchy determinant # n in an integer # log is a logical; defaults to FALSE # returns a scalar out &lt;- sum(lfactorial(seq_len(n - 1))) if (!log) out &lt;- exp(out) out } and then to use that to calculate the determinant of \\(\\mathbf{H}_n\\). det_hilbert &lt;- function(n, log = FALSE) { # function to calculate determinant of Hilbert matrix # n in an integer # log is a logical; defaults to FALSE # returns a scalar out &lt;- 4 * det_cauchy(n, TRUE) - det_cauchy(2 * n, TRUE) if (!log) out &lt;- exp(out) out } Calculate \\(|\\mathbf{H}_n|\\) and \\(\\log(|\\mathbf{H}_n|)\\) through the QR decomposition of \\(\\mathbf{H}_n\\) for \\(n = 5\\) and confirm that both give the same result as det_hilbert() above. Solution We’ll start with a function det_QR(), which calculates the determinant of a matrix via its QR decomposition det_QR &lt;- function(A) { # function to calculate determinant of a matrix via QR decomposition # A is a matrix # returns a scalar qrA &lt;- qr(A) R &lt;- qr.R(qrA) prod(abs(diag(R))) } and we see that this gives the same answer as det_hilbert() for \\(n = 5\\). det_QR(hilbert(5)) ## [1] 3.749295e-12 det_hilbert(5) ## [1] 3.749295e-12 Then we’ll write a function to calculate the logarithm of the determinant of a matrix through its QR decomposition. logdet_QR &lt;- function(A) { # function to calculate log determinant of a matrix via QR decomposition # A is a matrix # returns a scalar qrA &lt;- qr(A) R &lt;- qr.R(qrA) sum(log(abs(diag(R)))) } which we also see gives the same answer as det_hilbert() with log = TRUE. logdet_QR(hilbert(5)) ## [1] -26.30945 det_hilbert(5, log = TRUE) ## [1] -26.30945 Compute \\(\\mathbf{H}_4^{-1}\\) via its QR decomposition, and confirm your result with solve() and qr.solve(). Solution If \\(\\mathbf{H}_4 = \\mathbf{QR}\\) then \\(\\mathbf{H}_4^{-1} = (\\mathbf{QR})^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^{\\text{T}}\\), since \\(\\mathbf{Q}\\) is orthogonal. Therefore we want to solve \\(\\mathbf{R} \\mathbf{X} = \\mathbf{Q}^{\\text{T}}\\) for \\(\\mathbf{X}\\). The following calculates the QR decomposition of \\(\\mathbf{H}_4\\) with qr() and then extracts \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) as Q and R, respectively. H4 &lt;- hilbert(4) qrH &lt;- qr(H4) Q &lt;- qr.Q(qrH) R &lt;- qr.R(qrH) Then we want to solve \\(\\mathbf{R} \\mathbf{X} = \\mathbf{Q}^{\\text{T}}\\) for \\(\\mathbf{X}\\), which we do with backsolve(), since \\(\\mathbf{R}\\), stored as R, is upper-triangular. X1 &lt;- backsolve(R, t(Q)) The following use solve() and qr.solve(). Note that if we already have the QR decomposition from qr(), then qr.solve() uses far fewer calculations to obtain the inverse. X2 &lt;- solve(H4) X3 &lt;- qr.solve(qrH) We see that both give the same answer all.equal(X2, X3) ## [1] TRUE and also the same answer as with backsolve() above all.equal(X1, X2) ## [1] TRUE Benchmark Cholesky, eigen (with symmetric = TRUE and symmetric = FALSE), singular value and QR decompositions of \\(\\mathbf{A} = \\mathbf{I}_{100} + \\mathbf{H}_{100}\\), where \\(\\mathbf{H}_{100}\\) is the \\(100 \\times 100\\) Hilbert matrix. (If you’re feeling impatient, consider reducing the value of argument times for function microbenchmark::microbenchmark().) Solution hilbert &lt;- function(n) { # Function to evaluate n by n Hilbert matrix. # n is an integer # Returns n by n matrix. ind &lt;- 1:n 1 / (outer(ind, ind, FUN = &#39;+&#39;) - 1) } H100 &lt;- hilbert(1e2) + diag(1, 1e2) microbenchmark::microbenchmark( chol(H100), eigen(H100, symmetric = TRUE), eigen(H100), svd(H100), qr(H100), times = 1e2 ) ## Unit: microseconds ## expr min lq mean median uq ## chol(H100) 66.809 84.2275 166.2587 92.1795 108.1305 ## eigen(H100, symmetric = TRUE) 961.128 1249.9195 1657.7332 1375.6110 1557.2425 ## eigen(H100) 1326.503 1803.9120 2304.3965 1919.4595 2143.8665 ## svd(H100) 1228.792 1728.2755 2149.2000 1873.2510 2051.8125 ## qr(H100) 201.741 302.7470 423.6967 362.3305 429.8370 ## max neval ## 2455.016 100 ## 9464.479 100 ## 17990.564 100 ## 15323.668 100 ## 4043.018 100 If we consider median computation times, we see that the Cholesky decomposition is quickest, at nearly six times quicker than the QR decomposition, which is next quickest. The QR decomposition is then about just under three times quicker than the symmetric eigen-decomposition, which takes about the same amount of time as the singular value decomposition. The asymmetric eigen-decomposition is slowest, and demonstrates that if we know the matrix we want an eigen-decomposition of is symmetric, then we should pass this information to R. Remark. The following shows us that if we only want the eigenvalues of a symmetric matrix, then we can further save times by specifying only.values = TRUE. microbenchmark::microbenchmark( eigen(H100, symmetric = TRUE), eigen(H100, symmetric = TRUE, only.values = TRUE), times = 1e2 ) ## Unit: microseconds ## expr min lq mean ## eigen(H100, symmetric = TRUE) 1212.621 1333.0995 1580.2325 ## eigen(H100, symmetric = TRUE, only.values = TRUE) 539.388 621.2895 758.8781 ## median uq max neval ## 1424.1850 1545.2805 5091.164 100 ## 675.5365 750.6165 3469.992 100 Given that \\[\\mathbf{A} = \\begin{pmatrix}1.23&amp;0.30&amp;2.58 \\\\0.30&amp;0.43&amp;1.92 \\\\2.58&amp;1.92&amp;10.33 \\\\\\end{pmatrix},~\\mathbf{A}^{-1} = \\begin{pmatrix}6.9012&amp;16.9412&amp;-4.8724 \\\\16.9412&amp;55.2602&amp;-14.5022 \\\\-4.8724&amp;-14.5022&amp;4.0092 \\\\\\end{pmatrix},\\] \\[\\mathbf{B} = \\begin{pmatrix}1.49&amp;0.40&amp;2.76 \\\\0.40&amp;0.53&amp;2.16 \\\\2.76&amp;2.16&amp;11.20 \\\\\\end{pmatrix},\\] and that \\(\\mathbf{B} = \\mathbf{A} + \\mathbf{LL}^{\\text{T}}\\), where \\(\\mathbf{L}\\) is a lower-triangular matrix, find \\(\\mathbf{B}^{-1}\\) using Woodbury’s formula, i.e. using \\[(\\mathbf{A} + \\mathbf{UV}^{\\text{T}})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_n + \\mathbf{V}^{\\text{T}} \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^{\\text{T}} \\mathbf{A}^{-1}. \\] Solution We note that we can write \\(\\mathbf{U} = \\mathbf{V} = \\mathbf{L}\\) and that \\(\\mathbf{LL}^{\\text{T}} = \\mathbf{B} - \\mathbf{A}\\), and so we can obtain \\(\\mathbf{L}\\) via the Cholesky decomposition of \\(\\mathbf{B} - \\mathbf{A}\\). We’ll load \\(\\mathbf{A}\\), \\(\\mathbf{A}^{-1}\\) and \\(\\mathbf{B}\\) below as A, iA and B, and then compute the lower-triangular Cholesky decomposition of \\(\\mathbf{B} - \\mathbf{A}\\) and store this as L. A &lt;- matrix( c(1.23, 0.3, 2.58, 0.3, 0.43, 1.92, 2.58, 1.92, 10.33), 3, 3) iA &lt;- matrix( c(6.9012, 16.9412, -4.8724, 16.9412, 55.2602, -14.5022, -4.8724, -14.5022, 4.0092), 3, 3) B &lt;- matrix( c(1.49, 0.4, 2.76, 0.4, 0.53, 2.16, 2.76, 2.16, 11.2), 3, 3) L &lt;- t(chol(B - A)) Then we’ll write a function to implement Woodbury’s formula, woodbury &lt;- function(iA, U, V) { # function to implement Woodbury&#39;s formula # iA, U and V are matrices # returns a matrix I_n &lt;- diag(nrow(iA)) iAU &lt;- iA %*% U iA - iAU %*% solve(I_n + crossprod(V, iAU), crossprod(V, iA)) } and then use this to find \\(\\mathbf{B}^{-1}\\), which we’ll call iB, iB &lt;- woodbury(iA, L, L) and see that, subject to rounding, this is equal to \\(\\mathbf{B}^{-1}\\) all.equal(iB, solve(B)) ## [1] &quot;Mean relative difference: 9.113679e-06&quot; Recall the cement factory data of Example 3.25. Now suppose that a further observation has been obtained based on the factory operating at 20 degrees for 14 days. For given \\(\\sigma^2\\), the sampling distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) is \\(MVN_3(\\hat{\\boldsymbol{\\beta}}, \\sigma^{-2} (\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1})\\). Use the Sherman-Morrison formula to give an expression for the estimated standard errors of \\(\\hat{\\boldsymbol{\\beta}}\\) in terms of \\(\\sigma\\) given that \\[(\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1} = \\begin{pmatrix} 2.78 \\times 10^0 &amp; -1.12 \\times 10^{-2} &amp; -1.06 \\times 10^{-1}\\\\ -1.12 \\times 10^{-2} &amp; 1.46 \\times 10^{-4} &amp; 1.75 \\times 10^{-4}\\\\ -1.0 \\times 10^{-1} &amp; 1.75 \\times 10^{-4} &amp; 4.79 \\times 10^{-3} \\end{pmatrix}. \\] Solution If we refer to the Sherman-Morrison formula, we take \\(\\mathbf{A}^{-1} = (\\mathbf{X}^{\\text{T}}\\mathbf{X})^{-1}\\) and \\(\\mathbf{u}^{\\text{T}} = \\mathbf{v} = (1, 20, 14)\\). Then we can calculate the updated variance-covariance matrix of the sampling distribution of \\(\\hat{\\boldsymbol{\\beta}}\\) as \\[\\sigma^{-2} (\\mathbf{A} + \\mathbf{u} \\mathbf{v}^{\\text{T}})^{-1} = \\sigma^{-1} \\left[ \\mathbf{A}^{-1} - \\dfrac{\\mathbf{A}^{-1} \\mathbf{u} \\mathbf{v}^{\\text{T}} \\mathbf{A}^{-1}}{1 + \\mathbf{v}^{\\text{T}}\\mathbf{A}^{-1}\\mathbf{u}} \\right] \\] which can be calculated in R with V &lt;- matrix( c(2.78, -0.0112, -0.106, -0.0112, 0.000146, 0.000175, -0.106, 0.000175, 0.00479), 3, 3) u &lt;- c(1, 20, 14) V2 &lt;- V - (V %*% tcrossprod(u) %*% V) / (1 + crossprod(u, V %*% u))[1, 1] V2 ## [,1] [,2] [,3] ## [1,] 1.992477728 -6.917113e-03 -7.996475e-02 ## [2,] -0.006917113 1.227078e-04 3.340903e-05 ## [3,] -0.079964749 3.340903e-05 3.929282e-03 Taking the diagonal elements of V2 we get \\[ \\left(\\text{e.s.e.}(\\hat \\beta_0), \\text{e.s.e.}(\\hat \\beta_1), \\text{e.s.e.}(\\hat \\beta_2)\\right) = \\sigma^{-1} \\left( 1.412~,0.011~,0.063 \\right). \\] "],["chapter-4-exercises.html", "4 Chapter 4 exercises", " 4 Chapter 4 exercises For \\(f(x) = \\text{e}^{2x}\\) approximate \\(f&#39;(x)\\) by finite-differencing with \\(\\delta = 10^{-6}\\). Use \\(x = -1.0, -0.9, \\ldots, 0.9, 1.0\\) and find the difference between your approximations and the true derivatives. Solution The following calculates the finite-difference approximation to the derivative for \\(x = -1.0, -0.9, \\ldots, 0.9, 1.0\\) and \\(\\delta = 10^{-6}\\). x &lt;- seq(-1, 1, by = .1) delta &lt;- 1e-6 fd &lt;- (exp(2 * (x + delta)) - exp(2 * x)) / delta Then the following calculates the true derivative for the stated \\(x\\) values as true and calculates its difference from the finite-difference approximations. true &lt;- 2 * exp(2 * x) fd - true ## [1] 2.706593e-07 3.306135e-07 4.038067e-07 4.932252e-07 6.023919e-07 ## [6] 7.357217e-07 8.986189e-07 1.097651e-06 1.340685e-06 1.637477e-06 ## [11] 2.000013e-06 2.442704e-06 2.983658e-06 3.644212e-06 4.450992e-06 ## [16] 5.436657e-06 6.640438e-06 8.110137e-06 9.906360e-06 1.210034e-05 ## [21] 1.477673e-05 Looking at the range of the differences range(fd - true) ## [1] 2.706593e-07 1.477673e-05 we see that finite-differencing overestimates the true derivative for all the \\(x\\) values, but at most only by a small amount, i.e. \\(&lt; 2 \\times 10^{-5}\\). The normal pdf, denoted \\(\\phi(y; \\mu, \\sigma^2)\\), where \\(\\mu\\) is its mean and \\(\\sigma^2\\) is its variance, is defined as \\[ \\phi(y; \\mu, \\sigma^2) = \\dfrac{\\text{d} \\Phi(y; \\mu, \\sigma^2)}{\\text{d} y}\\] where \\(\\Phi(y; \\mu, \\sigma^2)\\) denotes its corresponding cdf. Confirm this result by finite-differencing for \\(y = -2, -1, 0, 1, 2\\), \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\) using pnorm() to evaluate \\(\\Phi(y; \\mu, \\sigma^2)\\). Solution We’ll start by defining the \\(y\\) values y &lt;- seq(-2, 2) Then we’ll evaluate \\(\\Phi(y; \\mu, \\sigma^2)\\) and \\(\\Phi(y + \\delta; \\mu, \\sigma^2)\\) for these \\(y\\) values, choosing \\(\\delta = 10^{-6}\\). Fy &lt;- pnorm(y) delta &lt;- 1e-6 Fy2 &lt;- pnorm(y + delta) The finite-difference approximation is then given by \\([\\Phi(y + \\delta; \\mu, \\sigma^2) - \\Phi(y; \\mu, \\sigma^2)] / \\delta\\), which we can calculate in R with the following. fy_fd &lt;- (Fy2 - Fy) / delta fy_fd ## [1] 0.05399102 0.24197085 0.39894228 0.24197060 0.05399091 We can then compare this to R’s dnorm() function fy &lt;- dnorm(y) all.equal(fy, fy_fd) ## [1] &quot;Mean relative difference: 3.53179e-07&quot; and see that the two are approximately equal. (We shouldn’t expect exact equality because pnorm() is only an approximation to \\(\\Phi(y; \\mu, \\sigma^2)\\), since the normal distribution’s cdf doesn’t have closed form, and because finite-differencing is only an approximation.) The Rosenbrock function (sometimes called the banana function) is given by \\[f(\\mathbf{x})=(a - x_1)^2 + b (x_2 - x_1^2)^2,\\] where \\(\\mathbf{x} = (x_1, x_2)^{\\text{T}}\\). Write a function, rosenbrock(x, a, b), to evaluate the Rosenbrock function in R for vector \\(\\mathbf{x} =\\) x and scalars \\(a =\\) a and \\(b =\\) b, with default values a = 1 and b = 100, and evaluate the function at \\(\\mathbf{x}_0 = (1, 2)^{\\text{T}}\\) with the default values of \\(a\\) and \\(b\\). Solution rosenbrock &lt;- function(x, a = 1, b = 100) { # function to evaluate Rosenbrock&#39;s banana function # x, a and b are scalars # returns a scalar (a - x[1])^2 + b * (x[2] - x[1]^2)^2 } x0 &lt;- c(1, 2) rosenbrock(x0) ## [1] 100 It’s also useful to plot the function, so that we know what we’re dealing with. The following gives a contour plot and coloured surface plot. par(mfrow = 1:2) x1 &lt;- seq(-2, 2, by = .1) x2 &lt;- seq(-1, 1.5, by = .1) x12_grid &lt;- expand.grid(x1 = x1, x2 = x2) f12 &lt;- matrix(apply(x12_grid, 1, rosenbrock), length(x1)) contour(x1, x2, f12, nlevels = 20) x1_mat &lt;- matrix(x12_grid$x1, length(x1)) x2_mat &lt;- matrix(x12_grid$x2, length(x1)) plot3D::surf3D(x1_mat, x2_mat, f12, colvar = f12, colkey = TRUE, box = TRUE, bty = &quot;b&quot;, phi = 20, theta = 15) Figure 4.1: Rosenbrock’s function as a contour plot (left) and surface plot (right). As we see, Rosenbrock’s function is a rather complicated function. Find \\(\\partial f(\\mathbf{x}) / \\partial x_1\\) and \\(\\partial f(\\mathbf{x}) / \\partial x_2\\) and then write a function in R, rosenbrock_d1(x, a, b), that returns the vector \\((\\partial f(\\mathbf{x}) / \\partial x_1, \\partial f(\\mathbf{x}) / \\partial x_2)^{\\text{T}}\\) and has the same default values as rosenbrock(), and then evaluate this gradient operator for \\(\\mathbf{x}_0 = (1, 2)^{\\text{T}}\\) and the default values of \\(a\\) and \\(b\\). Solution The partial derivatives are \\[\\dfrac{\\partial f(\\mathbf{x})}{\\partial x_1} = -2(a - x_1) - 4bx_1(x_2 - x_1^2) \\text{ and } \\dfrac{\\partial f(\\mathbf{x})}{\\partial x_2} = 2b(x_2 - x_1^2),\\] which can be evaluated in R with rosenbrock_d1 &lt;- function(x, a = 1, b = 100) { # function to evaluate first partial derivatives # of Rosenbrock&#39;s banana function # x, a and b are scalars # returns a 2-vector d_x2 &lt;- 2 * b * (x[2] - x[1]^2) c(-2 * (a - x[1]) - 2 * x[1] * d_x2, d_x2) } and for \\(\\mathbf{x}_0\\) and the default values of \\(a\\) and \\(b\\) we get rosenbrock_d1(x0) ## [1] -400 200 Find \\(\\partial^2 f(\\mathbf{x}) / \\partial x_1^2\\), \\(\\partial^2 f(\\mathbf{x}) / \\partial x_1 \\partial x_2\\) and \\(\\partial^2 f(\\mathbf{x}) / \\partial x_2^2\\) and then write a function in R, rosenbrock_d2(x, a, b), that returns the \\(2 \\times 2\\) Hessian matrix of \\(f(\\mathbf{x})\\) and has the same default values as rosenbrock(), and then evaluate this Hessian matrix for \\(\\mathbf{x}_0 = (1, 2)^{\\text{T}}\\) and the default values of \\(a\\) and \\(b\\). Solution The second partial derivatives are \\[\\dfrac{\\partial f(\\mathbf{x})}{\\partial^2 x_1^2} = 2 - 4b(x_2 - 3x_1^2),~\\dfrac{\\partial f(\\mathbf{x})}{\\partial x_1 \\partial x_2} = - 4bx_1 \\text{ and } \\dfrac{\\partial^2 f(\\mathbf{x})}{\\partial x_2^2} = 2b,\\] which can be evaluated in R with rosenbrock_d2 &lt;- function(x, a = 1, b = 100) { # function to evaluate second partial derivatives # of Rosenbrock&#39;s banana function # x, a and b are scalars # returns a 2 x 2 matrix d_x1x1 &lt;- 2 - 4 * b * (x[2] - 3 * x[1]^2) d_x1x2 &lt;- -4 * b * x[1] d_x2x2 &lt;- 2 * b matrix(c(d_x1x1, d_x1x2, d_x1x2, d_x2x2), 2, 2) } and for \\(\\mathbf{x}_0\\) and the default values of \\(a\\) and \\(b\\) we get rosenbrock_d2(x0) ## [,1] [,2] ## [1,] 402 -400 ## [2,] -400 200 Use rosenbrock_d1() to confirm that \\(\\tilde{\\mathbf{x}} = (1, 1)^{\\text{T}}\\) is a minimum of \\(f(\\mathbf{x})\\), i.e. that gradient operator is approximately a vector of zeros at \\(\\tilde{\\mathbf{x}} = (1, 1)^{\\text{T}}\\). Solution We’ll call \\(\\tilde{\\mathbf{x}}\\), x_tilde, x_tilde &lt;- c(1, 1) and then evaluating the first derivatives rosenbrock_d1(x_tilde) ## [1] 0 0 we see that these are zero, and to confirm that the Hessian matrix is positive definite, we can confirm that all its eigenvalues are positive H &lt;- rosenbrock_d2(x_tilde) # Hessian matrix lambda &lt;- eigen(H, only.values = TRUE, symmetric = TRUE)$values all(lambda &gt; 0) ## [1] TRUE which they are, and so \\(\\tilde{\\mathbf{x}}\\) is a minimum (although we don’t know whether it’s a local or global minimum from these two calculations). Recall the \\(MVN_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) log-likelihood, e.g. from Example 3.2. Find \\(\\partial \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) / \\partial \\boldsymbol{\\mu}\\) analytically and evaluate this for \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as given in Example 3.2. Solution \\[\\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})}{\\partial \\boldsymbol{\\mu}} = \\boldsymbol{\\Sigma}^{-1} ({\\bf y} - {\\boldsymbol \\mu}).\\] Evaluating this for \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as given can be done with the following code y &lt;- c(.7, 1.3, 2.6) mu &lt;- 1:3 Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) (d_mu &lt;- solve(Sigma, y - mu)) ## [1] 0.08 -0.38 0.14 so that \\[\\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})}{\\partial \\boldsymbol{\\mu}} \\simeq \\begin{pmatrix}0.08 \\\\-0.38 \\\\0.14 \\\\\\end{pmatrix}.\\] Confirm the above result by finite-differencing. Solution We’ll use function fd() again from Example 4.4 of the notes. fd &lt;- function(x, f, delta = 1e-6, ...) { # Function to evaluate derivative by finite-differencing # x is a p-vector # fn is the function for which the derivative is being calculated # delta is the finite-differencing step, which defaults to 10^{-6} # returns a vector of length x f0 &lt;- f(x, ...) p &lt;- length(x) f1 &lt;- numeric(p) for (i in 1:p) { x1 &lt;- x x1[i] &lt;- x[i] + delta f1[i] &lt;- f(x1, ...) } (f1 - f0) / delta } We’ll also need function dmvn3() from Example 3.13. dmvn3 &lt;- function(y, mu, Sigma, log = TRUE) { # Function to evaluate multivariate Normal pdf by solving # a system of linear equations via Cholesky decomposition # y and mu are vectors # Sigma is a square matrix # log is a logical # Returns scalar, on log scale, if log == TRUE. p &lt;- length(y) res &lt;- y - mu L &lt;- t(chol(Sigma)) out &lt;- - sum(log(diag(L))) - 0.5 * p * log(2 * pi) - 0.5 * sum(forwardsolve(L, res)^2) if (!log) out &lt;- exp(out) out } Then the following approximates \\(\\partial \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) / \\partial \\boldsymbol{\\mu}\\) by finite-differencing d_mu_fd &lt;- fd(mu, dmvn3, y = y, Sigma = Sigma) Note above that we’ve given y = y and Sigma = Sigma, then the first free argument to dmvn3() is mu, and hence fd() takes this as x. The finite-differencing approximation is d_mu_fd ## [1] 0.0799998 -0.3800007 0.1399992 and is the same as the analytical result all.equal(d_mu, d_mu_fd) ## [1] &quot;Mean relative difference: 2.833149e-06&quot; once we allow for error in the finite-difference approximation. The Gamma(\\(\\alpha\\), \\(\\beta\\)) pdf is given by \\[\\begin{equation} f(y \\mid \\alpha, \\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha - 1} \\exp(-\\beta y) \\quad {\\text{ for }}y&gt;0, \\tag{4.1} \\end{equation}\\] with \\(\\alpha,\\beta &gt; 0\\). The gamma distribution’s cdf, however, has no closed form. Approximate its cdf \\(F(y \\mid \\alpha, \\beta)\\) for \\(y = 1.5\\), \\(\\alpha = 1\\) and \\(\\beta = 2\\) using the midpoint rule with \\(N = 14\\) and compare your result to R’s pgamma(y, alpha, beta) function. Solution We can use dgamma(y, alpha, beta) to evaluate the Gamma(\\(\\alpha\\), \\(\\beta\\)) pdf (although it’s fine to explicitly write a function in R to evaluate \\(f(y \\mid \\alpha, \\beta)\\)). Then we want to specify the integration nodes for \\(N = 14\\). These will be \\(0.05, 0.15, \\ldots, 1.35, 1.45\\), if they’re equally spaced, and hence \\(h = 0.1\\). The following then approximates the integral as I_midpoint. nodes &lt;- seq(.05, 1.45, by = .1) h &lt;- .1 I_midpoint &lt;- h * sum(dgamma(nodes, 1, 2)) I_midpoint ## [1] 0.9486311 Using R’s pgamma() function we get I_pgamma &lt;- pgamma(1.5, 1, 2) I_pgamma ## [1] 0.9502129 Assuming R’s approximation to be the more accurate (which we’ll assume in MTH3045 is always a good assumption for R’s functions for evaluating cdfs), we’ll compares the relative absolute error of the midpoint approximation to pgamma(), which we’ll call rel_err rel_err &lt;- abs((I_pgamma - I_midpoint) / I_pgamma) rel_err ## [1] 0.001664724 and we see is about 0.17%. Consider integrating a function \\(f(x)\\) over and arbitrary interval \\([a, b]\\). Write a function to integrate \\(f(x)\\) over \\([a, b]\\) using the midpoint rule with \\(N\\) midpoints. Solution The following function, integrate_midpoint(), integrates a function over \\([a, b]\\) using the midpoint rule with \\(N\\) midpoints. integrate_midpoint &lt;- function(f, a, b, N) { # Function to approximate integral of f by midpoint rule over [a, b] # f is a function # a and b are scalars # N is integer number of midpoints # returns scalar ends &lt;- seq(0, 1, l = N + 1) h &lt;- ends[2] - ends[1] mids &lt;- ends[-1] - .5 * h h * sum(f(mids)) } Then use this to approximate \\(\\int_0^1 f(x) \\text{d} x\\), where \\[f(x) = 1 + 2 \\cos(2 x) + 2 \\sin(2 x)\\] using the midpoint rule with \\(N = 8\\). Solution We’ll first create function \\(f(x)\\), which we’ll call f, f &lt;- function(x) 1 + 2 * cos(2 * x) + 2 * sin(2 * x) and then we’ll use integrate_midpoint() to approximate its integral over \\([0, 1]\\) with \\(N = 8\\). midpoint &lt;- integrate_midpoint(f, 0, 1, 8) midpoint ## [1] 3.331511 Compare your integral approximation to the exact result in terms of its relative absolute error. Solution As \\[\\begin{align*} \\int_0^1 1 + 2 \\cos(2 x) + 2 \\sin(2 x)\\text{d}x &amp;= \\left[x + \\sin(2 x) - \\cos(2 x)\\right]_0^1 \\\\ &amp;= \\left[1 + \\sin(2) - \\cos(2)\\right] - \\left[\\sin(0) - \\cos(0) \\right]\\\\ &amp;= 2 + \\sin(2) - \\cos(2),\\end{align*}\\] then the true integral is \\(2 + \\sin(2) - \\cos(2) \\simeq 3.3254\\), which we’ll store as true. The relative absolute error, rel_err, is then true &lt;- 2 + sin(2) - cos(2) rel_err &lt;- abs((true - midpoint) / true) rel_err ## [1] 0.001824388 and on this occasion we see a relative absolute error of approximately 0.18%, for just \\(N = 8\\). The composite trapezium rule for integration is given by \\[\\int_a^b f(x) \\text{d}x \\simeq \\dfrac{h}{2}\\left[f(x_1^*) + f(x_N^*) + 2\\sum_{i = 2}^{N - 1} f(x_i^*)\\right],\\] for \\(N\\) integration nodes \\(x_i^* = a + (i - 1)h\\) where \\(h = (b - a) / (N - 1)\\). Write a function, integrate_trapezium(f, a, b, N), to implement the trapezium rule for an integral \\(\\int_a^b f(x)\\text{d}x\\) with \\(N\\) integration nodes. Solution integrate_trapezium &lt;- function(f, a, b, N) { # Function to approximate integral of f by trapezium rule over [a, b] # f is a function # a and b are scalars # N is integer number of integration nodes # returns scalar nodes &lt;- seq(a, b, l = N) h &lt;- (b - a) / (N - 1) int &lt;- 2 * sum(f(nodes[2:(N - 1)])) int &lt;- int + sum(f(nodes[c(1, N)])) .5 * h * int } Then use integrate_trapezium() to approximate \\(F(y \\mid \\alpha, \\beta) = \\int_0^y f(z \\mid \\alpha, \\beta) \\text{d} z\\) using \\(N = 14\\), where \\(f(y \\mid \\alpha, \\beta)\\) is the gamma pdf as given in Equation (4.1). Solution We’ll start with a function to evaluate the Gamma(1, 2) pdf, dgamma2(y). dgamma2 &lt;- function(y) dgamma(y, 1, 2) Then the trapezium rule approximation to \\(F(1.5 \\mid 1, 2)\\) is given below as I_trapezium. I_trapezium &lt;- integrate_trapezium(dgamma2, 0, 1.5, 14) I_trapezium ## [1] 0.9544261 Compare your integral approximation by the trapezium rule to that of the midpoint rule by using pgamma() as the benchmark of ‘the truth’. Solution rel_err &lt;- c(midpoint = abs((I_pgamma - I_midpoint) / I_pgamma), trapezium = abs((I_pgamma - I_trapezium) / I_pgamma)) rel_err ## midpoint trapezium ## 0.001664724 0.004433936 We see that the relative absolute error of the trapezium rule is 0.44% and so is greater than that of the midpoint rule, given the same number of integration nodes. Repeat Question 5 using Simpson’s rule and \\(N = 14\\), comparing your approximation to the actual integral and your midpoint rule approximation. Solution For Simpson’s composite rule, the \\(N\\) integration nodes are given by \\(x_{1i} = a + h(2i - 1)/2\\) and \\(x_{2i} = a + ih\\) where \\(h = (b - a)/N\\). a &lt;- 0 b &lt;- 1.5 N &lt;- 140 h &lt;- (b - a) / N nodes1 &lt;- a + h * (2*c(1:N) - 1) / 2 nodes2 &lt;- a + h * c(1:(N - 1)) I_simpson &lt;- dgamma2(a) + dgamma2(b) I_simpson &lt;- I_simpson+ 4 * sum(dgamma2(nodes1)) + 2 * sum(dgamma2(nodes2)) I_simpson &lt;- h * I_simpson / 6 I_simpson ## [1] 0.9502129 rel_err &lt;- abs((c(I_pgamma, I_midpoint) - I_simpson) / c(I_pgamma, I_midpoint)) rel_err ## [1] 7.321086e-11 1.667500e-03 Repeat Question 6 using Simpson’s rule and \\(N = 8\\), comparing your approximation to the actual integral and your midpoint rule approximation. Solution integrate_simpson &lt;- function(f, a, b, N) { # Function to approximate integral of f by Simpson&#39;s rule over [a, b] # f is a function # a and b are scalars # N is integer number of integration nodes # returns scalar h &lt;- (b - a) / N nodes1 &lt;- a + h * (2 * c(1:N) - 1) / 2 nodes2 &lt;- a + h * c(1:(N - 1)) h * (f(a) + 4 * sum(f(nodes1)) + 2 * sum(f(nodes2)) + f(b)) / 6 } simpson &lt;- integrate_simpson(f, 0, 1, 8) simpson ## [1] 3.325447 rel_err &lt;- abs((c(true, midpoint) - simpson) / c(true, midpoint)) rel_err ## [1] 9.502376e-07 1.820118e-03 Simpson’s composite 3/8 rule approximates an integral as \\[\\int_a^b f(x) \\text{d}x \\simeq \\dfrac{3h}{8} \\left(f(a) + 3 \\sum_{i=1}^{N/3} f(x_{3i - 2}^*) + 3 \\sum_{i=1}^{N/3} f(x_{3i-1}^*) + 2\\sum_{i = 1}^{N/3 - 1} f(x_{3i}^*) + f(b)\\right),\\] for integration nodes \\(x_i^* = a + ih\\), \\(i = 1, \\ldots, N - 1\\), with \\(h = (b - a) / N\\) and where \\(N\\) must be a multiple of three. Approximate \\(I = \\int_0^1 \\text{e}^x \\text{d}x\\) using Simpson’s 3/8 rule with \\(N = 9\\) nodes and find the relative absolute error of your approximation. Solution f &lt;- function(x) exp(x) true &lt;- exp(1) - 1 N &lt;- 9 a &lt;- 0 b &lt;- 1 h &lt;- (b - a) / N nodes &lt;- a + h * (1:(N - 1)) id2 &lt;- 3 * 1:((N / 3) - 1) # multiply by two id3 &lt;- c(3 * (1:(N / 3)) - 1, 3 * (1:(N / 3)) - 2) # multiply by three simpson38 &lt;- f(a) + 3 * sum(f(nodes[id3])) + 2 * sum(f(nodes[id2])) + f(b) (simpson38 &lt;- 3 * h * simpson38 / 8) ## [1] 1.718285 rel_err &lt;- abs((true - simpson38) / true) rel_err ## [1] 1.899613e-06 Repeat Question 5 using Gaussian quadrature with \\(N = 7\\) integration nodes, comparing your approximation to the actual integral and your midpoint and Simpson’s rule approximations of questions 5 and 9, respectively. Solution N &lt;- 7 gq_nodes &lt;- pracma::gaussLegendre(N, 0, 1.5) I_gq &lt;- sum(gq_nodes$w * dgamma(gq_nodes$x, 1, 2)) I_gq ## [1] 0.9502129 I_vec &lt;- c(I_pgamma, I_midpoint, I_simpson) rel_err &lt;- abs((I_vec - I_gq) / I_vec) rel_err ## [1] 2.281873e-13 1.667500e-03 7.343905e-11 Approximate \\[I = \\int_{-1}^1 \\int_{-1}^1 \\int_{-1}^1 (x_1 x_2 x_3)^2 \\text{d}x_1\\text{d}x_2\\text{d}x_3\\] using the midpoint rule with \\(N = 20\\) and evaluate the approximation’s relative absolute error. Solution Let’s start by finding the true integral \\[I = \\int_{-1}^1 \\int_{-1}^1 \\int_{-1}^1 (x_1 x_2 x_3)^2 \\text{d}x_1\\text{d}x_2\\text{d}x_3 = \\int_{-1}^1 \\int_{-1}^1 \\left[\\dfrac{x_1^3}{3}\\right]_{-1}^1 \\text{d}x_2\\text{d}x_3\\] \\[ = \\int_{-1}^1 \\int_{-1}^1 \\left[\\dfrac{1}{3} - \\dfrac{-1}{3}\\right]_{-1}^1 \\text{d}x_2\\text{d}x_3 = \\dfrac{2}{3} \\int_{-1}^1 \\int_{-1}^1 (x_2 x_3)^2 \\text{d}x_2\\text{d}x_3 = \\ldots = \\left(\\dfrac{2}{3}\\right)^3.\\] true &lt;- (2 / 3)^3 For the midpoint rule, we’ll first find the midpoints, which can be the same for each dimension in this case. N &lt;- 20 edges &lt;- seq(-1, 1, l = N + 1) h &lt;- edges[2] - edges[1] mids &lt;- edges[-1] - .5 * h Then we’ll find all combinations of these, x123, and evaluate the integrand at them, f123. x123 &lt;- expand.grid(x1 = mids, x2 = mids, x3 = mids) f123 &lt;- apply(x123, 1, prod)^2 We’ll call our midpoint approximation midpoint, which can be calculated with midpoint &lt;- sum(h^3 * f123) midpoint ## [1] 0.2940796 Then its relative absolute error, rel_err_mid, is rel_err_mid &lt;- abs((true - midpoint) / true) rel_err_mid ## [1] 0.007481266 which is reasonably small. Approximate the integral of Question 12 by applying Simpson’s rule with \\(N = 20\\) to each dimension, instead of the midpoint rule. Solution The following create the nodes and weights needed to apply Simpson’s rule. N &lt;- 20 a &lt;- -1 b &lt;- 1 h &lt;- (b - a) / N x1i &lt;- a + h * (2 * (1:N) - 1) / 2 x2i &lt;- a + h * (1:(N - 1)) weights &lt;- rep(c(1, 4, 2, 1), c(1, length(x1i), length(x2i), 1)) nodes &lt;- c(a, x1i, x2i, b) weights &lt;- h * weights / 6 x123 &lt;- expand.grid(x1 = nodes, x2 = nodes, x3 = nodes) f123 &lt;- apply(x123, 1, prod)^2 w123 &lt;- weights %x% weights %x% weights We’ll call our Simpson’s rule approximation simpson, which can be calculated with simpson &lt;- sum(w123 * f123) simpson ## [1] 0.2962963 Then its relative absolute error, rel_err_simp, is rel_err_simp &lt;- abs((true - simpson) / true) rel_err_simp ## [1] 5.620504e-16 which is very small. Approximate the integral \\[I = \\int_0^2 \\int_{-\\pi}^\\pi \\left[\\sin(x_1) + x_2 + 1\\right] \\text{d}x_1 \\text{d}x_2\\] using Gaussian quadrature with \\(N = 9\\) integration nodes per dimension and estimate the relative absolute error of your approximation. Solution We’ll start by calculating \\(I\\), the true integral, \\[\\begin{align*} I &amp;= \\int_0^2 \\int_{-\\pi}^\\pi \\left[\\sin(x_1) + x_2 + 1\\right] \\text{d}x_1 \\text{d}x_2\\\\ &amp;= \\int_0^2 \\Big\\{\\big[- \\cos(x_1) + x_1 x_2 + x_2\\big]_{-\\pi}^\\pi\\Big\\} \\text{d}x_2\\\\ &amp;= \\int_0^2 \\big[ \\left(1 + \\pi x_2 + \\pi\\right) - \\left(1 - \\pi x_2 - \\pi\\right)\\big] \\text{d}x_2\\\\ &amp;= \\int_0^2 2\\pi(x_2 + 1) \\text{d}x_2 = 2\\pi \\left[\\dfrac{x_2^2}{2} + x_2\\right]_0^2 = 8\\pi, \\end{align*}\\] and then store this as true. (true &lt;- 8 * pi) ## [1] 25.13274 To calculate this is R we want to set our number of nodes, N, and write a function, f(), to evaluate the integrand. N &lt;- 9 f &lt;- function(x1, x2) sin(x1) + x2 + 1 Then we want to set our integration nodes for \\(x1\\), gq1, and for \\(x2\\), gq2. gq1 &lt;- pracma::gaussLegendre(N, -pi, pi) gq2 &lt;- pracma::gaussLegendre(N, 0, 2) Then we want to evaluate f() at each combination of our \\(x_1\\) and \\(x_2\\) integration nodes, which we’ll call f12. f12 &lt;- outer(gq1$x, gq2$x, f) The weights corresponding to these nodes, w12, are w12 &lt;- gq1$w %o% gq2$w and then the integral approximation is Ihat &lt;- sum(w12 * f12) Ihat ## [1] 25.13274 Its relative absolute error, rel_err, is rel_err &lt;- abs((true - Ihat) / true) rel_err ## [1] 2.82716e-16 and again is incredibly small. Repeat examples 4.10 and 4.11 by avoiding for() loops. [You might want to consider the functions expand.grid() or outer().] Solution We’ll start with Example 4.10 and will use expand.grid(). Then we’ll set the dimension d, the number of integration nodes per dimension, N, the integration nodes, x1 and x2, and calculate the true value of the integral, true, as in the Lecture Notes. d &lt;- 2 true &lt;- (exp(1) - 1)^d N &lt;- 10 x1 &lt;- x2 &lt;- (1:N - .5) / N Then we want to use expand.grid() to give us all combinations of the integration nodes for both dimensions, and we’ll quickly use head() to see what it’s done. x12 &lt;- expand.grid(x1 = x1, x2 = x2) head(x12) ## x1 x2 ## 1 0.05 0.05 ## 2 0.15 0.05 ## 3 0.25 0.05 ## 4 0.35 0.05 ## 5 0.45 0.05 ## 6 0.55 0.05 We’ll calculate the integration weights, w, as in the Lecture Notes, w &lt;- 1/(N^d) and then approximate the integral, which we’ll call midpoint, by taking the first column of x12 as the \\(x_{1i}\\) nodes, and the second column as the \\(x_{2j}\\) nodes, which gives midpoint &lt;- w * sum(exp(x12[, 1] + x12[, 2])) midpoint ## [1] 2.950033 and is the same as in the Lecture Notes. For Example 4.11 we’ll use outer() instead, using d and true from above. Then we need to set the number of integration nodes, N, and find the nodes for each margin, i.e. \\(x_{1i}\\) and \\(x_{2j}\\) for \\(i, j = 1, \\ldots, N\\), which we’ll store as xw. N &lt;- 4 xw &lt;- pracma::gaussLegendre(N, 0, 1) Next we want to calculate \\(\\exp(x_{1i} + x_{2j})\\) for \\(i, j = 1, \\ldots, N\\), which we’ll store as f12. f12 &lt;- exp(outer(xw$x, xw$x, FUN = &#39;+&#39;)) f12 ## [,1] [,2] [,3] [,4] ## [1,] 1.148967 1.490991 2.094725 2.718282 ## [2,] 1.490991 1.934829 2.718282 3.527458 ## [3,] 2.094725 2.718282 3.818971 4.955800 ## [4,] 2.718282 3.527458 4.955800 6.431040 And then we want to calculate the integration weights, \\(w_{ij} = w_{1i} w_{2j}\\), for \\(i, j = 1, \\ldots, N\\), where the \\(w_{1i}\\)s and \\(w_{2j}\\) can both be taken from xw$w, and will store these as w12, obtaining them through outer(). w12 &lt;- outer(xw$w, xw$w) w12 ## [,1] [,2] [,3] [,4] ## [1,] 0.03025075 0.05671296 0.05671296 0.03025075 ## [2,] 0.05671296 0.10632333 0.10632333 0.05671296 ## [3,] 0.05671296 0.10632333 0.10632333 0.05671296 ## [4,] 0.03025075 0.05671296 0.05671296 0.03025075 which uses that FUN = '*' is outer()’s default. Then we’ll calculate the integral approximation, and store this as gq gq &lt;- sum(w12 * f12) gq ## [1] 2.952492 which is the same approximation as in the Lecture Notes. [On this occasion outer() is perhaps a bit tidier, but is less easy to implement for \\((d &gt; 2)\\)-dimensional integral approximations.] Consider a random vector \\(\\mathbf{X} = (X_1, X_1)^{\\text{T}}\\) from the standard bivariate normal distribution, which we write as \\[\\mathbf{X} = \\left(\\begin{array}{c} X_1\\\\ X_2 \\end{array}\\right) \\sim BVN\\left(\\left(\\begin{array}{c} 0\\\\ 0 \\end{array}\\right), \\left(\\begin{array}{cc} 1 &amp; \\rho\\\\ \\rho &amp; 1 \\end{array}\\right)\\right)\\] so that \\(-1 \\leq \\rho \\leq 1\\) is the correlation between \\(X_1\\) and \\(X_2\\). The standard bivariate normal pdf is given by \\[f(\\mathbf{x} ; \\rho) = \\dfrac{1}{2 \\pi \\sqrt{1 - \\rho^2}} \\exp \\left(-\\dfrac{x_1^2 - 2 \\rho x_1 x_2 + x_2^2}{2(1 - \\rho^2)} \\right)\\] but its cdf, which is given by \\[\\text{Pr}(a_1 \\leq x_1 \\leq b_1, a_2 \\leq x_2 \\leq b_2; \\rho) = \\int_{a_1}^{b_1} \\int_{a_2}^{b_2} \\dfrac{1}{2 \\pi} f(\\mathbf{x} ; \\rho) \\text{d}x_1 \\text{d} x_2,\\] doesn’t have closed form. Estimate \\(\\text{Pr}(-1 \\leq x_1 \\leq 1, -1 \\leq x_2 \\leq 1; 0.9)\\) using the midpoint rule with \\(N=10\\) integration nodes per dimension, and then using Gaussian quadrature with \\(N = 7\\) integration nodes per dimension. Solution We’ll start with a function to evaluate \\(f(\\mathbf{x} ; \\rho)\\), which we’ll call f_bvn(). f_bvn &lt;- function(x1, x2, rho) { # Function to evaluate bivaraiate Normal pdf # x1, x2 can be scalar, vector or matrix # rho is a scalar # Returns object in same format as x1 temp &lt;- 1 - rho^2 out &lt;- x1^2 - 2 * rho * x1 * x2 + x2^2 out &lt;- out / (2 * temp) exp(-out) / (2 * pi * sqrt(temp)) } Then we’ll write a generic function to implement the two-dimensional midpoint rule, which we’ll call integrate_bvn_midpoint(), which is only useful for the case when we want the same integration nodes for each dimension. integrate_bvn_midpoint &lt;- function(N, a, b, f, ...) { # Function to evaluate a cdf by midpoint rule # N is an integer giving the number of integration nodes per dimension # a and b are scalars # f is the corresponding pdf # Returns a scalar edges &lt;- seq(a, b, l = N + 1) h &lt;- edges[2] - edges[1] mids &lt;- edges[-1] - .5 * h grid &lt;- expand.grid(x1 = mids, x2 = mids) sum(h * h * f(grid$x1, grid$x2, ...)) } I_bvn_midoint &lt;- integrate_bvn_midpoint(10, -1, 1, f_bvn, rho = .9) I_bvn_midoint ## [1] 0.5986239 Next we’ll repeat the process, writing a function integrate_bvn_gq() to approximate the integral by Gaussian quadrature. integrate_bvn_gq &lt;- function(N, a, b, f, ...) { # Function to evaluate a cdf by Gaussian quadrature # N is an integer giving the number of integration nodes per dimension # a and b are scalars # f is the corresponding pdf # Returns a scalar x1 &lt;- x2 &lt;- pracma::gaussLegendre(N, a, b) x_grid &lt;- expand.grid(x1 = x1$x, x2 = x2$x) w_vec &lt;- x1$w %x% x2$w sum(w_vec * f(x_grid$x1, x_grid$x2, ...)) } I_bvn_gq &lt;- integrate_bvn_gq(7, -1, 1, f_bvn, rho = .9) I_bvn_gq ## [1] 0.5963602 Note that on this occasion, we don’t know the true integral, and so can’t calculate the relative absolute errors. Repeat Example 4.12 using the midpoint rule and \\(N = 10\\). Solution On this occasion, we can effectively merge code from Question \\(\\ref{nofor}\\) and Example 4.12. N &lt;- 10 d &lt;- 5 x &lt;- (1:N - .5) / N xx &lt;- lapply(1:d, function(i) x) X &lt;- expand.grid(xx) w &lt;- 1/(N^d) f &lt;- sum(w * exp(rowSums(X))) true &lt;- (exp(1) - 1)^d c(true = true, midpoint = f, rel.err = abs((true - f) / true)) ## true midpoint rel.err ## 14.978626322 14.947455928 0.002080991 We’ve now got a relative absolute error about six orders of magnitude larger than for Gaussian quadrature in Example 4.12, and yet here we’ve had to evaluate the integrand 10^{5} times, as opposed to 1024 times in Example 4.12. Here we see that when dealing with high-dimensional integrals, we may want to carefully choose our integration scheme, especially if the integrand is an expensive function to evaluate. Consider a single random variable \\(Y \\mid \\mu \\sim \\text{N}(\\mu, \\sigma^2)\\), where we can characterise our prior beliefs about \\(\\mu\\) as \\(\\mu \\sim \\text{N}(\\alpha, \\beta)\\). The marginal distribution of \\(y\\) is given by \\[f(y) = \\int_{-\\infty}^\\infty f(y \\mid \\mu) f(\\mu) \\text{d}\\mu, \\] and that the marginal distribution of \\(Y\\) is \\(Y \\sim \\text{N}(\\alpha, \\sigma^2 + \\beta)\\) can be derived in various ways. Approximate \\(f(y)\\) using Laplace’s method for \\(\\sigma = 1.5\\), \\(\\alpha = 2\\) and \\(\\beta = 0.5\\), and then compare the relative absolute error of your approximation to the true marginal pdf for \\(y = -2, 0, 2, 4, 6\\). Solution For the Laplace approximation, we take \\(n = 1\\) and \\(f()\\) from Equation (4.4) as \\(-\\log \\phi(y; \\mu, \\sigma^2) - \\log \\phi(\\mu; \\alpha, \\beta)\\), where we swap \\(x\\) for \\(\\mu\\). Its first derivative w.r.t. \\(\\mu\\) is \\(-(y - \\mu)/\\sigma^2 + (\\mu - \\alpha)/\\beta\\) and its second derivative w.r.t. \\(\\mu\\) is \\(1/\\sigma^2 + 1/\\beta\\). Setting the first derivative to zero gives \\(\\hat \\mu = (y \\beta + \\alpha \\sigma^2) / (\\beta + \\sigma^2)\\). Our Laplace approximation then \\[\\begin{align*} \\hat f(y) &amp;\\simeq \\sqrt{\\dfrac{2 \\pi}{\\frac{1}{\\sigma^2} + \\frac{1}{\\beta}}} \\text{exp}\\left\\{-\\log \\phi(y; \\hat \\mu, \\sigma^2) - \\log \\phi(\\hat \\mu; \\alpha, \\beta)\\right\\}\\\\ &amp;= \\sqrt{\\dfrac{2 \\pi}{\\frac{1}{\\sigma^2} + \\frac{1}{\\beta}}} \\phi(y; \\hat \\mu, \\sigma^2) \\phi(\\hat \\mu; \\alpha, \\beta). \\end{align*}\\] We can evaluate this in R with the following. We’ll start by setting \\(\\sigma\\), \\(\\alpha\\) and \\(\\beta\\). alpha &lt;- 2 beta &lt;- .5 sigma &lt;- 1.5 Then we’ll create fy(), which evaluates the true marginal pdf, i.e. \\(f(y)\\), given \\(y\\), \\(\\sigma\\), \\(\\alpha\\) and \\(\\beta\\), fy &lt;- function(y, sigma, alpha, beta) { # Function to evaluate Normal(alpha, beta = sigma^2) pdf # y is scalar, vector or matrix # sigma, alpha and beta are scalars # returns objects in same format as y dnorm(y, alpha, sqrt(beta + sigma^2)) } evaluate this for the \\(y\\) values specified, y_vals, y_vals &lt;- seq(-2, 6) and store this as f_true f_true &lt;- fy(y_vals, sigma, alpha, beta) The following function calculates the Laplace approximation, by finding \\(\\hat \\mu\\), called mu_hat, and then substituting this into the approximation above. fla &lt;- function(y, sigma, alpha, beta) { # Function to compute Laplace approximate for Normal # prior on mean mu and Normal likelihood on data # y is scalar or vector # sigma, alpha and beta are scalars # Returns object of same format as y mu_hat &lt;- (y * beta + alpha * sigma^2) / (beta + sigma^2) mult &lt;- 2 * pi / abs(1 / beta + 1 / sigma^2) sqrt(mult) * dnorm(y, mu_hat, sigma) * dnorm(mu_hat, alpha, sqrt(beta)) } Then we use this to evaluate the approximation for the specified \\(y\\) values, f_la. f_la &lt;- fla(y_vals, sigma, alpha, beta) We’ll look at these alongside each other rbind(true = f_true, laplace = f_la) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## true 0.01311721 0.04683602 0.1162501 0.200577 0.2405712 0.200577 0.1162501 ## laplace 0.01311721 0.04683602 0.1162501 0.200577 0.2405712 0.200577 0.1162501 ## [,8] [,9] ## true 0.04683602 0.01311721 ## laplace 0.04683602 0.01311721 and then by calculating the relative absolute error, i.e. abs((f_true - f_la) / f_true) ## [1] 6.612393e-16 0.000000e+00 1.193788e-16 1.383787e-16 1.153736e-16 ## [6] 1.383787e-16 0.000000e+00 0.000000e+00 1.322479e-16 we see that the relative absolute error is effectively zero, once we allow for R’s machine tolerance. [In fact if we carry on with the algebra above by substituting in the expression for the Normal pdfs, i.e. \\[\\begin{align*} I_n &amp;= \\sqrt{\\dfrac{2 \\pi}{\\frac{1}{\\sigma^2} + \\frac{1}{\\beta}}} \\left(\\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp}\\left\\{-\\dfrac{(y - \\hat \\mu)^2}{2 \\sigma^2}\\right\\} \\right) \\left(\\dfrac{1}{\\sqrt{2 \\pi \\beta}} \\text{exp}\\left\\{-\\dfrac{(\\hat \\mu - \\alpha)^2}{2 \\beta}\\right\\} \\right), \\end{align*}\\] we eventually find that on this occasion the Laplace approximation is exact (which you’re welcome to confirm in your own time).] The negative binomial distribution, denoted NegBin\\((r, p)\\), has probability mass function (pmf) \\[f(y) = \\dfrac{\\Gamma(y + r)}{y!\\Gamma(r)} p^y (1 - p)^r.\\] If \\(Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\) and \\(\\lambda \\sim \\text{Gamma}(r, [1 - p]/p)\\), then \\(Y \\sim \\text{NegBin}(r, p)\\), so that the marginal distribution of \\(Y\\) is negative binomial, i.e.  \\[\\begin{equation} f(y) = \\int_{-\\infty}^\\infty f(y \\mid \\lambda) f(\\lambda) \\text{d} \\lambda \\tag{4.2} \\end{equation}\\] if \\(f(y \\mid \\lambda)\\) is the \\(\\text{Poisson}(\\lambda)\\) pmf and \\(f(\\lambda)\\) is the \\(\\text{Gamma}(\\alpha, \\beta)\\) pdf with \\(\\alpha = r\\) and \\(\\beta = (1 - p)/p\\). Use Laplace’s method to approximate the marginal pmf, i.e. to approximate \\(f(y)\\), for \\(y = 0, 1, \\ldots, 20\\), \\(\\alpha= 5\\) and \\(\\beta = 0.5\\). Solution For Laplace’s method, we first note that \\[\\begin{align*} f(y, \\lambda) &amp;= f(y \\mid \\lambda) f(\\lambda)\\\\ &amp;= \\text{e}^{-\\lambda}\\dfrac{\\lambda^y}{y!} \\times \\dfrac {\\lambda^{\\alpha -1}\\text{e}^{-\\beta \\lambda} \\beta^\\alpha}{\\Gamma(\\alpha)} = \\text{e}^{-\\lambda(1 + \\beta)} \\dfrac{\\lambda^{y + \\alpha - 1} \\beta^\\alpha}{y!\\Gamma(\\alpha)} \\end{align*}\\] Then, considering only \\(\\lambda\\), \\[ \\log f(y, \\lambda) = -\\lambda(\\beta + 1) + (y + \\alpha - 1) \\log(\\lambda) + \\text{constant} \\] and so \\[ \\dfrac{\\partial \\log f(y, \\lambda)}{\\partial \\lambda} = -(\\beta + 1) + (y + \\alpha - 1) / \\lambda \\] which we set to zero at \\(\\tilde \\lambda\\) so that \\(\\beta + 1 = (y + \\alpha - 1) / \\tilde{\\lambda}\\), i.e. \\[ \\tilde \\lambda = \\dfrac{y + \\alpha - 1}{\\beta + 1}. \\] Then \\[ \\dfrac{\\partial^2 \\log f(y, \\lambda)}{\\partial \\lambda^2} = -(y + \\alpha - 1) / \\lambda^2. \\] Using the formula for Laplace’s method from the notes, we take \\(-nf(x)\\) as \\(\\log f(y, \\lambda)\\) so that \\[ f(y) \\simeq \\text{e}^{\\log f(y, \\tilde \\lambda)} \\sqrt{\\dfrac{2\\pi}{nf&#39;&#39;(\\tilde \\lambda)}} \\] where \\(f&#39;&#39;(\\tilde \\lambda) = -\\partial^2 \\log f(y, \\lambda) / \\partial \\lambda^2\\). Therefore \\[\\begin{align*} f(y) &amp;\\simeq \\text{e}^{-\\tilde \\lambda(1 + \\beta)} \\dfrac{\\tilde{\\lambda}^{y + \\alpha - 1} \\beta^\\alpha}{y!\\Gamma(\\alpha)} \\sqrt{\\dfrac{2\\pi\\tilde{\\lambda}^2}{y + \\alpha - 1}}\\\\ &amp;= \\text{e}^{-\\tilde \\lambda(1 + \\beta)} \\dfrac{\\tilde{\\lambda}^{y + \\alpha} \\beta^\\alpha}{y!\\Gamma(\\alpha)} \\sqrt{\\dfrac{2\\pi}{y + \\alpha - 1}}. \\end{align*}\\] Next we’ll set the \\(y\\) values and \\(\\alpha\\) and \\(\\beta\\). y &lt;- 0:20 alpha &lt;- 5 beta &lt;- .5 The following function can then give the Laplace approximation to \\(f(y)\\) for given \\(y\\), \\(\\alpha\\) and \\(\\beta\\). fhat &lt;- function(y, alpha, beta) { # Function to give Laplace approximation to marginal distribution # y can be scalar or vector # alpha and beta are scalar # returns scalar if y scalar, and vector if y vector tlambda &lt;- (y + alpha - 1) / (beta + 1) dpois(y, tlambda) * dgamma(tlambda, alpha, beta) * tlambda * sqrt(2 * pi / (y + alpha - 1)) } Then we can evaluate this for the specific values of \\(y\\). fhat(y, alpha, beta) ## [1] 0.004030551 0.013490989 0.027056782 0.042171720 0.056312548 0.067653206 ## [7] 0.075239786 0.078882315 0.078932092 0.076049269 0.071011807 0.064581799 ## [13] 0.057425971 0.050079002 0.042936547 0.036266361 0.030228594 0.024899075 ## [19] 0.020291794 0.016378531 0.013104802 It’s useful to plot this against the \\(\\text{NegBin}(\\alpha, p)\\) pmf. First we’ll write a function dnbinom2(y, r, p) to evaluate this pmf dnbinom2 &lt;- function(y, r, p) gamma(y + r) * p^y * (1 - p)^r / factorial(y) / gamma(r) and then we’ll plot it against the Laplace approximation. fy &lt;- dnbinom2(y, alpha, 1 / (beta + 1)) plot(y, fy, ylim = c(0, max(fy))) lines(y, fhat(y, alpha, beta), col = 2) Figure 4.2: Comparison of true negative binomial marginal pmf with its Laplace approximation. The plot shows agreement between the true marginal pmf and its Laplace approximation to be very good. Recall the integral \\[I = \\int_0^1 \\big[1 + 2 \\cos(2 x) + 2 \\sin(2 x)\\big] \\text{d} x\\] from above. Use Monte Carlo integration to approximate \\(I\\) with Monte Carlo samples of size \\(N = 100\\), 1000 and 10000 and calculate the relative absolute error of each of your approximations. Solution For a given value of \\(N\\), we want to generate \\(N\\) Uniform([0, 1]) random variates, evaluate the integrand at each, and take the mean as the approximation to \\(I\\). The following code approximates the integral as I_hat for for \\(N = 100\\) N &lt;- 100 x &lt;- runif(N) fx &lt;- 1 + 2 * cos(2 * x) + 2 * sin(2 * x) I_hat &lt;- mean(fx) I_hat ## [1] 3.330527 and its relative absolute error, rel_err, is true &lt;- 2 + sin(2) - cos(2) rel_err &lt;- abs((true - I_hat) / true) rel_err ## [1] 0.001528359 The following code then loops this over the \\(N\\) values \\(N = 100\\), 1000 and 10000. N_vals &lt;- c(100, 1000, 10000) I_hat &lt;- rel_err &lt;- numeric(length(N_vals)) for (i in 1:length(N_vals)) { N &lt;- N_vals[i] x &lt;- runif(N) fx &lt;- 1 + 2 * cos(2 * x) + 2 * sin(2 * x) I_hat[i] &lt;- mean(fx) rel_err[i] &lt;- abs((true - I_hat[i]) / true) } which gives the integral approximations I_hat ## [1] 3.316098 3.311553 3.325624 and corresponding relative absolute errors rel_err ## [1] 2.810404e-03 4.177179e-03 5.404947e-05 We might expect that our relative absolute errors will decrease as \\(N\\) increases. Probabilistically they will: but our approximations are subject to the variability in the random samples, and so sometimes a smaller Monte Carlo sample size may give a better approximation than a larger sample size. In general, though, we should rely on the probabilistic properties, and use the largest possible Monte Carlo sample size that is feasible. If \\(Y \\sim \\text{Uniform}([a, b])\\) then \\[E(Y) = \\int_a^b \\frac{u}{b - a} \\text{d} u = \\frac{1}{2}(a + b).\\] Use the mean of \\(m = 1000\\) Monte Carlo replicates of Monte Carlo samples of size \\(N = 500\\) to approximate the above integral, and hence to approximate \\(E(Y)\\), for \\(a = -1\\) and \\(b = 1\\). Solution We’ll start by setting the Monte Carlo sample size, N, and then the number of replicates, m. N &lt;- 500 m &lt;- 1e3 For a given replicate we need to generate \\(N\\) points from the Uniform[-1, 1] distribution, \\(u_1^*, \\ldots, u_N^*\\), say, and then our approximation to the integral is given by \\(N^{-1} \\sum_{i = 1}^N u_i^* / 2\\), as \\(b - a = 2\\). The following calculates this for a single replicate, where \\(a =\\) a and \\(b =\\) b, a &lt;- -1 b &lt;- 1 u &lt;- runif(N, -1, 1) mean(u) / (b -a) ## [1] -0.001125121 and then replicate() can perform the \\(m\\) replicates uu &lt;- replicate(m, runif(N, -1, 1)) dim(uu) ## [1] 500 1000 which stored the sampled values in a \\(N \\times m\\) matrix uu. Then we can use colMeans() to calculate the mean of each sample, and mean() to take the mean over replicates. mean(colMeans(uu) / (b - a)) ## [1] 0.0005700557 [Note that due to the independence of the samples, this is equivalent to mean(uu) / (b - a) ## [1] 0.0005700557 and so \\(m\\) Monte Carlo samples of size \\(N\\) is equivalent to one Monte Carlo sample of size \\(mN\\).] Approximate the integral the earlier integral \\[I = \\int_0^2 \\int_{-\\pi}^\\pi \\left[\\sin(x_1) + x_2 + 1\\right] \\text{d}x_1\\text{d}x_2 \\] using Monte Carlo with samples of size \\(N = 100\\), \\(200\\) and \\(400\\) and \\(m = 1000\\) replicates for each. Calculate the mean and variance of your approximations for each value of \\(N\\), i.e. across replicates. What do you observe? Solution We’ll start with a single replicate for \\(N = 100\\), generating \\(x_1\\) from the Uniform([\\(-\\pi, \\pi\\)]) distribution and \\(x_2\\) from the Uniform([0, 2]) distribution. Let \\(x_{11}^*, \\ldots, x_{1N}^*\\) and \\(x_{21}^*, \\ldots, x_{2N}^*\\) denote these respective samples. Then the integral approximation is given by \\[\\hat I = \\dfrac{4\\pi}{N} \\sum_{i = 1}^N \\big[\\sin(x_{1i}^*) + x_{2i}^* + 1\\big]\\] as our domain for \\((x_1, x_2)\\), denoted by \\(\\mathcal{X}\\) in the Lecture Notes, is \\([0, 2] \\times [-\\pi, \\pi]\\), and hence \\(V(\\mathcal{X}) = 4\\pi\\). The following then calculates this in R. N &lt;- 100 x1 &lt;- runif(N, -pi, pi) x2 &lt;- runif(N, 0, 2) f &lt;- function(x1, x2) sin(x1) + x2 + 1 4 * pi * mean(f(x1, x2)) ## [1] 26.53961 Now we can to replicate this \\(m\\) times for the different \\(N\\) values. For explicitness, we’ll use loops. N_vals &lt;- c(100, 200, 400) m &lt;- 1000 I_hat &lt;- matrix(NA, length(N_vals), m) for (i in 1:length(N_vals)) { N &lt;- N_vals[i] for (j in 1:m) { x1 &lt;- runif(N, -pi, pi) x2 &lt;- runif(N, 0, 2) I_hat[i, j] &lt;- 4 * pi * mean(f(x1, x2)) } } The following give the mean and variance across replicates. rowMeans(I_hat) ## [1] 25.06577 25.08629 25.16046 apply(I_hat, 1, sd) ## [1] 1.1376463 0.8392819 0.6005032 Our main observation here is that the variance decreases as \\(N\\) increases, and roughly halves as \\(N\\) doubles, which is consistent with the formula for \\(\\text{Var}(\\hat I_{\\mathcal{X}})\\) given in the Lecture Notes, i.e. being inversely proportional to \\(N\\). Approximate \\[I = \\int_{-1}^1 \\int_{-1}^1 \\int_{-1}^1 (x_1 x_2 x_3)^2 \\text{d}x_1\\text{d}x_2\\text{d}x_3\\] using a Monte Carlo sample of size \\(N=10^4\\) and evaluate the approximation’s relative absolute error. Solution Let’s start by finding the true integral \\[I = \\int_{-1}^1 \\int_{-1}^1 \\int_{-1}^1 (x_1 x_2 x_3)^2 \\text{d}x_1\\text{d}x_2\\text{d}x_3 = \\int_{-1}^1 \\int_{-1}^1 \\dfrac{2}{3}\\text{d}x_2\\text{d}x_3 = \\ldots = \\left(\\dfrac{2}{3}\\right)^3 = \\dfrac{8}{27}.\\] We’ll store this as true. true &lt;- 8 / 27 Then we want to generate three sets of \\(N\\) draws from the Uniform([-1, 1]) distribution, representing \\(x_1\\), \\(x_2\\) and \\(x_3\\), which we’ll store as x123. N &lt;- 1e4 x123 &lt;- matrix(runif(3 * N, -1, 1), N) Our domain is \\([-1, 1]^3\\) which has volume \\(2^3 = 8\\). Our approximation to the integral, I_hat, is I_hat &lt;- 8 * mean(apply(x123^2, 1, prod)) I_hat ## [1] 0.2993306 which has a relative absolute error, rel_err, of rel_err &lt;- abs((true - I_hat) / true) rel_err ## [1] 0.01024071 i.e. about 1%. Approximate the negative binomial marginal distribution given in Equation (4.2) using Monte Carlo sampling with \\(N = 10^3\\), \\(\\alpha = 5\\), \\(\\beta = 0.5\\) and for \\(y = 0, 1, \\ldots, 20\\), and calculate the relative absolute error of your approximation. Solution We’ll start by storing \\(N\\), \\(\\alpha\\), \\(\\beta\\) and \\(y = 0, 1, \\ldots, 20\\). N &lt;- 1e3 alpha &lt;- 5 beta &lt;- .5 y_vals &lt;- 0:20 Then we want to generate \\(N\\) values for \\(\\lambda\\) from the \\(\\text{Gamma}(\\alpha, \\beta)\\) distribution, which we’ll call lambda_samp. lambda_samp &lt;- rgamma(N, alpha, beta) Our approximation to the marginal distribution is obtained by evaluating the Poisson(\\(\\lambda\\)) distribution for the \\(y\\) values specified at each sampled \\(\\lambda\\) value, which we’ll call fy_samp, fy_samp &lt;- sapply(lambda_samp, function(lambda) dpois(y_vals, lambda)) and then averaging over the samples, which we’ll call fy_hat. fy_hat &lt;- rowMeans(fy_samp) fy_hat ## [1] 0.003646912 0.012868946 0.026607133 0.042330466 0.057387922 0.069712704 ## [7] 0.078056072 0.081994808 0.081816047 0.078298252 0.072440391 0.065222290 ## [13] 0.057453215 0.049716276 0.042382039 0.035655861 0.029631056 0.024332628 ## [19] 0.019746861 0.015838362 0.012558630 The relative absolute error, rel_err, relative to the true marginal pdf, fy, can be caluclated with fy &lt;- dnbinom2(y_vals, alpha, 1 / (beta + 1)) rel_err &lt;- abs((fy - fy_hat) / fy) rel_err ## [1] 0.113800441 0.061853809 0.030169988 0.008106467 0.008541485 0.020948770 ## [7] 0.028824333 0.031614646 0.029365575 0.022995284 0.014064485 0.004323243 ## [13] 0.004722164 0.012097060 0.017473383 0.021136090 0.023842795 0.026619715 ## [19] 0.030533783 0.036477436 0.044998852 and is reasonably small for each \\(y\\) value. Finally, as in question \\(\\ref{negbin}\\), we’ll plot the approximation against the true marginal pdf plot(y_vals, fy, ylim = c(0, max(fy_hat)), xlab = &#39;y&#39;, ylab = &#39;f(y)&#39;) lines(y, fy_hat, col = 2) Figure 4.3: Comparison of true negative binomial marginal pmf with its Monte Carlo approximation. and see that the approximation (in red) closely follows the true marginal pdf. "],["chapter-5-exercises.html", "5 Chapter 5 exercises", " 5 Chapter 5 exercises Suppose that we want to generate random variables from a distribution with cdf \\[ F(y) = \\dfrac{2}{\\pi} \\arcsin(\\sqrt{y}) \\quad \\text{for}~y \\in [0, 1]. \\] We can generate a single random variable, \\(Y^*\\), say, by generating a random Uniform([0, 1]) random variable, \\(U^*\\), say, and then finding \\(Y^*\\) such that \\(F(Y^*) = U^*\\). Write a function in R, rarcsine(n), that generates \\(n\\) random variables with cdf \\(F(y)\\) above using R’s uniroot() function, and then generate \\(n = 10\\) variates. Note that asin(y) evaluates \\(\\arcsin(y)\\) in R for \\(y =\\) y. Solution We’ll start by writing a function to evaluate \\(F()\\), called F. F &lt;- function(y) 2 * asin(sqrt(y)) / pi Then we want the function for which we want the root, which we’ll call F_root(). F_root &lt;- function(y, u) F(y) - u Next we’ll use uniroot() to find \\(y\\) such that \\(F(y) = u\\). uniroot(F_root, c(0, 1), u = runif(1))$root ## [1] 0.09332532 Putting this together into function rarcsine(n), we get qarcsin &lt;- function(n) { replicate(n, uniroot(F_root, c(0, 1), u = runif(1))$root) } and so qarcsin(10) ## [1] 0.9034382 0.3821925 0.8172672 0.2686639 0.9919419 0.9232536 0.6268923 ## [8] 0.8650127 0.1923155 0.8662630 gives \\(n = 10\\) random variates. Consider that the independent sample \\(\\mathbf{y} = (y_1, \\ldots, y_n)\\) is from the pdf \\[ f(y) = 2\\theta y\\text{exp}\\left\\{-\\theta y^2\\right\\} \\quad \\text{for}~y &gt; 0 \\] with parameter \\(\\theta &gt; 0\\). Show that the maximum likelihood estimate of \\(\\theta\\) is given by \\(\\hat \\theta = n / (\\sum_{i = 1}^n y_i^2)\\). Solution The log-likelihood is \\[ \\log f(\\mathbf{y} \\mid \\theta) = n \\log (\\theta) - \\theta \\sum_{i = 1}^n y_i^2 + \\text{constant} \\] and so \\[ \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\theta)}{\\partial \\theta} = \\dfrac{n}{\\theta} - \\sum_{i = 1}^n y_i^2. \\] Setting \\(\\partial \\log f(\\mathbf{y} \\mid \\theta) / \\partial \\theta = 0\\) gives \\(\\hat \\theta = n / \\sum_{i = 1}^n y_i^2\\). Given that \\(y_1, \\ldots, y_n\\) take the values \\[ 0.15, 0.24, 0.33, 0.43, 0.49, 0.57, 0.57, 0.63, 0.71, 0.93, 1.15, 1.22, 1.23, 1.23, 1.28 \\] find \\(\\hat \\theta\\). Solution We can calculate this in R with # y &lt;- c(...) # read in y theta_hat &lt;- length(y) / sum(y^2) and so \\(\\hat \\theta = 1.428\\). Use R’s optimize() function to verify \\(\\hat \\theta\\), assuming \\(\\hat \\theta \\in [0.1, 4]\\). Solution We now need a function to evaluate the negative log-likelihood (ignoring the constant, because this doesn’t vary with \\(\\theta\\)), as we want to find \\(\\theta\\) that minimises this. We’ll call this function nd0(theta, y). nd0 &lt;- function(theta, y) theta * sum(y^2) - n * log(theta) Then we call optimize() accordingly optimize(nd0, c(.1, 4), y = y) ## $minimum ## [1] 1.427901 ## ## $objective ## [1] 9.656731 and we see that element minimum in the list confirms \\(\\hat \\theta\\). Consider using Newton’s method to find \\(\\hat \\theta\\) from Question 2. Find the second derivative of the log-likelihood w.r.t. \\(\\theta\\). Solution The second derivative of the log-likelihood is \\[ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\theta)}{\\partial \\theta^2} = -\\dfrac{n}{\\theta^2}. \\] To find maximum likelihood estimates using Newton’s method, we want to minimise the negative log-likelihood. Hence show that if we want to minimise \\(-\\log f(\\mathbf{y} \\mid \\theta)\\) then a step of Newton’s method, given \\(\\theta\\), is given by \\[ \\dfrac{\\partial [-\\log f(\\mathbf{y} \\mid \\theta)] / \\partial \\theta}{\\partial^2 [-\\log f(\\mathbf{y} \\mid \\theta)] / \\partial \\theta^2} = \\dfrac{\\sum_{i = 1}^n y_i^2 - n / \\theta}{n / \\theta^2}. \\] Solution The step is \\[ \\dfrac{\\partial [-\\log f(\\mathbf{y} \\mid \\theta)] / \\partial \\theta}{\\partial^2 [-\\log f(\\mathbf{y} \\mid \\theta)] / \\partial \\theta^2} = \\dfrac{\\sum_{i = 1}^n y_i^2 - n / \\theta}{n / \\theta^2} \\] as required. Write functions in R, nd1(theta, y) and nd2(theta, y), that return the first and second derivatives of \\(-\\log f(\\mathbf{y} \\mid \\theta)\\) w.r.t. \\(\\theta\\) based on analytical expressions for the derivatives. Solution Here we’ll ensure that nd1() and nd2() return a vector and matrix, respectively, for generality if we’re working with higher dimensions, but returning scalars is also fine here. nd1 &lt;- function(theta, y) { # Function to evaluate first derivative w.r.t. theta # theta is a scalar # y is a vector # returns a 1-vector as.vector(sum(y^2) - n / theta) } nd2 &lt;- function(theta, y) { # Function to evaluate second derivative w.r.t. theta # theta is a scalar # y is a vector # returns a 1x1 matrix matrix(n / theta^2, 1, 1) } Use your functions from Question 3(c) to find the first Newton step, given \\(\\theta_0 = 1\\) and the sample of data from Question 2(b). Solution We’ll load the data # y &lt;- ... theta_0 &lt;- 1 and then first step is -solve(nd2(theta_0, y), nd1(theta_0, y)) ## [1] 0.29968 Perform four further steps of Newton’s method. After how many steps does Newton’s method agree with \\(\\hat \\theta\\) from Question 2(b) to within three decimal places. Solution The following code performs five iterations of Newton’s method in total, starting at \\(\\theta_0\\). iterations &lt;- 5 theta_i &lt;- numeric(iterations + 1) theta_i[1] &lt;- 1 for (i in 1 + 1:iterations) { theta_i[i] &lt;- theta_i[i - 1] - solve(nd2(theta_i[i - 1], y), nd1(theta_i[i - 1], y)) } theta_i ## [1] 1.000000 1.299680 1.416402 1.427826 1.427919 1.427919 theta_hat &lt;- n / sum(y^2) theta_hat ## [1] 1.427919 If we compare these to \\(\\hat \\theta\\) by calculating the absolute difference we get abs(theta_i - theta_hat) ## [1] 4.279187e-01 1.282387e-01 1.151687e-02 9.288927e-05 6.042653e-09 ## [6] 0.000000e+00 which is less than \\(10^{-3}\\) by its fourth element, i.e. the third iteration of Newton’s method. Evaluate the Hessian of the negative log-likelihood at \\(\\hat \\theta\\). This should be positive definite if \\(\\hat \\theta\\) is the maximum likelihood estimate, which can be assessed by all of its eigenvalues being positive. Check whether this is the case. Solution The following calculates the Hessian and its eigenvalues H &lt;- nd2(theta_hat, y) ev &lt;- eigen(H, symmetric = TRUE, only.values = TRUE) and we can then check whether they’re all positive all(ev$values &gt; 0) ## [1] TRUE which they are. Suppose that the sample of data \\[ -7.7, -0.5, -0.1, 1.2, 1.3, 2.4, 3.7, 5.7, 6.2, 8.4, 13.9, 24.4 \\] can be modelled as independent realisations from the pdf \\[ f(y \\mid \\mu) = \\dfrac{2}{\\pi \\left[(y - \\mu)^2 + 4\\right]}, \\] where \\(-\\infty &lt; \\mu &lt; \\infty\\) is an unknown parameter. Find the maximum likelihood estimate of \\(\\mu\\), denoted \\(\\hat \\mu\\), using Newton’s method with a suitable numbers of iterations. Solution We’ll start by finding the log-likelihood and its first and second derivatives w.r.t. \\(\\mu\\). The log-likelihood is given by \\[ \\log f(\\mathbf{y} \\mid \\mu) = n \\log 2 - n \\log \\pi - \\sum_{i = 1}^n \\log\\left([y_i - \\mu]^2 + 4\\right) \\] and so \\[ \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\mu)}{\\partial \\mu} = 2\\sum_{i = 1}^n \\dfrac{y_i - \\mu}{(y_i - \\mu)^2 + 4} \\] and therefore \\[ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu)}{\\partial \\mu^2} = 4 \\sum_{i = 1}^n \\left[\\dfrac{y_i - \\mu}{(y_i - \\mu)^2 + 4}\\right]^2 - 2 \\sum_{i = 1}^n \\dfrac{1}{(y_i - \\mu)^2 + 4}. \\] Then we’ll write functions in R to evaluate these, d0(mu, y, mult = 1), d1(mu, y, mult = 1) and d2(mu, y, mult = 1), respectively, where mult can be set to -1 when we want to deal with the negative log-likelihood. d0 &lt;- function(mu, y, mult = 1) { # function to evaluate log-likelihood # mu is a scalar # y is a vector # returns a scalar n &lt;- length(y) mult * (n * log(2) - n * log(pi) - sum(log((y - mu)^2 + 4))) } d1 &lt;- function(mu, y, mult = 1) { # function to evaluate first derivative of log-likelihood w.r.t. mu # mu is a scalar # y is a vector # returns a scalar n &lt;- length(y) mult * (2 * sum((y - mu) / ((y - mu)^2 + 4))) } d2 &lt;- function(mu, y, mult = 1) { # function to evaluate second derivative of log-likelihood w.r.t. mu # mu is a scalar # y is a vector # returns a scalar n &lt;- length(y) n * log(2) - n * log(pi) - sum(log((y - mu)^2 + 4)) 2 * mult * (2 * sum(((y - mu) / ((y - mu)^2 + 4))^2) - sum(1 / ((y - mu)^2 + 4))) } Next we’ll perform our iterations of Newton’s method. We’ll deem a ‘suitable’ number of iterations to be when the parameter estimates for one iteration are within \\(10^{-4}\\) of those of the previous iteration. theta_i &lt;- 1 # set theta_0 while(TRUE) { theta_last &lt;- theta_i[length(theta_i)] theta_next &lt;- theta_last - solve(d2(theta_last, y2, -1), d1(theta_last, y2, -1)) theta_i &lt;- c(theta_i, theta_next) if (abs(theta_last - theta_next) &lt; 1e-4) break } theta_i ## [1] 1.000000 2.099045 2.215480 2.219332 2.219336 We see that our 4th iteration has met our stopping criteria. Hence we have \\(\\hat \\theta =\\) 2.2193, to four decimal places. Then use optimize() to verify your estimate of \\(\\mu\\) based on Newton’s method from Question 4(a). Solution For optimize() we can use the following call, and will assume \\(\\hat \\theta \\in [1, 3]\\). optimize(d0, c(1, 3), y = y2, mult = -1) ## $minimum ## [1] 2.219318 ## ## $objective ## [1] 41.79389 Looking at the minimum element of the list that optimize() returns, we see agreement between its estimates of \\(\\theta\\) and that of Question 4(a). Consider a log-Normal model for the wind speeds of Example 5.5, so that \\(Y \\sim \\text{log-Normal}(\\mu, \\sigma^2)\\) and hence \\[ f_Y(y) = \\dfrac{1}{y\\sqrt {2\\pi \\sigma^2}}\\exp \\left\\{-\\dfrac{\\left[\\log(y) - \\mu \\right]^2}{2\\sigma^2}\\right\\} \\quad y &gt; 0 \\] and for \\(\\sigma^2 &gt; 0\\). The log-likelihood for an independent sample \\(\\mathbf{y} = (y_1, \\ldots, y_n)\\) is given by \\[ \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2) = -\\dfrac{n}{2} \\log(2 \\pi) - \\dfrac{n}{2} \\log \\sigma^2 - \\sum_{i = 1}^n \\log(y_i) - \\dfrac{1}{2 \\sigma^2} \\sum_{i = 1}^n (\\log y_i- \\mu)^2 \\] and its gradient operator is given by \\[ \\begin{pmatrix} \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu}\\\\ \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\sigma^2} \\end{pmatrix} = \\begin{pmatrix} \\dfrac{1}{\\sigma^2} \\sum_{i = 1}^n (\\log y_i - \\mu)\\\\ -\\dfrac{n}{2 \\sigma^2} + \\dfrac{1}{2\\sigma^4} \\sum_{i = 1}^n (\\log y_i - \\mu)^2 \\end{pmatrix}. \\] For the wind speed data, \\[ \\sum_{i = 1}^n \\log y_i = -12.755, \\quad \\sum_{i = 1}^n (\\log y_i)^2 = 155.675 \\] and \\(n = 31\\). Write \\(\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\) in terms of the summary statistics \\(s_1\\) and \\(s_2\\), so that \\(\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\) can be evaluated without knowing \\(y_1, \\ldots, y_n\\). Solution \\[ \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2) = -\\dfrac{n}{2} \\log(2 \\pi) - \\dfrac{n}{2} \\log \\sigma^2 - s_1 - \\dfrac{1}{2 \\sigma^2} (s_2 - 2\\mu s_1 + n \\mu^2). \\] Write a function in R, ln(pars, s1, s2, n, mult = 1), that evaluates \\(m \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\), where pars is a 2-vector such that pars[1] \\(= \\mu\\) and pars[2] \\(= \\sigma^2\\), s1 \\(= s_1\\), s2 \\(= s_2\\), n \\(= n\\) and mult \\(= m\\). Your function should ensure that \\(\\sigma^2 &gt; 0\\). Solution mu &lt;- 1 sigsq &lt;- 2 ln &lt;- function(pars, s1, s2, n, mult = 1) { # Function to evaluate log-Normal(mu, sig^2) log-likelihood # pars is a 2-vector: pars[1] = mu, pars[2] = sig^2 # s1 and s2 are scalars # n is an integer # mult is a scalar; defaults to 1 # returns a scalar mu &lt;- pars[1] sigsq &lt;- pars[2] if (sigsq &lt;= 0) return(mult * -1e8) out &lt;- -.5 * n * (log(2 * pi) + log(sigsq)) out &lt;- out - .5 * (s2 - 2 * mu * s1 + n * mu^2) / sigsq mult * (out - s1) } Find \\[ \\begin{pmatrix} \\dfrac{\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu}\\\\ \\dfrac{\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\sigma^2} \\end{pmatrix} \\] in terms of \\(s_1\\) and \\(s_2\\). Solution \\[ \\begin{pmatrix} \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu}\\\\ \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\sigma^2} \\end{pmatrix} = \\begin{pmatrix} \\dfrac{1}{\\sigma^2} (s_1 - n\\mu^2)\\\\ -\\dfrac{n}{2 \\sigma^2} + \\dfrac{1}{2\\sigma^4} (s_2 - 2 \\mu s_1 + n\\mu^2) \\end{pmatrix}. \\] Write a function in R, ln_d1(pars, s1, s2, n, mult = 1), that evaluates the first derivative of \\(m \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\) w.r.t. \\((\\mu, \\sigma^2)\\), where pars is a 2-vector such that pars[1] \\(= \\mu\\) and pars[2] \\(= \\sigma^2\\), s1 \\(= s_1\\), s2 \\(= s_2\\), n \\(= n\\) and mult \\(= m\\). Solution ln_d1 &lt;- function(pars, s1, s2, n, mult= 1) { # Function to evaluate first derivative of log-Normal(mu, sig^2) # log-likelihood w.r.t (\\mu, \\sigma^2) # pars is a 2-vector: pars[1] = mu, pars[2] = sig^2 # s1 and s2 are scalars # n is an integer # mult is a scalar; defaults to 1 # returns a 2-vector mu &lt;- pars[1] sigsq &lt;- pars[2] out &lt;- numeric(2) out[1] &lt;- (s1 - n * mu) / sigsq out[2] &lt;- -.5 * n / sigsq + .5 * (s2 - 2 * mu * s1 + n * mu^2) / (sigsq^2) mult * out } Using values for \\((\\mu, \\sigma^2)\\) of \\((\\mu_0, \\sigma_0^2) = (1, 2)\\), check your function ln_d1() by finite-differencing. Solution We’ll use function fd() from the lecture notes. fd &lt;- function(x, f, delta = 1e-6, ...) { # Function to evaluate derivative by finite-differencing # x is a p-vector # fn is the function for which the derivative is being calculated # delta is the finite-differencing step, which defaults to 10^{-6} # returns a vector of length x f0 &lt;- f(x, ...) p &lt;- length(x) f1 &lt;- numeric(p) for (i in 1:p) { x1 &lt;- x x1[i] &lt;- x[i] + delta f1[i] &lt;- f(x1, ...) } (f1 - f0) / delta } Then we’ll load \\(\\mu_0\\) and \\(\\sigma_0^2\\). mu0 &lt;- 1 sigsq0 &lt;- 2 Finally we’ll evaluate the gradient and its finite-differencing counterpart ln_d1(c(mu0, sigsq0), s1, s2, n) ## [1] -21.87750 18.77313 fd(c(mu0, sigsq0), ln, s1 = s1, s2 = s2, n = n) ## [1] -21.87751 18.77311 which are both the same, so it looks as if our function to evaluate the gradient is returning the correct values. (We could check with more values of \\((\\mu, \\sigma^2)\\) if we’re really keen, but one check is usually sufficient!) Using \\((\\mu_0, \\sigma_0^2)\\) from Question 5(e) as starting values, use optim() together with ln() and ln_d1() to find the maximum likelihood estimates of \\((\\mu, \\sigma^2)\\), \\((\\hat \\mu, \\hat \\sigma^2)\\), via the BFGS method. Solution s1 &lt;- -12.755 s2 &lt;- 155.675 n &lt;- 31 hats &lt;- optim(c(mu0, sigsq0), ln, ln_d1, s1 = s1, s2 = s2, n = n, mult = -1, method = &#39;BFGS&#39;)$par hats ## [1] -0.4114503 4.8524782 Evaluate the first derivative of \\(\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\) at \\((\\hat \\mu, \\hat \\sigma^2)\\) and comment on whether \\((\\hat \\mu, \\hat \\sigma^2)\\) are likely to be maximum likelihood estimates. Solution The first derivatives at \\((\\hat \\mu, \\hat \\sigma^2)\\) are given by ln_d1(hats, s1, s2, n) ## [1] -8.394066e-06 2.346390e-06 which are very closely to zero, indicating we’re likely to have found the maximum likelihood estimates. For the log-Normal wind speed model of Question 5(a), the Hessian matrix of second derivatives is given by \\[ \\nabla^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2) = \\begin{pmatrix} \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu^2} &amp; \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu \\partial \\sigma^2}\\\\ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu \\partial \\sigma^2} &amp; \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial (\\sigma^2)^2} \\end{pmatrix} \\] where \\[\\begin{align*} \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu^2} &amp;= -\\dfrac{n}{\\sigma^2}\\\\ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial \\mu \\partial \\sigma^2} &amp;= -\\dfrac{1}{\\sigma^4} \\sum_{i = 1}^n (\\log y_i - \\mu)\\\\ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)}{\\partial (\\sigma^2)^2} &amp;= \\dfrac{n}{2 \\sigma^4} - \\dfrac{1}{\\sigma^6} \\sum_{i = 1}^n (\\log y_i - \\mu)^2. \\end{align*}\\] Write a function, nl_d2(pars, s1, s2, n, mult = -1), that evaluates the Hessian matrix in terms of \\(s_1\\) and \\(s_2\\) given in Question 5(a) and assuming that same arguments as nl_d1() of Question 5(d). Solution ln_d2 &lt;- function(pars, s1, s2, n, mult= 1) { # Function to evaluate second derivatives of log-Normal(mu, sig^2) # log-likelihood w.r.t (\\mu, \\sigma^2) # pars is a 2-vector: pars[1] = mu, pars[2] = sig^2 # s1 and s2 are scalars # n is an integer # mult is a scalar; defaults to 1 # returns a 2x2 matrix mu &lt;- pars[1] sigsq &lt;- pars[2] out &lt;- matrix(0, 2, 2) out[1, 1] &lt;- - n / sigsq out[1, 2] &lt;- out[2, 1] &lt;- -(s1 - n * mu) / (sigsq^2) out[2, 2] &lt;- .5 * n / sigsq^2 - (s2 - 2 * mu * s1 + n * mu^2) / (sigsq^3) mult * out } Using nl() and nl_d1() from Question 5(a) and nl_d2() from Question 6(a), find \\((\\hat \\mu_2, \\hat \\sigma_2^2)\\), the maximum likelihood estimates, using Newton’s method via nlminb() in R. Solution hats2 &lt;- nlminb(c(mu0, sigsq0), ln, ln_d1, ln_d2, s1 = s1, s2 = s2, n = n, mult = -1)$par hats2 ## [1] -0.4114516 4.8524818 Evaluate the first derivative of \\(\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\) at \\((\\hat \\mu_2, \\hat \\sigma_2^2)\\) from Question 6(b) and comment on whether \\((\\hat \\mu_2, \\hat \\sigma_2^2)\\) are likely to be maximum likelihood estimates. Solution We can proceed as in question \\(\\ref{wind12}\\). ln_d1(hats2, s1, s2, n) ## [1] -1.098215e-15 5.506706e-14 which are still very closely to zero, indicating we’re likely to have found the maximum likelihood estimates. Evaluate the Hessian matrix of \\(\\log f(\\mathbf{y} \\mid \\mu, \\sigma^2)\\) at \\((\\hat \\mu_2, \\hat \\sigma_2^2)\\) from Question 6(b) and comment on whether \\((\\hat \\mu_2, \\hat \\sigma_2^2)\\) are likely to be maximum likelihood estimates. Solution We’ll calculate the Hessian H &lt;- ln_d2(hats2, s1, s2, n) and then, after it’s been negated, want all of its eigenvalues to be positive ev &lt;- eigen(-H, symmetric = TRUE, only.value = TRUE) all(ev$values &gt; 0) ## [1] TRUE which they are. Recall the wind speed data of Example 5.5. Consider these as independent realisations from the Gamma(\\(\\alpha\\), \\(\\beta\\)) distribution, i.e. with pdf \\[ f_Y(y \\mid \\alpha, \\beta) = \\dfrac{y^{\\alpha -1} e^{-\\beta x}\\beta^\\alpha}{\\Gamma(\\alpha)}, \\quad y &gt; 0, \\] for parameters \\(\\alpha, \\beta &gt; 0\\). Find the maximum likelihood estimates of \\((\\alpha, \\beta)\\) using the BFGS method with optim() in R by supplying a function that evaluates the negative log-likelihood’s gradient vector w.r.t. \\((\\alpha, \\beta)\\). [Note that in R lgamma(x) evaluates \\(\\log \\Gamma(x)\\) and digamma(x) evaluates \\(\\text{d} \\log \\Gamma(x) / \\text{d}x\\) for x \\(= x\\), where digamma() relies on the so-called polygamma function.] Solution We’ll load the wind speed data as y0. y0 &lt;- c(3.52, 1.95, 0.62, 0.02, 5.13, 0.02, 0.01, 0.34, 0.43, 15.5, 4.99, 6.01, 0.28, 1.83, 0.14, 0.97, 0.22, 0.02, 1.87, 0.13, 0.01, 4.81, 0.37, 8.61, 3.48, 1.81, 37.21, 1.85, 0.04, 2.32, 1.06) Then the following functions evaluate the negative log-likelihood and its gradient, respectively. nldgamma &lt;- function(pars, y) { # Function to evaluate Gamma(alpha, beta) negative log-likelihood # pars is a 2-vector # y is a vector # returns a scalar alpha &lt;- pars[1] beta &lt;- pars[2] if (min(c(alpha, beta)) &lt;= 0) return(1e8) n &lt;- length(y) - n * alpha * log(beta) + n * lgamma(alpha) - (alpha - 1) * sum(log(y)) + beta * sum(y) } nldgamma_d1 &lt;- function(pars, y) { # Function to evaluate first derivative of Gamma(alpha, beta) # negative log-likelihood w.r.t. (alpha, beta) # pars is a 2-vector # y is a vector # returns a 2-vector alpha &lt;- pars[1] beta &lt;- pars[2] n &lt;- length(y) out &lt;- numeric(2) out[1] &lt;- - n * log(beta) + n * digamma(alpha) - sum(log(y)) out[2] &lt;- - n * alpha / beta + sum(y) out } We’ll choose starting values for \\((\\alpha, \\beta)\\) as \\((\\alpha_0, \\beta_0) = (1, 1)\\) and store these as pars0 pars0 &lt;- c(1, 1) and then call optim() to implement the BFGS method pars_bfgs &lt;- optim(pars0, nldgamma, nldgamma_d1, y = y0, method = &#39;BFGS&#39;)$par pars_bfgs ## [1] 0.4017348 0.1179670 storing the results maximum likelihood estimates as pars_bfgs. Find the maximum likelihood estimates of \\((\\alpha, \\beta)\\) using Newton’s method with nlminb() in R by supplying the function that evaluates the negative log-likelihood’s gradient vector w.r.t. \\((\\alpha, \\beta)\\) created in Question ref qgbfgs, and by supplying a function that evaluates the negative log-likelihood’s Hessian matrix w.r.t. \\((\\alpha, \\beta)\\). [Note that in R trigamma(x) evaluates \\(\\text{d}^2 \\log \\Gamma(x) / \\text{d}x^2\\) for x \\(= x\\).] Solution We can re-use a few functions and objects from above, so next we’ll write a function to evaluate the second derivative of the log-likelihood w.r.t. \\((\\alpha, \\beta)\\). nldgamma_d2 &lt;- function(pars, y) { # Function to evaluate second derivative of Gamma(alpha, beta) # negative log-likelihood w.r.t. (alpha, beta) # pars is a 2-vector # y is a vector # returns a 2x2 matrix alpha &lt;- pars[1] beta &lt;- pars[2] n &lt;- length(y) out &lt;- matrix(0, 2, 2) out[1, 1] &lt;- n * trigamma(alpha) out[1, 2] &lt;- out[2, 1] &lt;- - n / beta out[2, 2] &lt;- n * beta^2 out } We supply this to nlminb() with pars0 from above pars_newt &lt;- nlminb(pars0, nldgamma, nldgamma_d1, nldgamma_d2, y = y0)$par pars_newt ## [1] 0.4017348 0.1179670 and store the maximum likelihood estimates as pars_newt. By considering the gradient at your estimates in Questions 7(a) and 7(b), verify that on both occasions the maximum likelihood estimates have been reached. Solution We’ll evaluate the gradient of the negative log-likelihood at pars_bfgs and pars_newt nldgamma_d1(pars_bfgs, y0) ## [1] -1.200319e-06 8.312922e-06 nldgamma_d1(pars_newt, y0) ## [1] 1.999826e-06 -1.875220e-06 and find both are sufficiently close to the zero vector that we think we’ve reached the maximum likelihood estimates. Use the Nelder-Mead method to find the maximum likelihood estimates of \\((\\mu, \\sigma^2)\\) for the log-Normal model, starting values and data of Question 5. Solution hats3 &lt;- optim(c(mu0, sigsq0), ln, s1 = s1, s2 = s2, n = n, mult = -1)$par hats3 ## [1] -0.4113588 4.8537235 As a quick aside we’ll have a look at the derivative of our estimates ln_d1(hats3, s1, s2, n) ## [1] -0.0005929934 -0.0008170016 which we see are still close to zero, but not as close as with the gradient-based optimisation methods. This is something to expect with the Nelder-Mead method because it doesn’t use gradients in its iterations, and doesn’t use gradient-based criteria to determine convergence, whereas the gradient-based methods do (amongst other criteria). Find the maximum likelihood estimates for the wind speed data of Example 5.5 based on the Gamma(\\(\\alpha\\), \\(\\beta\\)) model of Question 7 using the Nelder-Mead method. Solution We’ll use the following call to optim() pars_nm &lt;- optim(pars0, nldgamma, y = y0)$par pars_nm ## [1] 0.4017200 0.1179219 and see that we get similar estimates to question \\(\\ref{gmm}\\). Use simulated annealing with \\(N = 1000\\) iterations (i.e. control = list(maxit = 1e3))) to approximate the maximum likelihood estimates of \\((\\mu, \\sigma^2)\\) for the wind speed data and log-Normal model of Question 5. Solution hats4 &lt;- optim(c(mu0, sigsq0), ln, s1 = s1, s2 = s2, n = n, mult = -1, method = &#39;SANN&#39;, control = list(maxit = 1e3)) hats4 ## $par ## [1] -0.394841 4.795629 ## ## $value ## [1] 55.71617 ## ## $counts ## function gradient ## 1000 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Find the maximum likelihood estimates for the wind speed data of Example 5.5 based on the Gamma(\\(\\alpha\\), \\(\\beta\\)) model of Question 7 using simulated annealing with \\(N = 10^4\\) iterations, and then evaluate the gradient w.r.t. (\\(\\alpha\\), \\(\\beta\\)) at the maximum likelihood estimates using your gradient function from Question 7(a). Solution We’ll use the following call to optim() pars_sann &lt;- optim(pars0, nldgamma, y = y0, method = &#39;SANN&#39;)$par pars_sann ## [1] 0.3995571 0.1175635 which recognises that \\(N = 10^4\\) iterations is optim()’s default. We get similar estimates to questions 7 and 9. The following evaluates the gradient nldgamma_d1(pars_sann, y0) ## [1] -0.3835148 0.2118473 which we see is near zero, but nowhere near as near as for estimates we’ve seen previously. The nature of simulated annealing means we’re only going to get a final gradient incredibly close to zero by chance, or if we use lots of iterations and allow the temperature to decrease sufficiently. In practice, we’d probably not want to wait this long. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
