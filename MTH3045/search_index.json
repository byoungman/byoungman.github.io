[["index.html", "MTH3045: Statistical Computing 1 Introduction 1.1 Module outline 1.2 Lectures / practical classes 1.3 Office hours 1.4 Resources 1.5 Acknowledgements 1.6 Assessment 1.7 Motivating example 1.8 Exploratory and refresher exercises", " MTH3045: Statistical Computing Dr. Ben Youngman b.youngman@exeter.ac.uk Laver 817; ext. 2314 16/01/2023 1 Introduction 1.1 Module outline MTH3045, Statistical Computing, is designed to introduce you to some important, advanced topics that, when considered during calculations, can improve using computers to fit statistical models to data. This can allow us to use more data, or to fit a model more quickly and/or more reliably, for example. 1.2 Lectures / practical classes In-person lectures will be held at the following times: Monday 9.35 – 10.25. Innovation 1 / Babbage Wednesday 10.35 – 11.25. Harrison 207 Friday 11.35 – 12.25. Innovation 1 / Babbage The first lectures takes place on Monday 16th January 2023. Lectures will typically involve a mix of me presenting a few slides being to introduce a method to you and then you engaging in hands-on programming so that you experience and confirm understanding of what’s been introduced. To engage with MTH3045 you should be attending lectures in person. I currently plan to use week 6 as a reading week for you. During week 6 there will be no MTH3045 lectures. 1.3 Office hours I will hold an office hour each week on Mondays at 11.00 - 12.00 in my office, Laver 817, or you can schedule an appointment to meet me online during that hour. 1.4 Resources All material that will be examined for MTH3045 can be found in the lecture notes and exercises provided. A complete set of lecture notes and exercises with solutions can be found in pdf format on the module’s ELE page https://vle.exeter.ac.uk/course/view.php?id=13566 There is also an identical web-based version of the lecture notes and exercises, which is available at https://byoungman.github.io/MTH3045/ and may be easier to use for your study. However, sometimes you may find it useful to get a different perspective on things. If so, you may want to consult the following books. Specific parts of these books useful for given topics will be highlighted throughout the lecture notes. Banerjee, S. and A. Roy (2014). Linear Algebra and Matrix Analysis for Statistics. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis. Eddelbuettel, D. (2013). Seamless R and C++ Integration with Rcpp. Use R! Springer New York. Gillespie, C. and R. Lovelace (2016). Efficient R Programming: A Practical Guide to Smarter Programming. O’Reilly Media. https://csgillespie.github.io/efficientR/. Monahan, J. F. (2011). Numerical Methods of Statistics (2 ed.). Cambridge University Press. Nocedal, J. and S. Wright (2006). Numerical Optimization (2 ed.). Springer Series in Operations Research and Financial Engineering. Springer New York. Petersen, K. B. and M. S. Pedersen (2012). The Matrix Cookbook. https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf Press, W., S. Teukolsky, W. Vetterling, and B. Flannery (2007). Numerical Recipes: The Art of Scientific Computing (3 ed.). Cambridge University Press. W. N. Venables, D. M. S. and the R Core Team (2021). An Introduction to R (4.1.0 ed.). https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf. Wickham, H. (2019). Advanced R (2 ed.). Chapman &amp; Hall/CRC the R series. CRC Press/Taylor &amp; Francis Group. https://adv-r.hadley.nz/. Wood, S. N. (2015). Core Statistics. Institute of Mathematical Statistics Textbooks. Cambridge University Press. https://www.maths.ed.ac.uk/~swood34/core-statistics.pdf. 1.5 Acknowledgements Much information used to form these notes came from the above resources. However, Simon Wood’s APTS notes and Charles Geyer’s Statistics 3701 notes have also proved incredibly useful for providing some additional information. 1.6 Assessment Assessment for MTH3045 will comprise two pieces of coursework (worth 30% and 40% of the total module mark, respectively) and an online practical exam (worth 30% of the total module mark). Your coursework must be your on work. 1.7 Motivating example Suppose that we want to find \\(\\bf z\\), where \\(\\mathbf{z} = \\mathbf{ABy}\\). Let’s try this in R. We can use system.time() to measure how long a command takes to execute. We’ll use \\(3000 \\times 3000\\) matrices, A and B, for \\(\\bf A\\) and \\(\\bf B\\), respectively, which we’ll fill with arbitrary (here Normal(0, 1), henceforth \\(N(0, 1)\\)) variates, and fill the \\(n\\)-vector \\(\\bf y\\), called y, similarly. &gt; n &lt;- 3e3 &gt; A &lt;- matrix(rnorm(n * n), n, n) &gt; B &lt;- matrix(rnorm(n * n), n, n) &gt; y &lt;- rnorm(n) &gt; system.time(z1 &lt;- A %*% B %*% y) ## user system elapsed ## 17.116 0.015 17.133 We’re interested in the total time, elapsed, which is 17.133 seconds. However, the next line gives exactly the same answer &gt; system.time(z2 &lt;- A %*% (B %*% y)) ## user system elapsed ## 0.022 0.000 0.022 which we can check with all.equal() &gt; all.equal(z1, z2) ## [1] TRUE yet is about 780 times faster. The point of this example is that, if we understand some of the basics behind computations that we need when fitting statistical models, we can make our fitting much more efficient. Improving efficiency can, for given computational cost, allow us to fit models to more data, for example, from which we should hope to draw more reliable inferences. What’s happened above? In first approach R has essentially calculated \\(\\mathbf{C} = \\mathbf{AB}\\), say, and then \\(\\mathbf{Cy}\\), i.e. multiplied two \\(n \\times n\\) matrices, and then an \\(n \\times n\\) matrix by an \\(n\\)-vector. In the second approach, however, R has essentially calculated \\(\\mathbf{x} = \\mathbf{By}\\) and then \\(\\mathbf{z} = \\mathbf{Ax}\\), say, i.e. two multiplications of \\(n \\times n\\) matrices and \\(n\\)-vectors. By considering the \\(n\\)-vector as an \\(n \\times 1\\) matrix, we should expect multiplying together two \\(n \\times n\\) matrices to require roughly a factor of \\(n\\) times more calculations than multiplying an \\(n \\times n\\) matrix by an \\(n\\)-vector. This is a trivial, but illuminating, example, worth bearing in mind when writing matrix-based code. 1.8 Exploratory and refresher exercises Generate a sample of \\(n = 20\\) \\(N(1, 3^2)\\) random variates, and without using mean(), var() or sd() write R functions to calculate the sample mean, \\(\\bar x\\), sample variance, \\(s^2\\), and sample standard deviation, \\(s\\), where \\[ \\bar x = \\dfrac{1}{n} \\sum_{i = 1}^n x_i \\text{ and } s^2 = \\dfrac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar x)^2. \\] Note than sum() may be used. Consider computing \\(\\text{Pr}(Z \\geq z) = 1 - \\Phi(z)\\) where \\(Z \\sim \\text{Normal}(0, 1)\\), or, for short, \\(Z \\sim N(0, 1)\\). For \\(z = 0, 0.5, 1, 1.5, 2, \\ldots\\) compute this in R in three different ways using the following three commands &gt; pnorm(z, lower.tail = FALSE) &gt; 1 - pnorm(z) &gt; pnorm(-z) and find the lowest value of \\(z\\) for which the three don’t give the same answer. The formula \\(\\text{Var}(Y) = \\text{E}(Y^2) - [\\text{E}(Y)]^2\\) is sometimes called the ‘short-cut’ variance formula, i.e. a short-cut for \\(\\text{Var}(Y) = \\text{E}[Y - \\text{E}(Y)]^2\\). Compare computing \\(\\text{Var}(Y)\\) using the two formulae above for the samples y1 and y2 below. &gt; y1 &lt;- 1:10 &gt; y2 &lt;- y1 + 1e9 "],["ch2.html", "2 Statistical computing in R 2.1 Overview 2.2 Mathematics by computer 2.3 The history of R 2.4 Why R? 2.5 Good practice 2.6 Compiled code with Rcpp 2.7 Bibliographic notes", " 2 Statistical computing in R 2.1 Overview In MTHM3045 we’ll be using the programming language R for computation. Note that RStudio is an Integrated Development Environment (IDE) for R: it simplifies working with scripts, R objects, plotting and issuing commands by putting them all in one place. In MTH3045 we’ll typically be interested in programming, so will just refer to R, but you may want to think of this as code that’s run in RStudio. 2.2 Mathematics by computer In statistical computing, we can usually rely on our computer software to take care of most things in the background. Nonetheless, it’s useful to know the basics of how computers perform calculations, as, if our code isn’t doing what we’d like, or what we’d expect, then such knowledge can help us diagnose any problems. Put simply, if we issue &gt; 1 + 2 ## [1] 3 in R, how does it get to the answer of 3? 2.2.1 Positional number systems Any positive number, \\(z\\), can be represented as a base-\\(B\\) number with the digits \\(\\{0, 1, \\ldots, B-1\\}\\) in the form \\[z = a_kB^k + \\ldots + a_2B^2 + a_1B + a_0 + a_{-1}B^{-1} + a_{-2}B^{-2} + \\ldots \\] for integer coefficients \\(a_j\\) in the set \\(\\{0, 1, \\ldots, B-1\\}\\). We can write this more tidily as \\[(a_k \\ldots a_2 a_1 a_0. a_{-1} a_{-2} \\ldots)_B.\\] The ‘dot’ in this representation is called the radix point, or the decimal point, in the special case of base 10, which is usually referred to simply as ‘decimal’. In a fixed point number system, the radix point is always placed in the same place, i.e. after \\(a_0\\), the coefficient of \\(B^0 = 1\\). So when we write \\(\\pi\\) as 3.14159… in decimal form, we have the decimal point after the 3, which is the coefficient of \\(10^0 = 1\\). Fixed point number systems are easy for humans to interpret. 2.2.2 A historical aside on exact representations of integers Humans now usually use base 10 for mathematics, and computers work in base two. However, humans used to primarily use base 60, known as sexagesimal, for mathematics. The sexagesimal system can be traced back to the Sumerians back in 3000BC. The many factors of 60, e.g., 1, 2, 3, 4, 5, 6, …, 30, 60, were one of its selling points, and it’s still used for some formats of angles and coordinates, and of course time! The decimal number system is attributed to Archimedes (c. 287–212 BCE). 2.2.3 Floating point representation In a floating point number system, the radix point is free to move. Computers use such number systems. Scientific notation, e.g. Avagadro’s number \\[N = 6.023 \\times 10^{23}\\] is an example of such a system. More generally, we can write any positive number in the form \\[M \\times B^{E - e},\\] where \\(M\\) is the mantissa, \\(E\\) is the exponent, and \\(e\\) is the excess, and even more generally, we can write any number in the form \\[S \\times M \\times B^{E - e}, \\] where \\(S\\) is its sign. We may think of \\(S\\) as from the set \\(\\{+, -\\}\\). Hence, for a given base, we can represent any number with the triple \\((S, E, e, M)\\). In base 10 Avogadro’s number can be written \\(N = (+, 24, 0, 0.6023)\\) or \\(N = (+, 23, 0, 6.023)\\). The latter is classed as normalised because its leading term is non-zero. 2.2.4 Single-precision arithmetic Computers work in base \\(B = 2\\), or binary, and usually consider the mantissa to be of the form \\(M = 1 + F\\), where \\(F \\in [0, 1)\\) is the fraction, giving \\[S \\times (1 + F) \\times 2^{E - e}\\] and hence using a normalised representation. Computers use a limited number of bits to store numbers. As a result any number is only stored approximately. Computers vary in how they store different types of number. A commonly-used standard is IEEE 754. Let’s first consider single-precision arithmetic, which uses 32 bits, \\(b_1, \\ldots, b_{32}\\), say, to store numbers. Under the IEEE 754 standard, the order of bits is arbitrary. What each does is defined, so that 1 bit, \\(b_1\\) say, gives the sign, \\(S = (-1)^{b_1}\\), 8 bits, \\(b_2, \\ldots, b_9\\), give the exponent, \\(E = \\sum_{i = 1}^8 b_{i + 1} 2^{8 - i}\\), 23 bits, \\(b_{10}, \\ldots, b_{32}\\), give the fraction, \\(F = \\sum_{i = 1}^{23} b_{i + 9} 2^{-i}\\), with \\(e = 127\\) fixed. Example 2.1 Consider the single-precision representation \\[0~10000000~10010010000111111011011\\] and use R to find the number in decimal form to 20 decimal places. We’ll start with a function bit2decimal() for converting a bit string into a decimal. Producing such a function is beyond the scope of MTH3045, but some aspects of the function may help you with other parts of the module. The comments should give an idea of what each line of bit2decimal() does. Note that bit2decimal() has arguments x, the bit string we want to convert, e for the excess \\(e\\), and dp, which specifies the number of decimal places we want it to return the decimal number to. These are repeated at the start of the function, which is often good practice, especially when sharing code. &gt; bit2decimal &lt;- function(x, e, dp = 20) { + # function to convert bits to decimal form + # x: the bits as a character string, with appropriate spaces + # e: the excess + # dp: the decimal places to report the answer to + bl &lt;- strsplit(x, &#39; &#39;)[[1]] # split x into S, E and F components by spaces + # and then into a list of three character vectors, each element one bit + bl &lt;- lapply(bl, function(z) as.integer(strsplit(z, &#39;&#39;)[[1]])) + names(bl) &lt;- c(&#39;S&#39;, &#39;E&#39;, &#39;F&#39;) # give names, to simplify next few lines + S &lt;- (-1)^bl$S # calculate sign, S + E &lt;- sum(bl$E * 2^c((length(bl$E) - 1):0)) # ditto for exponent, E + F &lt;- sum(bl$F * 2^(-c(1:length(bl$F)))) # and ditto to fraction, F + z &lt;- S * 2^(E - e) * (1 + F) # calculate z + out &lt;- format(z, nsmall = dp) # use format() for specific dp + # add (S, E, F) as attributes, for reference + attr(out, &#39;(S,E,F)&#39;) &lt;- c(S = S, E = E, F = F) + out + } Then we’ll input the numbers in binary form, and call bit2decimal(). &gt; b0 &lt;- &#39;0 10000000 10010010000111111011011&#39; &gt; sing_prec &lt;- bit2decimal(b0, 127) &gt; sing_prec ## [1] &quot;3.14159274101257324219&quot; ## attr(,&quot;(S,E,F)&quot;) ## S E F ## 1.0000000 128.0000000 0.5707964 That’s right: it’s the single-precision representation of \\(\\pi\\). Note that for MTH3045, you’re not expected to repeat a similar calculation. The example is merely designed to show how the given single-precision representation can be converted to a number in conventional format, as shown by R. Also note that the function format() guarantees printing a number to nsmall decimal places. 2.2.5 Double-precision arithmetic R usually uses double-precision arithmetic, which uses 64 bits to store numbers. 1 bit, \\(b_1\\) say, for the sign, \\(S = (-1)^{b_1}\\). 11 bits, \\(b_2, \\ldots, b_{12}\\), for the exponent, \\(E = \\sum_{i = 1}^{{\\color{red}{11}}} b_{i + 1} 2^{\\color{red}{11} - i}\\). 52 bits, \\(b_{13}, \\ldots, b_{64}\\), for the fraction, \\(F = \\sum_{i = 1}^{52} b_{i + 9} 2^{-i}\\). For double-precision arithmetic \\(e = 1023\\). Using twice as many bits essentially brings twice the precision; hence double- instead of single-precision. Example 2.2 Now consider the double-precision representation \\[0~10000000000~1001001000011111101101010100010001000010110100011000\\] and use R to find the number in decimal form to 20 decimal places. &gt; b0 &lt;- &#39;0 10000000000 1001001000011111101101010100010001000010110100011000&#39; &gt; doub_prec &lt;- bit2decimal(b0, 1023) &gt; doub_prec ## [1] &quot;3.14159265358979311600&quot; ## attr(,&quot;(S,E,F)&quot;) ## S E F ## 1.0000000 1024.0000000 0.5707963 Rather repetitively, it’s the double-precision representation of \\(\\pi\\). The constant \\(\\pi\\) is built in to R as pi. We can compare our single- and double-precision approximations to that built in &gt; format(pi - as.double(sing_prec), nsmall = 20) ## [1] &quot;-8.742278e-08&quot; &gt; format(pi - as.double(doub_prec), nsmall = 20) ## [1] &quot;0.00000000000000000000&quot; and we see that our double-precision approximation is exactly the same as that built in, whereas the single-precision version differs by \\(10^{-8}\\) in order of magnitude. Note that our function bit2decimal() generated a character string (which let us ensure it printed a specific number of decimal places), so we use as.double() to convert its output to a double-precision number, which allows direct comparison with pi. Remark. We can overwrite R’s constants. &gt; pi &lt;- 2 &gt; pi ## [1] 2 In general this is a bad idea. The simplest way to fix it is to remove the object we’ve created from R’s workspace with rm(). Then pi reverts back to R’s built in value. &gt; rm(pi) &gt; pi ## [1] 3.141593 Note that R also has T and F built in as aliases for TRUE and FALSE. T and F can be overwritten, but TRUE and FALSE can’t. In general, for example with function arguments, it is better to use argument = TRUE or argument = FALSE, just in case T or F are overwritten with something that could be interpreted as the opposite of what’s wanted, such as issuing T &lt;- 0, since &gt; as.logical(0) ## [1] FALSE 2.2.6 Flops: floating point operations Applying a mathematical operation, such as addition, subtraction, multiplication or division, to two floating point numbers is a floating point operation, or, more commonly, a flop1. The research area numerical analysis includes the analysis of the accuracy of flops, by virtue of numbers being approximately represented. Numerical analysis goes far beyond just analysing flops, and also includes the study of algorithms. Such analysis is far beyond the scope of MTH3045. We will merely cover a couple of examples, which will be illuminating for understanding when flops can go wrong. Such knowledge can help us avoid unnecessary errors. The addition of floating point numbers works by representing the numbers with a common exponent, and then summing their mantissas. Example 2.3 Calculate 123456.7 + 101.7654 using base-10 floating point representations. We first represent both numbers with a common exponent: that of the largest number. Therefore \\[\\begin{align*} 123456.7 &amp;= 1.234567 \\times 10^5\\\\ 101.7654 &amp; = 1.017654 \\times 10^2 = 0.001017654 \\times 10^5. \\end{align*}\\] We next sum their mantissas, i.e. \\[\\begin{align*} 123456.7 + 101.7654 &amp;= (1.234567 \\times 10^5) + (1.017654 \\times 10^2)\\\\ &amp;= (1.234567 \\times 10^5) + (0.001017654 \\times 10^5)\\\\ &amp;= (1.234567 + 0.001017654) \\times 10^5\\\\ &amp;= 1.235584654 \\times 10^5 \\end{align*}\\] Note that if the mantissa was rounded to six decimal places, the result would be \\(1.235584 \\times 10^5\\), and the final three decimal places would effectively be lost. We call the difference between the actual value and that given by the approximate algorithm roundoff error2. Example 2.4 Consider the following calculations in R. &gt; a &lt;- 1e16 &gt; b &lt;- 1e16 + pi &gt; d &lt;- b - a Obviously we expect that \\((1 \\times 10^{16} + \\pi) - 1 \\times 10^{16} = \\pi\\). &gt; d ## [1] 4 But d = 4! So, what’s happened here? Double-precision lets us represent the fractional part of a number with 52 bits. In decimal form, this corresponds to roughly 16 decimal places. Addition (or subtraction) with floating point numbers first involves making common the exponent. So above we have \\[ \\pi = 3.1415926535897932384626 \\times 10^0, \\] but when we align its exponent to that of a we get \\[ \\pi = 0.0000000000000003 \\times 10^{16}. \\] Then mantissas are added (or subtracted); hence \\[ \\texttt{b} = 1.0000000000000003 \\times 10^{16} \\] and so d\\(~= 3\\). This simple example demonstrates cancellation error. (Note that above we had d = 4, because R did its calculations in base 2, whereas we used base 10.) 2.2.7 Some useful terminology The machine accuracy, often written \\(\\epsilon_m\\), and sometimes called machine epsilon, is the smallest (in magnitude) floating point number that, when added to the floating point number 1.0, gives a result different from 1.0. Let’s take a look at this by considering \\(1 + 10^{-15}\\) and \\(1 + 10^{-16}\\). &gt; format(1.0 + 1e-15, nsmall = 18) ## [1] &quot;1.000000000000001110&quot; &gt; format(1.0 + 1e-16, nsmall = 18) ## [1] &quot;1.000000000000000000&quot; The former gives a result different from 1.0, whereas the latter doesn’t. This tells us that \\(10^{-16} &lt; \\epsilon_m &lt; 10^{-15}\\). In fact, R will tell us its machine accuracy, and it’s stored as .Machine$double.eps which is \\(2.220446\\times 10^{-16}\\). Note that \\(2.220446\\times 10^{-16} = 2^{-52}\\), and recall that for double-precision arithmetic we use 52 bits to represent the fractional part. Also note that this is the machine accuracy for double-precision arithmetic, and would be larger for single-precision arithmetic. By using the floating point representation for a number, we are effectively treating it as rational. We should anticipate that any subsequent flop introduces an additional fractional error of at least \\(\\epsilon_m\\). The accumulation of roundoff errors in one or more flops is calculation error. In statistics, underflow can sometimes present problems. Put simply, this is when numbers are sufficiently close to zero that they cannot be differentiated from zero using finite representations, which, for example, results in meaningless reciprocals. Maximum likelihood estimation gives a simple example of when this can occur, because we may repeatedly multiply near-zero numbers together until the likelihood becomes very close to zero. The opposite is overflow, when we have numbers too large for the computer to deal with. This is simple to demonstrate as &gt; log(exp(700)) ## [1] 700 works but &gt; log(exp(710)) ## [1] Inf doesn’t, just because exp(710) is too big. (Here’s a great example where logic, i.e. avoiding taking the logarithm of an exponential, would easily solve the problem.) 2.3 The history of R Before we discuss the history of R, we’ll consider S, which is sometimes considered to be its predecessor. S is a statistical programming language that aimed “to turn ideas into software, quickly and faithfully”, according to John Chambers. Chambers, along with Rick Becker and Allan Wilks, began developing S in 1975 when working for Bell Laboratories, an American industrial research and scientific development company. This was released outside Bell Labs in 1980 as a set of macros, and in 1988 updated to be function based. A commercial implementation of the S programming language, S-PLUS, was also released in 1988, but is no longer available. Ross Ihaka and Robert Gentleman of the University of Auckland began developing the R programming language in 1992. R is, at its core, an open-source implementation of the S programming language. Its name is partly inspired by that of S, and also by its authors’ first-name initials. R was officially released in 1995, which was followed by a ‘stable beta’ version (v1.0) in 2000. The R Core Team, which develops R, includes Chambers, Ihaka and Gentlemen, and 17 other members. As I write this, the latest version of R is v4.1.2, which is called `Bird Hippie’3. According to a survey by IEEE Spectrum, R is currently the 7th most popular programming language (after 1. Python, 2. Java, 3. C, 4. C++, 5. JavaScript, 6. C#). In my opinion, R would not have been as popular as it is if it weren’t free and open-source, and those that develop R could have easily heavily profited financially from it, yet, commendably, their work has instead led to highly regarded and free computer software, which is now frequently and widely used for statistical programming. 2.4 Why R? In MTH3045 we will only consider R for any computations. I could give various reasons for this. Instead, I’ll give the following quote: “…despite its sometimes frustrating quirks, R is, at its heart, an elegant and beautiful language, well tailored for data science.” — Wickham (2019) I concur with this. The reason for this is that R tends to be quite good – or better – at most computational aspects of statistics than other programming languages. For example, as a programming language it is relatively accessible and its computational speed is usually okay. A particularly appealing quality of R is that it is free and open-source. The entire code that comprises the software package can be viewed by anyone. This is perhaps why R has such a strong community, which includes package contributions, and helps keep R up-to-date in terms of the statistical models that its users can fit. However, it is not the only programming language available for statistical computing: Python and MATLAB are other popular choices. Other languages, such as Julia, are growing in popularity. Browsing the internet, you’ll find comparisons of these programming languages. In general the conclusion is that R is slower. However, in MTH3045, it will become apparent that statistical computing is heavily reliant on matrix computations. For these R calls on basic linear algebra subprograms, usually referred to as ‘BLAS’; see http://www.netlib.org/blas/ and https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms. These typically involve incredibly advanced code for matrix algebra, which is often the time-consuming part of statistical computations. If we’re doing computations that heavily rely on BLAS – or can be sensibly manipulated to rely on BLAS - then R becomes an attractive programming language. For these reasons, we use R in MTH3045. Nonetheless, most – if not all – of what we cover in MTH3045 could be achieved in other programming languages. 2.4.1 Basics To get started with R, the development team’s ‘An Introduction to R’ manual, available from https://cran.r-project.org/manuals.html is a good starting point. You’ll find the pdf version on the MTH3045 VLE page. I’ll just pick out some key information that will be vital for MTH3045. 2.4.1.1 How R works R is an interpreted programming language. The console in RStudio, the R GUI, or when using R from a terminal, is a command-line interpreter. This is essentially a program that converts what you request into something of the form that can be passed to another programming language. With R, this is to the ‘low-level’ programming languages C or Fortran. Calculations are based on code written in these languages. So, when we issue &gt; 1.5 + 2.3 ## [1] 3.8 R has very cleverly interpreted (for C in this case) that we’ve passed two numeric vectors, each of length one, and that we want to sum them, and that the result should be a vector of length one. 2.4.1.2 How functions work I’m quite a fan of R‘s help files, but others are not. If you’re ever unsure of how a function works, I suggest first consulting its help file. For example help(lm), or equivalently ?lm, gives help on R’s lm() function. The function is titled ’Fitting Linear Models’. It then has Description, describing what the function does; Usage, detailing how we use the function does, i.e. what command(s) to issue; Arguments, explaining what each argument is, and in what format it should be; Details, where present, goes into a bit more detail; Value, states what we’ll get when we call the function; and then Examples, where present, gives examples of usage (which are often incredibly useful). Note that we can also get help on basic arithmetic functions: e.g. help('*') for multiplication and help('%*%') for matrix multiplication. 2.4.2 Data structures R can handle various different data structures. Here we’ll introduce a few of the more commonly used ones. Familiarity with these will be helpful for various aspects of MTH3045. 2.4.2.1 Vectors We’ll ignore scalars, and start with the vector. Issuing &gt; vec &lt;- c(1, 2, 3) &gt; vec ## [1] 1 2 3 creates the column vector \\((1, 2, 3)^\\text{T}\\), where \\({}^\\text{T}\\) denotes transpose. We may note that this is equivalent to calling 1:3, which is also equivalent to calling seq(1, 3). seq() is a useful function. It generates regular sequences and its usage is &gt; seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)), length.out = NULL) Typically we specify from and to, i.e. the start and end points of the sequence, and then specify its length with length.out, or specify its ‘jumps’ with by. Note that when calling a function, we don’t need to give argument names if we give them in the right order, and we can shorten argument names if the shortenings are unique: &gt; seq(0, 1, 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 &gt; seq(0, 1, by = 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 &gt; seq(0, 1, b = 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 Another useful function for creating a vector is rep(), which replicates a vector. It can be used to repeat a vector one after the other, such as &gt; rep(vec, 3) ## [1] 1 2 3 1 2 3 1 2 3 or to replicate each element of the supplied vector the same number of times &gt; rep(vec, each = 3) ## [1] 1 1 1 2 2 2 3 3 3 or a different number of times &gt; rep(vec, c(2, 1, 3)) ## [1] 1 1 2 3 3 3 2.4.2.2 Matrices We create a matrix with matrix(). Issuing &gt; mat &lt;- matrix(1:6, 2, 3) &gt; mat ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 specifies a matrix with first row \\((1, 3, 5)\\) and second row \\((2, 4, 6)\\). The second and third arguments to matrix() are nrow and ncol, its numbers of rows and columns, respectively. Provided the vector that we specify is equal in length to the product of the matrix’s numbers of rows and columns, we only need to supply one of nrow and ncol, as the other can be determined. R will recycle the supplied vector if its length is below the product of nrow and ncol. Note that R, by default, fills matrices column-wise. We can fill row-wise with argument byrow = TRUE: &gt; mat2 &lt;- matrix(1:8, ncol = 4, byrow = TRUE) &gt; mat2 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 2.4.2.3 Arrays We may think of a vector as a one-column matrix. We can extend this by thinking of a matrix as a two-dimensional array. An array can be of any dimension. Here’s one of dimension \\(2 \\times 3 \\times 4\\). &gt; arr &lt;- array(1:24, c(2, 3, 4)) &gt; arr ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 ## ## , , 3 ## ## [,1] [,2] [,3] ## [1,] 13 15 17 ## [2,] 14 16 18 ## ## , , 4 ## ## [,1] [,2] [,3] ## [1,] 19 21 23 ## [2,] 20 22 24 In R we refer to each dimension as a margin. 2.4.2.4 Lists One of R’s particularly convenient data structures is the list. It is described, by R, as a generic vector: essentially as a vector in which each element can be more complicated than a scalar, which are the elements of a conventional mathematical vector. So a list is a collection of data structures, which might be the same, such as two vectors, or might be different, such as a vector and matrix, or even a vector and another list. For example, &gt; lst &lt;- list(vec, mat) &gt; lst ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 lists are therefore incredibly flexible, and hence incredibly useful. A data.frame is actually just a list, albeit one typically comprising vectors of the same length, and hence printable similarly to a matrix. 2.4.2.5 Attributes Sometimes we might have an object that we want to add a bit more information to. We can do this by assigning it attributes. Suppose we’re interested in the number \\(1 \\times 10^{20}\\). We could store this as 20, if we knew we were storing it in \\(\\log_{10}\\) format. We could use an attribute to remind us of this. &gt; x &lt;- 20 &gt; attr(x, &#39;format&#39;) &lt;- &#39;log10&#39; &gt; x ## [1] 20 ## attr(,&quot;format&quot;) ## [1] &quot;log10&quot; The function attributes() will then show us all the attributes of an object &gt; attributes(x) ## $format ## [1] &quot;log10&quot; and we can use attr() again to access a specific attribute &gt; attr(x, &#39;format&#39;) ## [1] &quot;log10&quot; 2.4.3 Some useful R functions 2.4.3.1 sum(), rowSums() and colSums() You’ll have encountered various R functions during your degree. For example, sum() gives the global sum of a vector, matrix, or even an array: &gt; sum(vec) ## [1] 6 &gt; sum(mat) ## [1] 21 Note that we may want to sum a matrix or array over one or more of its margins. For this we should use rowSums() and colSums(). For a matrix, this is fairly straightforward: &gt; rowSums(mat) ## [1] 9 12 &gt; colSums(mat) ## [1] 3 7 11 For an array, though, there are various options. First note that both rowSums() and colSums() have argument dims, which defaults to 1 and so the following implicitly use dims = 1. This means that first margin is regarded as the row, and the remaining margins are regarded as the columns. &gt; rowSums(arr) ## [1] 144 156 &gt; colSums(arr) ## [,1] [,2] [,3] [,4] ## [1,] 3 15 27 39 ## [2,] 7 19 31 43 ## [3,] 11 23 35 47 If we change dims to dims = 2, the first two margins are now the rows, and the final margin is the column. So rowSums() is over two margins, whereas colSums() is over one: &gt; rowSums(arr, dims = 2) ## [,1] [,2] [,3] ## [1,] 40 48 56 ## [2,] 44 52 60 &gt; colSums(arr, dims = 2) ## [1] 21 57 93 129 Note that rowSums() and colSums() require contiguous margins. If we wanted to sum over the first and third margins of the above array, then we can permute its margins with aperm() so that the first and third margins become the first and second margins: &gt; arr2 &lt;- aperm(arr, c(1, 3, 2)) &gt; rowSums(arr2) ## [1] 144 156 &gt; colSums(arr2) ## [,1] [,2] [,3] ## [1,] 3 7 11 ## [2,] 15 19 23 ## [3,] 27 31 35 ## [4,] 39 43 47 There are also functions rowMeans() and colMeans(), which are self-explanatory. 2.4.3.2 The *apply() family of functions apply() applies a function to margins of an array or matrix. In the following &gt; apply(mat, 1, prod) ## [1] 15 48 we get the product, through function prod(), of each row of mat, and it returns a vector of length the number of rows of mat. The second argument of apply() is the margin over which we want to apply the specified function. Therefore apply(mat, 2, prod) would give product over the columns of mat. We can also apply functions over margins of an array. So &gt; apply(arr, c(1, 3), max) ## [,1] [,2] [,3] [,4] ## [1,] 5 11 17 23 ## [2,] 6 12 18 24 uses max() to find the maximum over margin 2. As margins 1 and 3 are of length 2 and 4, respectively, here apply() returns a \\(2 \\times 4\\) matrix. Note that apply(..., ..., sum) is inefficient in comparison to rowSums() and colSums(). lapply() applies a function over a list. Consider &gt; lapply(lst, sum) ## [[1]] ## [1] 6 ## ## [[2]] ## [1] 21 Note that lapply() returns a list that is the same length as that supplied to it, i.e. that same length as lst here. We might want the result of lapply() simplified and coerced to an array, if possible, which is what sapply() does: &gt; sapply(lst, sum) ## [1] 6 21 Sometimes, though, this isn’t possible, and lapply() and sapply() do the same, such as if the applied function returns results of different length, such as &gt; lst2 &lt;- list(rnorm(4), runif(6)) &gt; lst2 ## [[1]] ## [1] 1.06760493 0.09913499 -0.68227158 -0.89413264 ## ## [[2]] ## [1] 0.8800592 0.5106950 0.6207256 0.6247940 0.4673564 0.3715127 &gt; sapply(lst2, sort) ## [[1]] ## [1] -0.89413264 -0.68227158 0.09913499 1.06760493 ## ## [[2]] ## [1] 0.3715127 0.4673564 0.5106950 0.6207256 0.6247940 0.8800592 which still returns a list. Such a list is sometimes called a ragged array. There are also useful functions tapply() and mapply(), which you may want to explore in your spare time. And if you like them, then take a look a Map() and Reduce()! 2.4.3.3 Other miscellaneous functions There are lots of other handy functions in R. For example, I often use split(), match(), combn() and expand.grid(). I won’t try and list all the handy functions here. This sample, however, may give you an idea of the breadth of function that R has to offer. Put simply, if you’re looking for a function in R, there’s a good chance that it’s there. Sometimes, though, the tricky bit is knowing what to search the web for in order find what the function you want is called! 2.4.4 Control structures So far we’ve considered executing all of our lines of code. Sometimes, we may want to control how our code is executed according to some logic. We can do this with control structures. For example, conditional execution only runs lines of code if one or more conditions is met, and repeated execution repeatedly runs lines of code until one or more stopping conditions is met. We sometimes refer to a calculation that’s repeated over and over again as a loop. For example, &gt; n &lt;- 10 &gt; x &lt;- integer(n) &gt; for (i in 1:n) { + x[i] &lt;- i + } &gt; x ## [1] 1 2 3 4 5 6 7 8 9 10 creates an empty integer vector, x, of length n, and then for is a loop that changes the \\(i\\)th value to \\(i\\), for \\(i = 1, \\ldots, 10\\). (Of course, these first five lines of code are equivalent to x &lt;- 1:10.) Another commonly used control structure is the if() condition, which is often coupled with an else condition. Without an else condition, nothing happens if the if() condition isn’t met. Here’s a simple example using if() and a random draw from the Uniform[0, 1] distribution to mimic a coin toss. &gt; u &lt;- runif(1) &gt; if (u &gt; 0.5) { + x &lt;- &#39;head&#39; + } else { + x &lt;- &#39;tail&#39; + } &gt; x ## [1] &quot;head&quot; &gt; u ## [1] 0.6922064 We could then combine the for() and if() control flows to mimic repeated coin tosses. The following gives ten: &gt; n &lt;- 10 &gt; x &lt;- character(n) &gt; for (i in 1:10) { + u &lt;- runif(1) + if (u &gt; 0.5) { + x[i] &lt;- &#39;head&#39; + } else { + x[i] &lt;- &#39;tail&#39; + } + } &gt; x ## [1] &quot;tail&quot; &quot;head&quot; &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;head&quot; &quot;tail&quot; &quot;head&quot; &quot;tail&quot; &quot;head&quot; The function ifelse() can tidy up if ... else ... calls. Equivalently to above we could use ifelse(runif(1) &gt; 0.5, 'head', 'tail'), so that the first argument is the if condition, the second is what’s returned if it’s TRUE and the third is what’s returned if it’s FALSE. Note that for() loops can be a bit untidy, especially if what’s inside the loops is just a line of code. Then replicate() can be useful. Now that we know ifelse(), we could have just used either of the following &gt; replicate(10, ifelse(runif(1) &gt; 0.5, &#39;head&#39;, &#39;tail&#39;)) ## [1] &quot;head&quot; &quot;tail&quot; &quot;tail&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;tail&quot; &quot;head&quot; &quot;head&quot; &gt; ifelse(runif(10) &gt; 0.5, &#39;head&#39;, &#39;tail&#39;) ## [1] &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;tail&quot; &quot;head&quot; &quot;tail&quot; which is equivalent to the above, once the randomness of the call is taken into account. In the above, we fixed how many coin tosses we wanted, but we might want to stop when we’ve reached a given number of heads (or tails). For such a random stopping condition, we can use the while() control flow. The following stops when we reach four heads. &gt; x &lt;- character(0) &gt; while(sum(x == &#39;head&#39;) &lt; 4) { + u &lt;- runif(1) + if (u &gt; 0.5) { + x &lt;- c(x, &#39;head&#39;) + } else { + x &lt;- c(x, &#39;tail&#39;) + } + } &gt; x ## [1] &quot;head&quot; &quot;head&quot; &quot;tail&quot; &quot;tail&quot; &quot;head&quot; &quot;head&quot; Note that above x started as a zero-length vector, and grew at each iteration. This is useful if we don’t know how long x needs to be. If we do, it’s better to set the vector’s length at the start. R also has the function repeat() for repeating a set of code. Somewhere this will need a stopping condition that invokes the break command; otherwise the code will be repeated forever! 2.4.5 Vectorisation A very useful skill to adopt in R early on is to vectorise your code, where possible. Vectorisation typically involves programming with vectors instead of scalars, when such an approach makes sense. Often this means avoiding writing for() loops. Two reasons for this, given in Wickham (2019), are: It makes problems simpler. Instead of having to think about the components of a vector, you can only think about entire vectors. The loops in a vectorised function are written in C instead of R. Loops in C are much faster because they have much less overhead. Here are a few illuminating examples. Later we’ll see what we’ve gained in efficiency. Suppose we’ve got a vector comprising some NAs (note that we use NA in R when a value is ‘not available’, such as missing data) and we want to swap them all for zeros. The function is.na() tells us which elements in a vector (or matrix or array) are NA. &gt; x &lt;- c(-2.09, NA, -0.25, NA, NA, 0.52, NA, 0.48, 0.29, NA) &gt; is.na(x) ## [1] FALSE TRUE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE We can then subset those elements in a vectorised way and set them to zero. &gt; x[is.na(x)] &lt;- 0 &gt; x ## [1] -2.09 0.00 -0.25 0.00 0.00 0.52 0.00 0.48 0.29 0.00 We could easily have gone through each element with a for() loop, and swapped each NA element one at a time for a zero, but that would have taken more complicated code, and, for large problems, such as very long vectors, would be slower at performing the operation. The tidiness of vectorisation should reduce the chance of errors in code, too. Another rather convincing example is if we have a vector and want to know in which interval, from a set of intervals, each of its elements fall. &gt; n &lt;- 10 &gt; x &lt;- runif(10) &gt; x ## [1] 0.8537695 0.2586036 0.9802115 0.9582944 0.3119767 0.1526501 0.7655764 ## [8] 0.1780375 0.4892205 0.1074371 &gt; intervals &lt;- seq(0, 1, by = 0.1) &gt; m &lt;- length(intervals) - 1 &gt; which_interval &lt;- integer(n) &gt; for (i in 1:n) { + for (j in 1:m) { + if (x[i] &gt; intervals[j] &amp; x[i] &lt;= intervals[j + 1]) { + which_interval[i] &lt;- j + } + } + } &gt; which_interval ## [1] 9 3 10 10 4 2 8 2 5 2 As you might imagine, we’re probably not the first person to want to perform such a calculation, and R thinks this too. Hence we can simply use its vectorised findInterval() function to perform the equivalent calculation to that above. &gt; findInterval(x, intervals) ## [1] 9 3 10 10 4 2 8 2 5 2 A related function is cut(), which you may want to explore in your own time. 2.5 Good practice 2.5.1 Useful tips to remember when coding 2.5.1.1 Use scripts: don’t type in the Console When programming in R, work from scripts or RMarkdown documents. 2.5.1.2 Use functions for repeated calculations If you’re using a piece of code more than once, it should probably be formulated as a function. 2.5.1.3 Avoid repeating lines of code If you’re copying and pasting code, think about whether this can be avoided. Here’s an example of ‘the bad’. &gt; n &lt;- 5 &gt; x &lt;- matrix(0, nrow = n, ncol = 4) &gt; x[, 1] &lt;- rnorm(n, 0, 1.0) &gt; x[, 2] &lt;- rnorm(n, 0, 1.5) &gt; x[, 3] &lt;- rnorm(n, 0, 2.0) &gt; x[, 4] &lt;- rnorm(n, 0, 3.0) Here’s an example of something better, given n and x above. &gt; sds &lt;- c(1.0, 1.5, 2.0, 3.0) &gt; for (i in 1:4) { + x[, i] &lt;- rnorm(n, 0, sds[i]) + } An improvement would be to swap for (i in 1:4) with for (i in 1:length(sds)) or for (i in 1:ncol(x)), as then we’re not relying on remembering the length of sds or equivalently the number of columns of x. Even more tidily, we could use either of the following: &gt; x1 &lt;- sapply(sds, rnorm, n = n, mean = 0) &gt; x2 &lt;- sapply(sds, function(z) rnorm(n, 0, z)) For x1 we’ve relied on knowing the order of the arguments to rnorm(), and by fixing its first and second arguments, n and mean, R knows that the first argument that we’re supplying to sapply() should be sd, its third argument, i.e. rnorm()’s first free argument. This approach relies on good knowledge of a function’s arguments. 2.5.1.4 Write with style In R we use the &lt;- symbol to assign an object to a name. The left-hand side of the &lt;- symbol is the name we’re giving the object, and the right-hand side is the code that defines the object. Wickham (2019, sec. 5.1.2) states: ‘Variable and function names should be lowercase. Use an underscore to separate words within a name.’ Particularly usefully, Wickham (2019, sec. 5.1.2) also states: ‘Strive for names that are concise and meaningful (this is not easy!)’. It is not easy, but worth aiming towards, especially if you’re re-using an object multiple times. Spacing is particularly useful for helping the appearance of code. For example, the two lines of code &gt; x1 &lt;- sapply(sds, rnorm, n = n, mean = 0) &gt; x1&lt;-sapply(sds,rnorm,n=n,mean=0) will both make the same object x1, but, I hope you’ll agree, the first line is easier to read. In general, spacing should be used either side of &lt;-, mathematical operators (e.g. =, +, *, and control flows (e.g. if (...) not if(...)), and after commas. Spacing can also be used to align arguments, such as &gt; x &lt;- list(a = c(1, 2, 3, 4, 5, 6), + b = c(7, 8, 9, 10, 11, 12)) which can sometimes make code more readable. 2.5.1.5 Comment your code What a line of code does might be self-explanatory, but might not. When it’s not, add comments to explain what it does. This is particularly useful at the start of a function, when what a function does and its arguments can be stated. &gt; fn &lt;- function(x, y, z) { + # function to compute (x + y) * z element-wise + # x, y and z can be compatible scalars, vectors, matrices or arrays + xplusy &lt;- x + y + xplusy * z + } &gt; fn(2, 3, 4) ## [1] 20 Remark. In the above we could – and should – have just done (x + y) * z on one line, but we’ll find the formulation above useful for later. Commenting code is essential if you’re sharing code. You will be sharing code in MTH3045 to submit assignments. Marks will be given for sensible commenting, and may also be given if comments make clear your intentions, even if the code itself contains errors. Commenting is one of those things when coding that can take a bit of discipline. It’s almost always more exciting, for example, to produce a working function as quickly as possible, than to take a bit longer and also comment that function. I tend to think there’s a balance between when to comment and when to code, and might delay commenting until a function is finished. Always comment code, though, while it’s fresh in your mind. 2.5.2 Debugging When we write code in R, we don’t always get it right first time. The errors or, more commonly, bugs in our code may not be straightforward to spot. We call identifying bugs debugging. We want an efficient strategy to identify bugs so that we can fix them. Let’s assume that our bug lies within a function somewhere. We’ve tried to run the function, and it’s failed. I tend to go through the following sequence of events. Read the error message R has returned, if any. From this we may be able to deduce where the bug in our code is. If 1. fails, inspect the code and hope to spot the bug (if there’s any hope of spotting it); otherwise Use some of R’s functions to help us debug our function. Somewhere within our function, something must not be as we expect. We can inspect what’s inside a function with debug(). I prefer debugonce(), which inspects a function once, as opposed to every time. Suppose we want to debug fn() above. We simply issue &gt; debugonce(fn) &gt; fn(2, 3, 4) in the Console, and then we’ll end up inside the function. So if we type x is the Console, R will print 2. It will also be showing the line of code that it’s about to execute, which will be xplusy &lt;- x + y. If we hit Enter then R will run the line of code. If we then type xplusy in the Console R will print 5. Debugging lets us sequentially go through each line of a function. When debugging, R will also throw up an error once we try and execute the line of code where the bug is. This approach to debugging can help us find the offending line of code, and then we may want to debug again up to that line, in order to find what the problem is. When our function has many lines of code, and we know the bug to be near the end, we may want to choose from which point of the function our debugging starts. We can do this with browser(). 2.5.3 Profiling and benchmarking You may have heard of the term Big Data. Essentially this means lots of data. “The definition of big data is data that contains greater variety, arriving in increasing volumes and with more velocity. This is also known as the three Vs. Put simply, big data is larger, more complex data sets, especially from new data sources.” — (https://www.oracle.com/uk/big-data/what-is-big-data/) The more data we attempt to fit a statistical model to, the more flops involved, and, in general, the longer it takes to fit and the more memory it needs. Typically we should try and use all the useful data that we can. If data aren’t useful, we shouldn’t use them. Profiling is the analysis of computer code, typically of its time taken or memory used. Let’s consider a matrix-based update to fn() above, which we’ll call fn2(), and computes \\((A + B) C\\) for matrices \\(A\\), \\(B\\) and \\(C\\). &gt; fn2 &lt;- function(x, y, z) { + # function to compute (x + y) %*% z + # x, y and z are matrices with compatible dimensions + xplusy &lt;- x + y + xplusy %*% z + } &gt; n &lt;- 5e2 &gt; p &lt;- 1e4 &gt; A &lt;- matrix(runif(n * p), n, p) &gt; B &lt;- matrix(runif(n * p), n, p) &gt; C &lt;- matrix(runif(n * p), p, n) We can profile with Rprof(). Here we’ll ask it to write what it’s currently doing to file profile.txt every \\(0.00001\\) seconds. Note that Rprof(NULL) ends the profiling. &gt; Rprof(&#39;profile.txt&#39;, interval = 1e-5) &gt; D &lt;- fn2(A, B, C) &gt; Rprof(NULL) Here’s what’s in profile.txt ## sample.interval=10 ## &quot;+&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; ## &quot;%*%&quot; &quot;fn2&quot; which is two-column output, as there are at most two functions active. The second column is the first function to be called and then the second is any subsequent functions. From the first column, we see that after 0.00001 seconds R is evaluating function +, i.e. matrix addition; and for the next 17 0.00001-second intervals R is evaluating function %*%, i.e. matrix multiplication. The second column tells us that R is evaluating fn2 throughout. For \\(n \\times p\\) matrices, matrix addition requires \\(np\\) additions, whereas matrix multiplication requires \\(p\\) multiplications and \\(p - 1\\) additions, each repeated \\(np\\) times. Considering only the dominant terms, we write the computational complexity of matrix multiplication as \\(O(np^2)\\) whereas that of matrix addition is \\(O(np)\\), which uses so-called ‘big-O’ notation4. Instead of trying to interpret the output of Rprof(), R’s summaryRprof() function will do that for us &gt; summaryRprof(&#39;profile.txt&#39;) ## $by.self ## [1] self.time self.pct total.time total.pct ## &lt;0 rows&gt; (or 0-length row.names) ## ## $by.total ## total.time total.pct self.time self.pct ## &quot;fn2&quot; 0 100.00 0 0.00 ## &quot;%*%&quot; 0 94.44 0 94.44 ## &quot;+&quot; 0 5.56 0 5.56 ## ## $sample.interval ## [1] 1e-05 ## ## $sampling.time ## [1] 0.00018 by working out the percentage of information in the Rprof() output attributable to each unique line of output. Profiling can be useful for finding bottlenecks in our code, i.e. lines that heavily contribute to the overall computational expense (e.g. time or memory) of the code. If we find a bottleneck and it’s unbearably slow and we think there’s scope to reduce or eliminate it without disproportionate effort, then we might consider changing our code to make it more efficient. We’ll focus on efficiency in terms of times taken for commands to execute. Suppose that we’ve put together some code, which we consider to be the benchmark that we want to improve on. Comparison against a benchmark is called benchmarking. One of the simplest ways to benchmark in R is with system.time(), which we saw in the first lecture. Suppose we’ve got the following line in our code, based on matrix A above. &gt; a_sum &lt;- apply(A, 1, sum) The following tells us how long it takes to execute. &gt; system.time(a_sum &lt;- apply(A, 1, sum)) ## user system elapsed ## 0.058 0.016 0.074 This gives us three timings. The last, elapsed, tells use how long our code has taken to execute in seconds. Then user and system partition this total time into so-called ‘user time’ and ‘system time’. Their definitions are operating system dependent, but this information from the R help file for proc.time() gives as idea. “The ‘user time’ is the CPU time charged for the execution of user instructions of the calling process. The ‘system time’ is the CPU time charged for execution by the system on behalf of the calling process.” We’ll just consider elapsed time for MTH3045. For benchmarking in R we’ll consider the microbenchmark::microbenchmark(). Note that this notation refers to function microbenchmark() within the microbenchmark package. If we have the microbenchmark installed, we can either issue microbenchmark::microbenchmark() to use the function or load the package, i.e. library(microbenchmark), and then just use microbenchmark(). I’ll initially use the :: notation to introduce any functions in R that aren’t loaded by default when R starts. &gt; library(microbenchmark) &gt; microbenchmark( + apply(A, 1, sum), + rowSums(A) + ) ## Unit: milliseconds ## expr min lq mean median uq max ## apply(A, 1, sum) 57.342651 76.683340 77.121356 77.202838 77.870379 112.763648 ## rowSums(A) 7.387562 7.523843 7.627246 7.552682 7.641306 9.340947 ## neval ## 100 ## 100 The microbenchmark::microbenchmark() function is particularly handy because it automatically chooses its units, which here is milliseconds. For functions that take longer to execute it might, for example, choose seconds. The output of microbenchmark::microbenchmark() includes neval, the number of evaluations it’s used for each function; from these, the range and quartiles are calculated. If we compare medians, we note that the rowSums() approach is about an order magnitude faster than the apply() approach. We should note that for either approach, timings differ between evaluations, even though they’re doing the same calculation and giving the same answer. On this occasion the minimum and maximum times are between two and three factors different. Note that we could also use benchmark::rbenchmark() for benchmarking, which gives similar details to system.time(). For MTH3045, I’ll use microbenchmark::microbenchmark(), because I find its output more useful. 2.6 Compiled code with Rcpp We’ve seen that vectorised functions can simplify our code (and later we’ll see that they can bring considerable gains in computation time). Suppose, though, that we want a vectorised function, but that it doesn’t exist for our purposes. We could write a function in C or FORTRAN. However, using Rcpp is much more convenient. Rcpp is an R package that efficiently and tidily links R and C++. Let’s consider a simple example of a function to calculate the sum of a vector. &gt; sum_R &lt;- function(x) { + out &lt;- 0 + for (i in 1:length(x)) + out &lt;- out + x[i] + out + } Obviously, we should use sum(), but if it wasn’t available to us, then the above would be an alternative. Consider the following C++ function, which I’ve got stored as sum_Rcpp.cpp, so that the contents of the .cpp file are as below. &gt; #include &lt;RcppArmadillo.h&gt; + // [[Rcpp::depends(RcppArmadillo)]] + + // [[Rcpp::export]] + double sum_Rcpp(arma::vec x) { + double out = 0.0; + int n = x.size(); + for (int i = 0; i++; i &lt; n) { + out += x[i]; + } + return out; + } We won’t go into great detail on Rcpp in MTH3045. The purpose of this section is to raise awareness of its existence, should you ever need it. In the above .cpp file: #include &lt;RcppArmadillo.h&gt; and \\\\ [[Rcpp::depends(RcppArmadillo)]] point to the RcppArmadillo library; // [[Rcpp::export]] makes a function that is visible to R; double sum_Rcpp(arma::vec x) { specifies that our function returns a double, i.e. a single value of double-precision, is called sum_Rcpp, and that its one argument is a vector, hence arma::vec, which we’re calling x; double out = 0.0; forms the double that will be returned, and sets its initial value to 0.0; int n = x.size(); finds the length of x and stores it as an integer called n; for (int i = 0; i++; i &lt; n) { initiates a loop: we’ve called our index i and specified that it’s an integer; then we’ve specified that it goes up by one each time with i++, and stops at n - 1 with i &lt; n. (Note here that indices in C++ start at zero, whereas they start at one in R.) We then update out at each iteration by adding x[i]. (Note that out += ... is equivalent to out = out + ... .) We then close the loop, and specify what we return. An important difference between R and C++ code is that for the latter we’re specifying what’s an integer and what’s a double. Then Rcpp::sourceCpp() checks that what we’ve specified is okay. By not specifying these normally in R, time is needed to interpret the code. We avoid these overheads with C++ code. The trade-off is that C++ code usually takes a bit longer to write, because we need to think about the form of our data, whereas R can handle most of this for us. To use the cpp function in R, we compile it with Rcpp::sourceCpp(). We’re also going to load RcppArmadillo, which gives access to the excellent Armadillo C++ library for linear algebra and scientific computing, more details of which can be found at http://arma.sourceforge.net/docs.html. We can then perform a quick benchmark on what we’ve done. &gt; library(RcppArmadillo) &gt; Rcpp::sourceCpp(&#39;sum_Rcpp.cpp&#39;) &gt; x &lt;- runif(1e3) &gt; microbenchmark::microbenchmark( + sum_R(x), + sum_Rcpp(x), + sum(x) + ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## sum_R(x) 24440 24824.0 43143.32 25007.5 25300.5 1818372 100 ## sum_Rcpp(x) 1527 1648.5 7116.85 1705.5 1808.5 521960 100 ## sum(x) 841 866.0 938.48 931.0 951.0 1926 100 We see that sum_Rcpp() is typically at least an order of magnitude faster than sum_R(). However, it’s still slower than sum(), because it’s one of R’s functions that’s heavily optimised for efficiency. It’s great that we have such efficiency at our disposal. 2.7 Bibliographic notes For further details on topics covered in this chapter, consider the following. Positional number systems: Press et al. (2007, sec. 1.1.1) and Monahan (2011, sec. 2.2). Fundamentals of programming in R: W. N. Venables and R Core Team (2021), Wickham (2019, Ch. 2) and Grolemund (2014, Ch. 1-5). Profiling and benchmarking: Wickham (2019, Ch. 22-24) and almost all of Gillespie and Lovelace (2016), especially Section 1.6. R coding style: Various parts of Wickham (2019) and Gillespie and Lovelace (2016). Note that in MTH3045 we’ll use flops as the plural of flop. It is also often used to abbreviate floating point operations per second, but for that we’ll use flop/s. For reference, ENIAC, the first (nonsuper)-computer, processed about 500 flop/s in 1946. My desktop computer can apparently complete 11,692,000,000 flop/s. The current record, set by Japanese supercomputer Fugaku in June 2020, is 415.5 petaflops, i.e. 415,500,000,000,000,000 flop/s!↩︎ A rather catastrophic example of roundoff error is that of the Ariane rocket launched on June 4, 1996 by the European Space Agency. Ultimately, it caused the rocket to be destoryed on its 37th flight, which the interested reader can read more about here and on various other parts of the web. The Patriot missile is another well-known example.↩︎ I’d always wondered where the names for different R versions come from. Then, when putting these lecture notes together, I decided to find out, and came across Lucy D’Agostino McGowan’s blog entry R release names. Anyway, the answer is that they’re inspired by Peanuts comic strips, films and badges.↩︎ big-O notation – Consider functions \\(f()\\) and \\(g()\\). We write \\(f(x) = O(g(x))\\) if and only if there exist constants \\(N\\) and \\(C\\) such that \\(|f(x)| \\leq C |g(x)|~\\forall~x&gt;N\\). Put simply, this means that \\(f()\\) does not grow faster than \\(g()\\).↩︎ "],["matrix-based-computing.html", "3 Matrix-based computing 3.1 Motivation 3.2 Definitions 3.3 Special matrices 3.4 Systems of linear equations 3.5 Matrix decompositions 3.6 Sherman-Morrison formula / Woodbury matrix identity 3.7 Bibliographic notes", " 3 Matrix-based computing 3.1 Motivation Perhaps surprisingly, much of a computation that we do when fitting a statistical model can be formulated with matrices. The linear model is a prime example. In this chapter we’ll explore some key aspects of matrices and calculations involving them that are important for statistical computing. A particularly useful reference for matrices, especially in the context of statistical computing, is the Matrix Cookbook (Petersen and Pedersen (2012)). 3.2 Definitions 3.2.1 Matrix properties Let’s consider an \\(n \\times n\\) matrix \\(\\bf A\\) and \\(n \\times p\\) matrix \\(\\bf B\\). We will let \\(A_{ij}\\), for \\(i, j = 1, \\ldots, n\\) denote the \\((i, j)\\)th element of \\(\\bf A\\), i.e. in row \\(i\\) and column \\(j\\). We’ll assume that \\(\\bf A\\) is stored in R as A. &gt; A %*% B # computes AB for matrices A and B Definition 3.1 (Real matrix) \\(\\bf A\\) is real if all its elements are real numbers. (We’ll only consider real matrices in MTH3045, so ‘real’ may be taken as given.) &gt; !is.complex(A) &amp;&amp; is.finite(A) # checks whether a matrix A is real Definition 3.2 (Matrix transpose) The transpose of a matrix, denoted \\(\\mathbf{A}^\\text{T}\\), is given by interchanging the rows and columns of \\(\\bf A\\). &gt; t(A) # computes the transpose of a matrix A Definition 3.3 (Cross product) The cross product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is \\(\\mathbf{A}^\\text{T} \\mathbf{B}\\). &gt; crossprod(A, B) # computes t(A) %*% B Remark. crossprod(A, B) is more efficient that t(A) %*% B because R recognises that it doesn’t need to transpose A and can instead perform a modified matrix multiplication in which the columns of A are multiplied by the columns of B. &gt; tcrossprod(A, B) # computes A %*% t(B) Remark. crossprod(A) is equivalent to crossprod(A, A) and tcrossprod(A) to tcrossprod(A, A). Definition 3.4 (Diagonal matrix) A matrix is diagonal if its values are zero everywhere, except for its diagonal, i.e. \\(A_{ij} = 0\\) for \\(i \\neq j\\). Definition 3.5 (Square matrix) A matrix is square if it has the same numbers of rows and columns. Definition 3.6 (Rank) The rank of \\(\\bf A\\), denoted rank(\\(\\bf A\\)), is the dimension of the vector space generated (or spanned) by its columns. This corresponds to the maximal number of linearly independent columns of \\(\\bf A\\). A matrix is of full rank if its rank is equal to its number of rows. The following apply only to square matrices. The \\(n \\times n\\) identity matrix, denoted \\({\\bf I}_n\\), is diagonal and all its diagonal elements are one. &gt; diag(n) # creates the n x n identity matrix for integer n \\(\\bf A\\) is orthogonal if \\(\\mathbf{A}^\\text{T}\\mathbf{A} = {\\bf I}_n\\) and \\(\\mathbf{A}\\mathbf{A}^\\text{T} = {\\bf I}_n\\). \\(\\bf A\\) is symmetric if \\({\\bf A} = {\\bf A}^\\text{T}\\). The trace of \\(\\bf A\\), denoted tr(\\(\\bf A\\)), is the sum of its diagonal entries, i.e. tr(\\(\\bf A\\)) = \\(\\sum_{i = 1}^n A_{ii}\\). In R, diag(A) extracts the diagonal elements of A, and so sum(diag(A)) computes the trace of A. \\(\\bf A\\) is invertible is there exists a matrix \\(\\bf B\\) such that \\({\\bf A} {\\bf B} = {\\bf I}_n\\). Note that \\(\\bf B\\) must be \\(n \\times n\\). The inverse of \\(\\bf A\\), if it exists, is denoted \\({\\bf A}^{-1}\\). &gt; solve(A) # computes the inverse of A A symmetric matrix \\(\\bf A\\) is positive definite if \\({\\bf x}^\\text{T} {\\bf A} {\\bf x} &gt; 0\\) for all non-zero \\(\\bf x\\), i.e. provided all elements of \\(\\bf x\\) aren’t zero. (Changes to the inequality define positive semi-definite (\\(\\geq\\)), negative semi-definite (\\(\\leq\\)), and negative definite (\\(&lt;\\)) matrices, but in statistical computing it’s usually positive definite matrices that we encounter.) Example 3.1 (Hilbert matrix) The Hilbert matrix, \\(\\mathbf{H}_n\\), is the \\(n \\times n\\) matrix with \\((i, j)\\)th elements \\(1 / (i + j - 1)\\) for \\(i, j = 1, \\ldots, n\\). Write a function to form a Hilbert matrix for arbitrary \\(n\\). Use this to form \\(\\mathbf{H}_3\\) and then check whether the matrix that you have formed is symmetric. There are many ways that we could write this function. We should, though, avoid a for loop. Here’s one option. &gt; hilbert &lt;- function(n) { + # Function to evaluate n by n Hilbert matrix. + # Returns n by n matrix. + ind &lt;- 1:n + 1 / (outer(ind, ind, FUN = &#39;+&#39;) - 1) + } &gt; H &lt;- hilbert(3) &gt; H ## [,1] [,2] [,3] ## [1,] 1.0000000 0.5000000 0.3333333 ## [2,] 0.5000000 0.3333333 0.2500000 ## [3,] 0.3333333 0.2500000 0.2000000 A matrix is symmetric if it and its transpose are equal. There a various ways we can check this. &gt; H - t(H) # should be all zero ## [,1] [,2] [,3] ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 &gt; all.equal(H, t(H)) # should be TRUE ## [1] TRUE Or we can turn to isSymmetric(), which is R’s function for checking matrix symmetry. &gt; isSymmetric(H) # should be TRUE ## [1] TRUE Example 3.2 (Evaluating the multivariate Normal pdf) Let \\({\\bf Y} \\sim MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) denote a random \\(p\\)-vector with a multivariate Normal (MVN) distribution that has mean vector \\({\\boldsymbol \\mu}\\) and variance-covariance matrix \\({\\boldsymbol \\Sigma}\\). Its probability density function (pdf) is then \\[\\begin{equation} f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\dfrac{1}{\\sqrt{(2 \\pi)^p | \\boldsymbol{\\Sigma} |}} \\exp\\left\\{ -\\dfrac{1}{2} (\\mathbf{y} - \\boldsymbol{\\mu})^\\text{T} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}) \\right\\}. \\tag{3.1} \\end{equation}\\] Thus note that, to compute the MVN pdf, we need to consider both the determinant and inverse of \\(\\boldsymbol \\Sigma\\), amongst other calculations. Write a function dmvn1() to evaluate its pdf in R, and then evaluate \\(\\log f({\\bf y} \\mid {\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) for \\[ {\\bf y} = \\left( \\begin{array}{c} 0.7\\\\ 1.3\\\\ 2.6 \\end{array} \\right), \\hspace{1cm} \\boldsymbol{\\mu} = \\left( \\begin{array}{c} 1\\\\ 2\\\\ 3 \\end{array} \\right), \\hspace{1cm} \\boldsymbol{\\Sigma} = \\left( \\begin{array}{rrr} 4 &amp; 2 &amp; 1\\\\ 2 &amp; 3 &amp; 2\\\\ 1 &amp; 2 &amp; 2\\\\ \\end{array} \\right). \\] The function dmvn1() below evaluates the multivariate Normal pdf &gt; dmvn1 &lt;- function(y, mu, Sigma, log = TRUE) { + # Function to evaluate multivariate Normal pdf + # y and mu are vectors + # Sigma is a square matrix + # log is a logical + # Returns scalar, on log scale, if log == TRUE. + p &lt;- length(y) + res &lt;- y - mu + out &lt;- - 0.5 * determinant(Sigma)$modulus - 0.5 * p * log(2 * pi) - + 0.5 * t(res) %*% solve(Sigma) %*% res + if (!log) + out &lt;- exp(out) + out + } although we’ll later see that this is a crude attempt. Then the following create \\({\\bf y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as objects y, mu and Sigma, respectively. &gt; y &lt;- c(.7, 1.3, 2.6) &gt; mu &lt;- 1:3 &gt; Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) Then we evaluate \\(\\log f({\\bf y} \\mid {\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) with &gt; dmvn1(y, mu, Sigma) ## [,1] ## [1,] -3.654535 ## attr(,&quot;logarithm&quot;) ## [1] TRUE Note that above determinant()$modulus directly calculates log(det()), and is usually more reliable, so should be used when possible. Remark. In general, it is much more sensible to work with log-likelihoods, and then if the likelihood itself is actually sought, simply exponentiate the log-likelihood at the end. This has been implemented for dmvn1(). This will sometimes avoid underflow. 3.3 Special matrices 3.3.1 Diagonal, band-diagonal and triangular matrices The following gives examples of various special types of square matrix, which we sometimes encounter in statistical computing. These are diagonal (as defined above), tridiagonal, block diagonal, band and lower triangular matrices. Instead of defining them formally, we’ll just show schematics of each. These are plotted in Figure 3.1 with image(), which plots the rows along the \\(x\\)-axis and columns across the \\(y\\)-axis. Hence, to visualise actually looking at the matrix written down on paper, each plot should be considered rotated clockwise through 90 degrees. Figure 3.1: Schematics of diagonal, tridiagonal, block diagonal, band and lower triangular matrices. 3.3.2 Sparse matrices Definition 3.7 (Sparse matrix) A matrix is sparse if most of its elements are zero. Remark. The definition of a sparse matrix is rather vague. Although no specific criterion exists in terms of the proportion of zeros, some consider that the number of non-zero elements should be similar to the number of rows or columns. 3.4 Systems of linear equations Systems of linear equations of the form \\[\\begin{equation} \\mathbf{Ax} =\\mathbf{b}, \\tag{3.2} \\end{equation}\\] where \\(\\bf A\\) is an \\(n \\times n\\) matrix and \\(\\bf x\\) and \\(\\bf b\\) are \\(n\\)-vectors are often encountered in statistical computing. The multivariate Normal pdf of equation (3.1) is one example: we don’t need to compute \\({\\boldsymbol \\Sigma}^{-1}\\) and then calculate \\({\\bf z} = {\\boldsymbol \\Sigma}^{-1} ({\\bf y} - {\\boldsymbol \\mu})\\); instead, left-multiplying by \\({\\boldsymbol \\Sigma}\\), we can recognise that \\({\\bf z}\\) is the solution to \\({\\boldsymbol \\Sigma} {\\bf z} = {\\bf y} - {\\boldsymbol \\mu}\\). R’s solve() function can not only invert a matrix, but can also solve a system of linear equations. Given equation (3.2), suppose we have \\(\\bf A\\) and \\(\\bf b\\) stored as A and b, respectively, then we obtain \\(\\bf x\\), which we’ll store as x, with x &lt;- solve(A, b). Example 3.3 (Evaluating the multivariate Normal pdf by solving systems of linear equations) Modify the function dmvn1() used in Example 3.2 to give a new function dmvn2() in which, instead of inverting \\(\\boldsymbol{\\Sigma}\\), the system of linear equations \\(\\boldsymbol{\\Sigma}(\\mathbf{y} - \\boldsymbol{\\mu})\\) is solved. We simply need to replace solve(Sigma) %*% res with solve(Sigma, res), giving dmvn2() as follows &gt; dmvn2 &lt;- function(y, mu, Sigma, log = TRUE) { + # Function to evaluate multivariate Normal pdf by solving + # a system of linear equations + # y and mu are vectors + # Sigma is a square matrix + # log is a logical + # Returns scalar, on log scale, if log == TRUE. + p &lt;- length(y) + res &lt;- y - mu + out &lt;- - 0.5 * determinant(Sigma)$modulus - 0.5 * p * log(2 * pi) - + 0.5 * t(res) %*% solve(Sigma, res) + if (!log) + out &lt;- exp(out) + out + } which reassuringly gives the same answer as dmvn1(). &gt; dmvn2(y, mu, Sigma) ## [,1] ## [1,] -3.654535 ## attr(,&quot;logarithm&quot;) ## [1] TRUE Remark. In general, solving a system of linear equations is faster and more numerically stable than inverting and multiplying. The latter is essentially a result of reducing numerical errors, as discussed in Chapter 2. Definition 3.8 (Elementary row operation) An elementary row operation on a matrix is any one of the following. Type-I: interchange two rows of the matrix; Type-II: multiply a row by a nonzero scalar; Type-III: replace a row by the sum of that row and a scalar multiple of another row. Definition 3.9 (Row echelon form) A \\(m \\times n\\) matrix \\(\\bf U\\) is said to be in row echelon form if the following two conditions hold. If a row \\(\\mathbf{u}_{i*}^\\text{T}\\) comprises all zeros, i.e. \\(\\mathbf{u}_{i*}^\\text{T} = \\mathbf{0}^\\text{T}\\), then all rows below also comprise all zeros. If the first nonzero element of \\(\\mathbf{u}_{i*}^\\text{T}\\) is the \\(j\\)th element, then the \\(j\\)th element in all rows below is zero. Example 3.4 (Gaussian elimination -- full rank case) Gaussian elimination is perhaps the best established method for solving systems of linear equations. In MTH3045 you won’t be examined on Gaussian elimination, but it will be useful to be familiar with how it works, so that the virtues of the matrix decompositions that follow will become apparent. The system of linear equations \\(\\mathbf{Ax} =\\mathbf{b}\\) may be verbosely written as \\[\\begin{equation} \\begin{array}{c} a_{11}x_1 + a_{12}x_2 + \\ldots + a_{1n}x_n = b_1\\\\ a_{21}x_1 + a_{22}x_2 + \\ldots + a_{2n}x_n = b_2\\\\ \\vdots\\\\ a_{n1}x_1 + a_{n2}x_2 + \\ldots + a_{nn}x_n = b_n.\\\\ \\end{array} \\tag{3.3} \\end{equation}\\] The aim of Gaussian elimination is to use elementary row operations to transform (3.3) into an equivalent but triangular system. Instead of algebraically writing the algorithm for Gaussian elimination, it will be simpler to consider a numerical example in which we want to solve the following system of four equations in four variables \\[ \\begin{array}{rrrrrrrr} 2x_1 &amp;+&amp; 3x_2 &amp; &amp; &amp; &amp; &amp;= 1\\\\ 4x_1 &amp;+&amp; 7x_2 &amp;+&amp; 2x_3 &amp; &amp; &amp;= 2\\\\ -6x_1 &amp;-&amp; 10x_2 &amp;+&amp; x_4 &amp; &amp; &amp;= 1\\\\ 4x_1 &amp;+&amp; 6x_2 &amp;+&amp; 4x_3 &amp;+&amp; 5x_4 &amp;= 0\\\\ \\end{array} \\] and for which we’ll write the coefficients of the \\(x_i\\)s and \\(\\bf b\\) in the augmented matrix form \\[ \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 4 &amp; 7 &amp; 2 &amp; 0 &amp; 2\\\\ -6 &amp; -10 &amp; 0 &amp; 1 &amp; 1\\\\ 4 &amp; 6 &amp; 4 &amp; 5 &amp; 0\\\\ \\end{array} \\right] \\] for convenience. We start by choosing the pivot. Our first choice is a coefficient of \\(x_1\\). We can choose any nonzero coefficient. Anything below this is set to zero through elementary operations. We’ll choose \\(a_{11}\\) as the pivot and then perform the following elementary matrix operations: row 2 \\(\\to\\) 2 \\(\\times\\) row 1 + - 1 \\(\\times\\) row 2; row 3 \\(\\to\\) 3 \\(\\times\\) row 1 + 1 \\(\\times\\) row 3; row 4 \\(\\to\\) -2 \\(\\times\\) row 1 + row 4, which gives the following transformation of the above augmented matrix. \\[ \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 4 &amp; 7 &amp; 2 &amp; 0 &amp; 2\\\\ -6 &amp; -10 &amp; 0 &amp; 1 &amp; 1\\\\ 4 &amp; 6 &amp; 4 &amp; 5 &amp; 0\\\\ \\end{array} \\right] \\to \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; -1 &amp; 0 &amp; 1 &amp; 4\\\\ 0 &amp; 0 &amp; 4 &amp; 5 &amp; -2\\\\ \\end{array} \\right] \\] Repeating this with the element in the position of \\(a_{22}\\) we get \\[ \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; -1 &amp; 0 &amp; 1 &amp; 4\\\\ 0 &amp; 0 &amp; 4 &amp; 5 &amp; -2\\\\ \\end{array} \\right] \\to \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; 1 &amp; 4\\\\ 0 &amp; 0 &amp; 4 &amp; 5 &amp; -2\\\\ \\end{array} \\right] \\] and then again with the element in the position of \\(a_{33}\\) we get \\[ \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; -1 &amp; 0 &amp; 1 &amp; 4\\\\ 0 &amp; 0 &amp; 4 &amp; 5 &amp; -2\\\\ \\end{array} \\right] \\to \\left[ \\begin{array}{rrrr|r} 2 &amp; 3 &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 2 &amp; 1 &amp; 4\\\\ 0 &amp; 0 &amp; 0 &amp; 3 &amp; -10\\\\ \\end{array} \\right] \\] The above operations have triangularised the system of linear equations, i.e. with an augmented matrix of the form \\[ \\left[ \\begin{array}{rrrr|r} u_{11} &amp; u_{12} &amp; \\ldots &amp; u_{1n} &amp; b_1^*\\\\ 0 &amp; u_{22} &amp; \\ldots &amp; u_{2n} &amp; b_2^*\\\\ \\vdots &amp; \\vdots \\ldots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; u_{nn} &amp; b_n^*\\\\ \\end{array} \\right] \\] which can be tidily written as \\(\\mathbf{Ux} = \\mathbf{b}^*\\). It is straightforward to find \\(\\bf x\\) from such a system, because, if we turn to the above example, \\(x_4 = -10/3\\), which can be substituted in the above line to give \\(x_3 = 11/3\\), and so forth gives \\(x_2 = -22/3\\) and \\(x_1 = 23/2\\), i.e. \\(\\mathbf{x} = (22/3, -22/3, 11/3, -10/3)^\\text{T}\\). The above is an example of backward substitution. Definition 3.10 (Backward substitution) Consider the system of linear equations given by \\(\\mathbf{Ux} = \\mathbf{b}\\), where \\(\\bf U\\) is an \\(n \\times n\\) upper triangular matrix. We can find \\(\\bf x\\) by backward substitution through the following steps. Calculate \\(x_n = b_n / u_{nn}\\). For \\(i = n - 1, n - 2, \\ldots, 2, 1\\), recursively compute \\[ x_i = \\dfrac{1}{u_{ii}} \\left(b_i - \\sum_{j = i + 1}^n u_{ij}x_j\\right). \\] Remark. Note that forward substitution is simply the analogous process of backward substitution where we find a lower triangular matrix, and then solve for \\(x_1\\), \\(x_2\\) given \\(x_1\\), and so forth. Gaussian elimination is the two-stage process of forming the triangular matrix and then performing backward substitution. Remark. If we want to perform backward or forward substitution in R we should use backsolve() and forwardsolve(), respectively. These have usage &gt; backsolve(r, x, k = ncol(r), upper.tri = TRUE, transpose = FALSE) &gt; forwardsolve(l, x, k = ncol(l), upper.tri = FALSE, transpose = FALSE) So backsolve() expects a right upper-triangular matrix and forwardsolve() expects a left lower-triangular matrix. Example 3.5 Confirm that \\(\\mathbf{x} = (23/2, -22/3, 11/3, -10/3)^\\text{T}\\) using backsolve(). We need to input the upper-triangular matrix \\(\\mathbf{U}\\) and \\(\\mathbf{b}^*\\), which we’ll call U and bstar, respectively. &gt; U &lt;- rbind( + c(2, 3, 0, 0), + c(0, 1, 2, 0), + c(0, 0, 2, 1), + c(0, 0, 0, 3) + ) &gt; bstar &lt;- c(1, 0, 4, -10) &gt; backsolve(U, bstar) ## [1] 11.500000 -7.333333 3.666667 -3.333333 Remark. We may want to solve multiple system of linear equations of the form \\(\\mathbf{Ax}_1 = \\mathbf{b}_1\\), \\(\\mathbf{Ax}_2 = \\mathbf{b}_2\\), \\(\\ldots\\), \\(\\mathbf{Ax}_p = \\mathbf{b}_p\\), which can be written with matrices as \\(\\mathbf{AX} = \\mathbf{B}\\) for \\(n \\times p\\) matrices \\(\\bf X\\) and \\(\\bf B\\). In this situation we can recognise that we only triangularise \\(\\bf A\\) once, and then use that triangularisation to go through the back substitution algorithm \\(p\\) times. Remark. We can find the inverse of \\(\\bf A\\) by solving \\(\\mathbf{AX} = \\mathbf{I}_n\\) for \\(\\mathbf{X}\\) and then setting \\(\\mathbf{A}^{-1} = \\mathbf{X}\\). Definition 3.11 (Reduced row echelon form) A \\(m \\times n\\) matrix \\(\\bf U\\) is said to be in reduced row echelon form if the following two conditions hold. It is in row echelon form; The first nonzero element of each row is one; All entries above each pivot are zero. Example 3.6 (Gaussian elimination (rank deficient case)) Consider Gaussian elimination of the \\(3 \\times 3\\) matrix \\(\\bf A\\) given by \\[ \\mathbf{A} = \\left[ \\begin{array}{rrr} 1&amp;2&amp;1\\\\-2&amp;-3&amp;1\\\\3&amp;5&amp;0 \\end{array} \\right] \\] We can go through the following steps to transform the matrix to reduced row echelon form. \\[ \\left[ \\begin{array}{rrr} 1&amp;2&amp;1\\\\-2&amp;-3&amp;1\\\\3&amp;5&amp;0 \\end{array} \\right] \\to \\left[ \\begin{array}{rrr} 1&amp;2&amp;1\\\\0&amp;1&amp;3\\\\0&amp;-1&amp;-3 \\end{array} \\right] \\to \\left[ \\begin{array}{rrr} 1&amp;0&amp;-5\\\\0&amp;1&amp;3\\\\0&amp;0&amp;0 \\end{array} \\right] \\] We’ve ended up with only two nonzero rows, and hence \\(\\bf A\\) has rank 2, and because it has three rows it is therefore rank deficient. Example 3.7 (Triangular, forward and backward solving) Use R to find \\(\\mathbf{x}\\) such that \\(\\mathbf{Dx} = \\mathbf{b}\\) and \\(\\mathbf{Lx} = \\mathbf{b}\\) and \\(\\mathbf{Ux} = \\mathbf{b}\\) where \\[ \\mathbf{D} = \\left(\\begin{array}{rrr} -0.75 &amp; 0.00 &amp; 0.00\\\\ 0.00 &amp; -0.61 &amp; 0.00\\\\ 0.00 &amp; 0.00 &amp; -0.28\\\\ \\end{array}\\right),~ \\mathbf{U} = \\left(\\begin{array}{rrr} 1.00 &amp; -0.19 &amp; 0.89\\\\ 0.00 &amp; 0.43 &amp; 0.02\\\\ 0.00 &amp; 0.00 &amp; -0.20\\\\ \\end{array}\\right), \\] \\[ \\mathbf{L} = \\left(\\begin{array}{rrr} -0.72 &amp; 0.00 &amp; 0.00\\\\ 0.00 &amp; 2.87 &amp; 0.00\\\\ -1.94 &amp; -2.04 &amp; 0.81\\\\ \\end{array}\\right)\\text{ and } \\mathbf{b} = \\left(\\begin{array}{r} -2.98\\\\ 0.39\\\\ 0.36\\\\ \\end{array}\\right). \\] We’ll start by loading \\(\\mathbf{b}\\), \\(\\mathbf{D}\\), \\(\\mathbf{U}\\) and \\(\\mathbf{L}\\) and which we’ll store as b, D, U and L, respectively. &gt; D &lt;- diag(c(-0.75, -0.61, -0.28)) &gt; L &lt;- matrix(c(-0.72, 0, -1.94, 0, 2.87, -2.04, 0, 0, 0.81), 3, 3) &gt; U &lt;- matrix(c(1, 0, 0, -0.19, 0.43, 0, 0.89, 0.02, -0.2), 3, 3) &gt; b &lt;- c(-2.98, 0.39, 0.36) We can solve \\(\\mathbf{Dx} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) with the following two lines of code &gt; solve(D, b) ## [1] 3.9733333 -0.6393443 -1.2857143 &gt; b / diag(D) ## [1] 3.9733333 -0.6393443 -1.2857143 but should note that the latter uses fewer calculations, and so is more efficient and hence better. Then we can solve \\(\\mathbf{Ux} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) with the following two lines of code &gt; solve(U, b) ## [1] -1.1897674 0.9906977 -1.8000000 &gt; backsolve(U, b) ## [1] -1.1897674 0.9906977 -1.8000000 and \\(\\mathbf{Lx} = \\mathbf{b}\\) for \\(\\mathbf{x}\\) with the following two lines of code &gt; solve(L, b) ## [1] 4.1388889 0.1358885 10.6995765 &gt; forwardsolve(L, b) ## [1] 4.1388889 0.1358885 10.6995765 and on both these occasions the latter is more efficient because it uses fewer calculations. 3.5 Matrix decompositions 3.5.1 Cholesky decomposition Definition 3.12 (Cholesky decomposition) Any positive definite real matrix \\(\\bf A\\) can be factorised as \\[\\begin{equation} \\mathbf{A} =\\mathbf{LL}^\\text{T} \\tag{3.4} \\end{equation}\\] where \\(\\mathbf{L}\\) is a real lower-triangular matrix of the same dimension as \\(\\mathbf{A}\\) with positive diagonal entries. The factorisation in (3.4) is the Cholesky decomposition5. In general, we won’t be concerned with algorithms for computing matrix decompositions in MTH3045. However, the algorithm for computing the Cholesky decomposition is rather elegant, and so is given below. You won’t, however, be expected to use it. Consider the following matrices \\[ \\mathbf{A} = \\left[ \\begin{array}{cccc} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1n}\\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{n1} &amp; a_{n2} &amp; \\ldots &amp; a_{nn}\\\\ \\end{array} \\right] \\hspace{1cm} \\mathbf{L} = \\left[ \\begin{array}{cccc} l_{11} &amp; 0 &amp; \\ldots &amp; 0\\\\ l_{21} &amp; l_{22} &amp; \\ldots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ l_{n1} &amp; l_{n2} &amp; \\ldots &amp; l_{nn}\\\\ \\end{array} \\right] \\] where \\(\\bf A\\) is symmetric and non-singular. The entries of \\(\\bf L\\) are given by \\[ l_{ii} = \\sqrt{a_{ii} - \\sum_{k = 1}^{i - 1}l_{ik}^2}, \\hspace{1cm} l_{ij} = \\dfrac{a_{ij} - \\sum_{k = 1}^{i - 1}l_{ik}l_{jk}}{l_{ii}}, \\text{ for }i &gt; j. \\] &gt; chol(A) # computes the Cholesky decomposition of a square matrix A Remark. The chol() function in R returns an upper-triangular decomposition, i.e. returns \\(\\bf U\\) for \\(\\mathbf{A} = \\mathbf{U}^\\text{T} \\mathbf{U}\\). To obtain \\(\\mathbf{L}\\) we just use t(chol()). Example 3.8 (Cholesky decomposition in R) Compute the Cholesky decomposition of \\(\\boldsymbol{\\Sigma}\\) from Example 3.2 in upper- and lower-triangular form, and verify that both are Cholesky decompositions of \\(\\boldsymbol{\\Sigma}\\). &gt; U &lt;- chol(Sigma) # upper-triangular form &gt; all.equal(crossprod(U), Sigma) ## [1] TRUE &gt; L &lt;- t(U) # lower-triangular form &gt; all.equal(tcrossprod(L), Sigma) ## [1] TRUE Remark. Above, instead of L &lt;- t(chol(Sigma)) we’ve used L &lt;- t(U) to avoid repeated calculation of chol(Sigma). In this example, where we compute the Cholesky decomposition of a \\(3 \\times 3\\) matrix, the calculation is trivial. However, for much larger matrices, calculating the Cholesky decomposition can be expensive, and so we could gain significant time by only calculating it once. 3.5.1.1 Properties Once a Cholesky decomposition has been calculated, it can be used to calculate determinants and inverses. det(\\(\\bf A\\)) = \\(\\left(\\prod_{i = 1}^n l_{ii}\\right)^2\\). \\(\\mathbf{A}^{-1} = \\mathbf{L}^\\text{-T} \\mathbf{L}^{-1}\\), where \\(\\mathbf{L}^\\text{-T}\\) denotes the inverse of \\(\\mathbf{L}^\\text{T}\\). Example 3.9 (Determinant and inverse via the Cholesky decomposition) For \\(\\boldsymbol{\\Sigma}\\) from Example 3.2, compute det\\((\\boldsymbol{\\Sigma})\\) and \\(\\boldsymbol{\\Sigma}^{-1}\\) based on either Cholesky decomposition computed in Example 3.8. Verify your results. We’ll start with the determinant &gt; det1 &lt;- det(Sigma) &gt; det2 &lt;- prod(diag(L))^2 &gt; all.equal(det1, det2) ## [1] TRUE and then have various options for the inverse &gt; inv1 &lt;- solve(Sigma) &gt; inv2 &lt;- crossprod(solve(L), solve(L)) &gt; all.equal(inv1, inv2) ## [1] TRUE &gt; inv3 &lt;- crossprod(solve(L)) &gt; all.equal(inv1, inv3) ## [1] TRUE &gt; inv4 &lt;- solve(t(L), solve(L)) &gt; all.equal(inv1, inv4) ## [1] TRUE &gt; inv5 &lt;- chol2inv(t(L)) &gt; all.equal(inv1, inv5) ## [1] TRUE which all give the same answer, although chol2inv() should be our default. 3.5.1.2 Solving systems of linear equations We can also use a Cholesky decomposition to solve a system of linear equations. Solving \\(\\mathbf{Ax} = \\mathbf{b}\\) is equivalent to solving \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\) and then \\(\\mathbf{L}^\\text{T}\\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\). This might seem inefficient at first glance, because we’re having to solve two systems of linear equations. However, \\(\\mathbf{L}\\) being triangular means that forward elimination is efficient for \\(\\mathbf{L}\\), and backward elimination is for \\(\\mathbf{L}^\\text{T}\\). Example 3.10 (Solving linear systems with Cholesky decompositions) Recall the multivariate Normal pdf of Example 3.2 in which we needed \\(\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y} - \\boldsymbol{\\mu})\\), with \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as given in Example 3.2. Compute \\(\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y} - \\boldsymbol{\\mu})\\) by solving \\(\\boldsymbol{\\Sigma} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\) for \\(\\mathbf{z}\\) using using the Cholesky decomposition of \\(\\boldsymbol{\\Sigma}\\) from Example 3.9. Verify your answer. We’ll first use solve() on \\(\\boldsymbol{\\Sigma}\\), as in Example 3.3, &gt; res1 &lt;- solve(Sigma, y - mu) and then we’ll solve \\(\\mathbf{L}\\mathbf{x} = \\mathbf{y} - \\boldsymbol{\\mu}\\) for \\(\\mathbf{x}\\) followed by \\(\\mathbf{L}^\\text{T}\\mathbf{z} = \\mathbf{x}\\) for \\(\\mathbf{z}\\). &gt; x &lt;- solve(L, y - mu) &gt; res2 &lt;- solve(t(L), x) which we can confirm gives the same as above, i.e. res1, with all.equal() &gt; all.equal(res1, res2) ## [1] TRUE We can tell R to use forward substitution, by calling function forwardsolve() instead of solve(), or backward substitution, by calling function backsolve(). Then R knows that one triangle of the supplied matrix comprises zeros, which speeds up solving the system of linear equations. Solving via the Cholesky decomposition is also more stable than without it. Otherwise, if we just used solve(), R performs a lot of needless calculations on zeros, because it doesn’t know that they’re zeros. Example 3.11 (Solving triangular linear systems with Cholesky decompositions) Repeat Example 3.10 by recognising that the Cholesky decomposition of \\(\\boldsymbol{\\Sigma}\\) is triangular. We want to use forwardsolve() to solve \\(\\mathbf{L}\\mathbf{x} = \\mathbf{y} - \\boldsymbol{\\mu}\\) and then backsolve() to solve \\(\\mathbf{L}^\\text{T}\\mathbf{z} = \\mathbf{x}\\). However, backsolve() expects an upper-triangular matrix, so we must use \\(\\mathbf{L}^\\text{T}\\), hence t(L) below. &gt; x2 &lt;- forwardsolve(L, y - mu) &gt; res3 &lt;- backsolve(t(L), x2) &gt; all.equal(res1, res3) ## [1] TRUE We can avoid the transpose operation by letting backsolve() know the format of Cholesky decomposition that we’re supplying. We’re supplying a lower-triangular matrix, hence upper.tri = FALSE, which needs transposing to be upper-triangular, hence transpose = TRUE. &gt; res4 &lt;- backsolve(L, forwardsolve(L, y - mu), upper.tri = FALSE, transpose = TRUE) &gt; all.equal(res1, res4) ## [1] TRUE If we begin with an upper-triangular matrix, U = t(L), then we’d want to use either of the following &gt; forwardsolve(U, backsolve(U, y - mu, transpose = TRUE), upper.tri = TRUE) &gt; backsolve(U, forwardsolve(U, y - mu, upper.tri = TRUE, transpose = TRUE)) and see that the first gives the required result. &gt; res5 &lt;- forwardsolve(U, backsolve(U, y - mu, transpose = TRUE), upper.tri = TRUE) &gt; all.equal(res1, res5) ## [1] TRUE Definition 3.13 (Mahalanobis distance) Given the \\(p\\)-vectors \\({\\bf x}\\) and \\({\\bf y}\\) and a variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\), the Mahalanobis distance is defined as \\[ D_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{y} - \\mathbf{x})^\\text{T} {\\boldsymbol \\Sigma}^{-1} (\\mathbf{y} - \\mathbf{x})}. \\] Example 3.12 (Mahalanobis distance via Cholesky decomposition) We can efficiently compute the Mahalanobis distance using the Cholesky decomposition. Consider \\(\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\text{T}\\) so that \\(\\boldsymbol{\\Sigma}^\\text{-1} = \\mathbf{L}^{-\\text{T}}\\mathbf{L}^\\text{-1}\\). Then \\[\\begin{align*} \\big[D_M(\\mathbf{x}, \\mathbf{y})\\big]^2 &amp;= (\\mathbf{y} - \\mathbf{x})^\\text{T} \\mathbf{L}^{-\\text{T}}\\mathbf{L}^\\text{-1} (\\mathbf{y} - \\mathbf{x})\\\\ &amp;= [\\mathbf{L}^{-1}(\\mathbf{y} - \\mathbf{x})]^\\text{T} \\mathbf{L}^\\text{-1} (\\mathbf{y} - \\mathbf{x})\\\\ &amp;= \\mathbf{z}^\\text{T} \\mathbf{z} \\end{align*}\\] where \\(\\mathbf{z}\\) is the solution of \\(\\mathbf{L} \\mathbf{z} = \\mathbf{y} - \\mathbf{x}\\). If we have \\(\\mathbf{x}\\), \\(\\mathbf{y}\\) and the lower-triangular Cholesky decomposition of \\(\\boldsymbol{\\Sigma}\\) stored as x, y and L, respectively, then we can efficiently compute the Mahalanobis distance in R with &gt; sqrt(crossprod(forwardsolve(L, y - x))) but may want to simplify the use of crossprod() and use sqrt(sum(forwardsolve(L, y - x)^2)) instead. Example 3.13 (Evaluating the multivariate Normal pdf using the Cholesky decomposition) Create a function dmvn3() that evaluates the multivariate Normal pdf, as in examples 3.2 and 3.3, based on a Cholesky decomposition of the variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\). Verify your function using \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\), and \\(\\boldsymbol{\\Sigma}\\) given in Example 3.2. We first recognise that, if matrix \\(\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^\\text{T}\\), then \\(\\log(\\text{det}(\\boldsymbol{\\Sigma})) = 2 \\sum_{i = 1}^n \\log(l_{ii})\\), which we’ll incorporate in dmvn3() &gt; dmvn3 &lt;- function(y, mu, Sigma, log = TRUE) { + # Function to evaluate multivariate Normal pdf by solving + # a system of linear equations via Cholesky decomposition + # y and mu are vectors + # Sigma is a square matrix + # log is a logical + # Returns scalar, on log scale, if log == TRUE. + p &lt;- length(y) + res &lt;- y - mu + L &lt;- t(chol(Sigma)) + out &lt;- - sum(log(diag(L))) - 0.5 * p * log(2 * pi) - + 0.5 * sum(forwardsolve(L, res)^2) + if (!log) + out &lt;- exp(out) + out + } along with the result to evaluate the Mahalanobis distance from Example 3.12. The following confirms the value seen previously. &gt; dmvn3(y, mu, Sigma) ## [1] -3.654535 Example 3.14 (Generating multivariate Normal random vectors) We can generate \\(\\mathbf{Y} \\sim MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\), i.e. multivariate Normal random vectors with mean \\(\\boldsymbol{\\mu}\\) and variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\), using the following algorithm. Step 1. Find some matrix \\(\\bf L\\) such that \\(\\mathbf{L} \\mathbf{L}^\\text{T} = \\boldsymbol{\\Sigma}\\). Step 2. Generate \\(\\mathbf{Z}^\\text{T} = (Z_1, \\ldots, Z_p)\\), where \\(Z_i\\), \\(i = 1, \\ldots, p\\), are independent \\(N(0, 1)\\) random variables. Step 3. Set \\(\\mathbf{Y} = \\boldsymbol{\\mu} + \\mathbf{L} \\mathbf{Z}\\). In R, we can write a function, rmvn(), to implement this. Example 3.15 (Generating multivariate normal random vectors in R) Write a function in R to generate \\(n\\) independent \\(MVN_p({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})\\) random vectors and then generate six vectors with \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2. Remark. The Cholesky decomposition clearly meets the criterion for \\(\\bf L\\) in Step 1. Suppose that \\(n =\\) n, \\(\\boldsymbol{\\mu} =\\) mu and \\(\\boldsymbol{\\Sigma} =\\) Sigma, then we can use function rmvn() below. &gt; rmvn &lt;- function(n, mu, Sigma) { + # Function to generate n MVN random vectors + # mean vector mu + # variance-covariance matrix Sigma + # integer n + # returns p x n matrix + p &lt;- length(mu) + L &lt;- t(chol(Sigma)) + Z &lt;- matrix(rnorm(p * n), nrow = p) + as.vector(mu) + L %*% Z + } &gt; rmvn(6, mu, Sigma) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.8457857 -0.6251981 -2.322655 0.8581701 -0.2873112 1.710739 ## [2,] 2.0171233 1.5664917 1.525484 2.6164901 1.5392341 2.640589 ## [3,] 4.2669704 2.5623726 2.376017 4.4479018 2.2114458 3.174201 3.5.2 Eigen-decomposition 3.5.2.1 Definition Definition 3.14 (Eigen-decomposition) We can write any symmetric matrix \\(\\bf A\\) in the form \\[\\begin{equation} \\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T} \\tag{3.5} \\end{equation}\\] where \\(\\mathbf{U}\\) is an orthogonal matrix and \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix. We will denote the diagonal elements of \\(\\boldsymbol{\\Lambda}\\) by \\(\\lambda_1 \\leq \\ldots \\leq \\lambda_n\\). Post-multiplying both sides of the decomposition by \\(\\bf U\\) we have \\(\\mathbf{AU} = \\mathbf{U}\\boldsymbol{\\Lambda}\\). Let \\({\\bf u}_i\\) denote the \\(i\\)th column of \\(\\bf U\\). Then \\(\\mathbf{Au}_i = \\lambda_i \\mathbf{u}_i\\). The \\(\\lambda_i\\)s are the eigenvalues of \\(\\bf A\\), and the columns of \\(\\bf U\\) are the corresponding eigenvectors. We call the decomposition in (3.5) the eigen-decomposition (or sometimes spectral decomposition) of \\(\\bf A\\). 3.5.2.2 Properties If \\(\\bf A\\) is symmetric, the following properties of eigen-decompositions hold. \\(\\mathbf{U}^{-1} = \\mathbf{U}^\\text{T}\\) \\({\\bf A}^{-1} = \\mathbf{U} \\boldsymbol{\\Lambda}^{-1} \\mathbf{U}^{-1}\\), and, because \\(\\boldsymbol{\\Lambda}\\) is diagonal, so too is \\(\\boldsymbol{\\Lambda}^{-1}\\) and its elements are \\((\\boldsymbol{\\Lambda}^{-1})_{ii} = 1/\\lambda_i\\). det(\\(\\bf A\\)) = \\(\\prod_{i=1}^n \\lambda_i\\). &gt; eigen(A) # computes the eigen-decomposition of a matrix A Example 3.16 (Eigen-decomposition of the Hilbert matrix in R) Use eigen() in R to give the eigen-decomposition of the \\(3 \\times 3\\) Hilbert matrix. We’ve already calculated the \\(3 \\times 3\\) Hilbert matrix and stored it as H, so we just need &gt; eigen(H) ## eigen() decomposition ## $values ## [1] 1.40831893 0.12232707 0.00268734 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.8270449 0.5474484 0.1276593 ## [2,] 0.4598639 -0.5282902 -0.7137469 ## [3,] 0.3232984 -0.6490067 0.6886715 Note that eigen() returns a list comprising element values, a vector of the eigenvalues in descending order, and vectors, a matrix of the eigenvectors, in column order corresponding to the eigenvalues. We can ask eigen() to return only the eigenvalues by specifying only.values = TRUE, and stipulate that the supplied matrix is symmetric by specifying symmetric = TRUE (which avoids checking symmetry, and can save a bit of time for large matrices). Example 3.17 (Orthogonality of the eigen-decomposition in R) Confirm that the eigenvectors of the eigen-decomposition of the \\(3 \\times 3\\) Hilbert matrix form an orthogonal matrix. If \\(\\mathbf{U}\\) denotes the matrix of eigenvectors, then we need to show that \\(\\mathbf{U}^\\text{T}\\mathbf{U} = \\mathbf{I}\\), which the following confirms. &gt; eH &lt;- eigen(H) &gt; U &lt;- eH$vectors &gt; crossU &lt;- crossprod(U) # should be 3 x 3 identity matrix &gt; all.equal(crossU, diag(3)) ## [1] TRUE Theorem 3.1 Let \\(\\bf A\\) be a positive definite matrix with eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n\\) and corresponding eigenvectors \\(\\mathbf{u}_1, \\mathbf{u}_2, \\ldots \\mathbf{u}_n\\). Then \\(\\mathbf{u}_1\\) maximises \\(\\mathbf{x}^\\text{T} \\mathbf{A} \\mathbf{x}\\) and \\(\\mathbf{u}_1^\\text{T} \\mathbf{A} \\mathbf{u}_1 = \\lambda_1\\). Furthermore, for \\(k = 1, \\ldots, p &lt; n\\), given \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\), \\(\\mathbf{u}_{k+1}\\) maximises \\(\\mathbf{x}^\\text{T} \\mathbf{A} \\mathbf{x}\\), subject to \\(\\mathbf{x}\\) being orthogonal to \\(\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\) and \\(\\mathbf{u}_{k+1}^\\text{T} \\mathbf{A} \\mathbf{u}_{k+1} = \\lambda_{k+1}\\). Proof. For a proof see, e.g. Johnson and Wichern (2007, 80), but note that knowledge of the proof is beyond the scope of MTH3045. Example 3.18 (Powers of matrices) Consider \\(n \\times n\\) matrix \\(\\bf A\\) with eigen-decomposition \\(\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T}\\). The second power of \\(\\bf A\\) is \\(\\mathbf{A}^2 = \\bf AA\\). Show that the \\(m\\)th power of is given by \\(\\mathbf{A}^m = \\mathbf{U} \\boldsymbol{\\Lambda}^m \\mathbf{U}^\\text{T}\\). \\[\\mathbf{A}^m = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T} \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T} \\ldots \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T}.\\] Because \\(\\mathbf{U}^\\text{T} \\mathbf{U} = \\mathbf{I}_n\\) this reduces to \\[\\mathbf{A}^m = \\mathbf{U} \\boldsymbol{\\Lambda} \\boldsymbol{\\Lambda} \\ldots \\boldsymbol{\\Lambda} \\mathbf{U}^\\text{T} = \\mathbf{U} \\boldsymbol{\\Lambda}^m \\mathbf{U}^\\text{T}.\\] Example 3.19 (Principal component analysis) Consider a random vector \\(\\mathbf{Y} = (Y_1, \\ldots, Y_n)^\\text{T}\\) with variance-covariance matrix \\(\\boldsymbol \\Sigma\\). Then consider taking linear combinations of \\(\\bf Y\\) so that \\[ \\begin{array}{ccccc} Z_1 &amp;=&amp; \\mathbf{a}_1^\\text{T} \\mathbf{Y} &amp;=&amp; a_{11} Y_1 + a_{12} Y_2 + \\ldots + a_{1n} Y_n,\\\\ Z_2 &amp;=&amp; \\mathbf{a}_2^\\text{T} \\mathbf{Y} &amp;=&amp; a_{21} Y_1 + a_{22} Y_2 + \\ldots + a_{2n} Y_n,\\\\ \\vdots &amp;=&amp; \\vdots &amp;=&amp; \\vdots\\\\ Z_n &amp;=&amp; \\mathbf{a}_n^\\text{T} \\mathbf{Y} &amp;=&amp; a_{n1} Y_1 + a_{n2} Y_2 + \\ldots + a_{nn} Y_n,\\\\ \\end{array} \\] where \\(\\mathbf{a}_1, \\ldots, \\mathbf{a}_n\\) are coefficient vectors. Then \\[ \\begin{array}{rrl} \\text{Var}(Z_i) &amp;= \\mathbf{a}_i^\\text{T} \\boldsymbol{\\Sigma} \\mathbf{a}_i&amp; \\text{for } i = 1, \\ldots, n\\\\ \\text{Cov}(Z_j, Z_k) &amp;= \\mathbf{a}_j^\\text{T} \\boldsymbol{\\Sigma} \\mathbf{a}_k&amp; \\text{for } j, k = 1, \\ldots, n. \\end{array} \\] The principal components are the uncorrelated linear combinations of \\(Y_1, \\ldots, Y_n\\) that maximise \\(\\text{Var}(\\mathbf{a}_i^\\text{T} \\mathbf{Y})\\), for \\(i = 1, \\ldots, n\\). Hence the first principal component maximises \\(\\text{Var}(\\mathbf{a}_1 \\mathbf{Y})\\) subject to \\(\\mathbf{a}_1^\\text{T} \\mathbf{a}_1 = 1\\), the second maximises \\(\\text{Var}(\\mathbf{a}_2 \\mathbf{Y})\\) subject to \\(\\mathbf{a}_2^\\text{T} \\mathbf{a}_2 = 1\\) and \\(\\text{Cov}(\\mathbf{a}_1^\\text{T} \\mathbf{Y}, \\mathbf{a}_2^\\text{T} \\mathbf{Y}) = 0\\), and so forth. More generally, the \\(i\\)th principal component, \\(i &gt; 1\\), maximises \\(\\text{Var}(\\mathbf{a}_i^\\text{T} \\mathbf{Y})\\) subject \\(\\mathbf{a}_i^\\text{T} \\mathbf{a}_i = 1\\) and \\(\\text{Cov}(\\mathbf{a}_i^\\text{T} \\mathbf{Y}, \\mathbf{a}_j^\\text{T} \\mathbf{Y}) = 0\\) for \\(j &lt; i\\). Now suppose that we form the eigen-decomposition \\(\\boldsymbol{\\Sigma} = \\mathbf{U}^\\text{T} \\boldsymbol{\\Lambda} \\mathbf{U}\\). The eigenvectors of \\(\\boldsymbol{\\Sigma}\\) therefore meet the criteria described above for principal components, and hence give one definition for principal components. Example 3.20 (Principal component analysis of Fisher's iris data) Fisher’s iris data are distributed with R as object iris. From the dataset’s help file the data comprise “the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.” Compute the principal components for the variables Sepal.Length, Sepal.Width, Petal.Length and Petal.Width. To compute the principals components, we’ll load the iris data, which we’ll quickly look at &gt; data(iris) &gt; head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa and then extract the relevant variables, and then form their empirical correlation matrix. &gt; vbls &lt;- c(&#39;Sepal.Length&#39;, &#39;Sepal.Width&#39;, &#39;Petal.Length&#39;, &#39;Petal.Width&#39;) &gt; species &lt;- as.factor(iris$Species) &gt; Y &lt;- as.matrix(iris[, vbls]) # data corresponding to variables under study &gt; corY &lt;- cor(Y) # correlation matrix of variables under study We can then use the eigen-decomposition of the correlation matrix to form the eigenvectors, which are also the coefficient vectors, and which we’ll store as A. Note that we use the correlation matrix because we’re measuring different features of the plant, which we don’t necessarily expect to be directly comparable. &gt; A &lt;- eigen(corY)$vectors &gt; A ## [,1] [,2] [,3] [,4] ## [1,] 0.5210659 -0.37741762 0.7195664 0.2612863 ## [2,] -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## [3,] 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## [4,] 0.5648565 -0.06694199 -0.6342727 0.5235971 The principal components are then obtained by calculating \\(\\mathbf{a}_j^\\text{T} \\mathbf{y}\\) for \\(j = 1, \\ldots, 4\\). We’ll store these as pcs and then use head() to show the first few. &gt; pcs &lt;- Y %*% A &gt; colnames(pcs) &lt;- paste(&#39;PC&#39;, 1:4, sep = &#39;&#39;) &gt; head(pcs) ## PC1 PC2 PC3 PC4 ## [1,] 2.640270 -5.204041 2.488621 -0.1170332 ## [2,] 2.670730 -4.666910 2.466898 -0.1075356 ## [3,] 2.454606 -4.773636 2.288321 -0.1043499 ## [4,] 2.545517 -4.648463 2.212378 -0.2784174 ## [5,] 2.561228 -5.258629 2.392226 -0.1555127 ## [6,] 2.975946 -5.707321 2.437245 -0.2237665 Remark. Principal component analysis is a commonly used statistical method, often as a method of dimension reduction, when a finite number of principal components, below the dimension of the data, are used to capture most of what’s in the original data. In practice, if we want to perform PCA then we’d usually use one of R’s built in functions, such as prcomp() or princomp(). For example &gt; prcomp(Y, center = TRUE, scale = TRUE) ## Standard deviations (1, .., p=4): ## [1] 1.7083611 0.9560494 0.3830886 0.1439265 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.5210659 -0.37741762 0.7195664 0.2612863 ## Sepal.Width -0.2693474 -0.92329566 -0.2443818 -0.1235096 ## Petal.Length 0.5804131 -0.02449161 -0.1421264 -0.8014492 ## Petal.Width 0.5648565 -0.06694199 -0.6342727 0.5235971 gives the same principal component coefficients as we obtained in Example 3.20. A benefit of working with prcomp() or princomp() is that R interprets the objects as relating to PCA, and then summary() and plot(), for example, perform useful actions. Principal component regression is a statistical model in which we perform regression on principal components. 3.5.3 Singular value decomposition Theorem 3.2 For an \\(m \\times n\\) matrix \\(\\bf A\\) with real elements and \\(m \\geq n\\), there exist orthogonal matrices \\(\\bf U\\) and \\(\\bf V\\) such that \\[\\mathbf{U}^\\text{T} \\mathbf{A} \\mathbf{V} = \\mathbf{D},\\] where \\(\\mathbf{D}\\) is a diagonal matrix with elements \\(d_1 \\geq d_2 \\geq \\ldots \\geq d_m\\). Definition 3.15 (Singular value decomposition) The singular value decomposition (SVD) of \\(\\mathbf{A}\\) is \\[\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\text{T}.\\] The diagonal entries of \\(m \\times n\\) matrix \\(\\mathbf{D}\\) are the singular values of \\(\\mathbf{A}\\). We can form a \\(m \\times m\\) matrix \\(\\mathbf{U}\\) from the eigenvectors of \\(\\mathbf{A}^\\text{T}\\mathbf{A}\\) and a \\(n \\times n\\) matrix \\(\\mathbf{V}\\) from the eigenvectors of \\(\\mathbf{AA}^\\text{T}\\). The singular values are the square roots of the positive eigenvalues of \\(\\mathbf{A}^\\text{T}\\mathbf{A}\\). &gt; svd(A) # calculates the SVD of a matrix A Example 3.21 (SVD of the Hilbert matrix in R) Compute a SVD of the \\(3 \\times 3\\) Hilbert matrix, \\(\\mathbf{H}_3\\), in R using svd(). We have \\(\\mathbf{H}_3\\) stored as H already, so we’ll now calculate its SVD. &gt; H.svd &lt;- svd(H) &gt; H.svd ## $d ## [1] 1.40831893 0.12232707 0.00268734 ## ## $u ## [,1] [,2] [,3] ## [1,] -0.8270449 0.5474484 0.1276593 ## [2,] -0.4598639 -0.5282902 -0.7137469 ## [3,] -0.3232984 -0.6490067 0.6886715 ## ## $v ## [,1] [,2] [,3] ## [1,] -0.8270449 0.5474484 0.1276593 ## [2,] -0.4598639 -0.5282902 -0.7137469 ## [3,] -0.3232984 -0.6490067 0.6886715 From svd() we get a three-element list where d is a vector of the diagonal elements of \\(\\mathbf{D}\\), u is \\(\\mathbf{U}\\) and v is \\(\\mathbf{V}\\). We can quickly confirm that \\(\\mathbf{H}_3 = \\mathbf{UDV}^\\text{T}\\). &gt; all.equal(H, H.svd$u %*% tcrossprod(diag(H.svd$d), H.svd$v)) ## [1] TRUE Remark. One application of the SVD is solving systems of linear equations. Let \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\text{T}\\) be the SVD of \\(\\mathbf{A}\\). Consider again solving \\(\\mathbf{Ax} = \\mathbf{b}\\). Then \\[\\begin{align} \\mathbf{U}^\\text{T}\\mathbf{Ax} &amp;= \\mathbf{U}^\\text{T}\\mathbf{b} \\tag{premultiplying by $\\mathbf{U}^\\text{T}$} \\\\ \\mathbf{U}^\\text{T}\\mathbf{U} \\mathbf{D} \\mathbf{V}^\\text{T}\\mathbf{x} &amp;= \\mathbf{U}^\\text{T}\\mathbf{b} \\tag{substituting $\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\text{T}$} \\\\ \\mathbf{D} \\mathbf{V}^\\text{T}\\mathbf{x} &amp;= \\mathbf{U}^\\text{T}\\mathbf{b} \\tag{as $\\mathbf{U}^\\text{T}\\mathbf{U} = \\mathbf{I}_n$} \\\\ \\mathbf{D} \\tilde{\\mathbf{x}} &amp;= \\tilde{\\mathbf{b}}. \\tag{setting $\\tilde{\\mathbf{x}} = \\mathbf{V}^\\text{T}\\mathbf{x}$ and $\\tilde{\\mathbf{b}} = \\mathbf{U}^\\text{T}\\mathbf{b}$} \\end{align}\\] As \\(\\mathbf{D}\\) is diagonal, we see that setting \\(\\tilde{\\mathbf{x}} = \\mathbf{V}^\\text{T}\\mathbf{x}\\) and \\(\\tilde{\\mathbf{b}} = \\mathbf{U}^\\text{T}\\mathbf{b}\\) results in a diagonal solve, i.e. essentially \\(n\\) divisions. Example 3.22 (Solving systems of linear equations using a SVD in R) Solve \\(\\boldsymbol{\\Sigma} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\) in R using a SVD with \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2. Following the above remark, we want to calculate \\(\\tilde{\\mathbf{x}} = \\mathbf{V}^\\text{T}\\mathbf{x}\\) and \\(\\tilde{\\mathbf{b}} = \\mathbf{V}^\\text{T}(\\mathbf{y} - \\boldsymbol{\\mu})\\). We’ll start by computing the SVD of \\(\\boldsymbol{\\Sigma}\\), and then extract \\(\\text{diag}(\\mathbf{D})\\) and \\(\\mathbf{V}\\), which we’ll call S.d and S.V, respectively. &gt; S.svd &lt;- svd(Sigma) &gt; S.d &lt;- S.svd$d &gt; S.V &lt;- S.svd$v Then we obtain \\(\\tilde{\\mathbf{b}} = \\mathbf{V}^\\text{T} (\\mathbf{y} - \\boldsymbol{\\mu})\\), which we’ll call b2. &gt; b2 &lt;- crossprod(S.V, y - mu) Then \\(\\tilde{\\mathbf{x}}\\) is the solution of \\(\\mathbf{D} \\tilde{\\mathbf{x}} = \\tilde{\\mathbf{b}}\\), which we’ll call x2, and can be computed with &gt; x2 &lt;- b2 / S.svd$d since \\(\\mathbf{D}\\) is diagonal. Finally, \\(\\mathbf{z}\\) is the solution of \\(\\mathbf{V}^T \\mathbf{z} = \\tilde{\\mathbf{x}}\\), which we’ll call res6 and can obtain with &gt; z &lt;- S.V %*% x2 since \\(\\mathbf{V}^\\text{-T} = \\mathbf{V}\\). Vectorising this and renaming it to res6, we confirm that we get the same result as in Example 3.10. &gt; res6 &lt;- as.vector(z) &gt; all.equal(res1, res6) ## [1] TRUE So far we have considered systems of linear equations where \\(\\bf A\\) is non-singular, which means that \\({\\bf A}^\\text{-1}\\) is unique. Now we’ll consider the case where \\(\\bf A\\) is singular, although this won’t be examined in MTH3045. Definition 3.16 (Generalised inverse) A generalized inverse matrix of the matrix \\(\\bf A\\) is any matrix \\(\\mathbf{A}^-\\) such that \\[\\mathbf{AA}^-\\mathbf{A} = \\mathbf{A}.\\] Note that \\(\\mathbf{A}^-\\) is not unique. Example 3.23 (Moore-Penrose pseudo-inverse) The Moore-Penrose pseudo-inverse6 of a matrix \\(\\bf A\\) is a generalized inverse that is unique by virtue of stipulating that it must satisfy the following four properties. \\(\\mathbf{AA}^-\\mathbf{A} = \\mathbf{A}\\). \\(\\mathbf{A}^-\\mathbf{A}\\mathbf{A}^- = \\mathbf{A}^-\\). \\((\\mathbf{A}\\mathbf{A}^-)^\\text{T} = \\mathbf{A} \\mathbf{A}^-\\). \\((\\mathbf{A}^-\\mathbf{A})^\\text{T} = \\mathbf{A}^- \\mathbf{A}\\). We can construct a Moore-Penrose pseudo-inverse via the SVD. Consider \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\text{T}\\), the SVD of \\(\\mathbf{A}\\). Let \\(\\mathbf{D}^-\\) denote the generalised inverse of \\(\\mathbf{D}\\), which is simply obtained by taking reciprocals of the positive diagonal values, with zeros left as zeros. Then we have \\[\\mathbf{A}^- = \\mathbf{U} \\mathbf{D}^- \\mathbf{V}^\\text{T}.\\] Example 3.24 (Image processing) Consider the following \\(n \\times m = 338 \\times 450\\) pixel greyscale image which can be represented as a matrix, \\(\\mathbf{A}\\), which we’ll store as A, comprising values on \\([0, 1]\\), where 0 is white and 1 is black; the bottom left \\(7 \\times 7\\) pixels take the following values. &gt; A[1:7, 1:7] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.3516340 0.4718954 0.4483660 0.2718954 0.2522876 0.2758170 0.2758170 ## [2,] 0.4261438 0.5228758 0.4718954 0.2915033 0.2562092 0.2718954 0.2679739 ## [3,] 0.4915033 0.5464052 0.4875817 0.3032680 0.2444444 0.2601307 0.2562092 ## [4,] 0.5346405 0.5503268 0.4915033 0.3150327 0.2326797 0.2601307 0.2758170 ## [5,] 0.5738562 0.5699346 0.5111111 0.3647059 0.2640523 0.3032680 0.3503268 ## [6,] 0.5895425 0.5738562 0.5490196 0.4470588 0.3503268 0.3856209 0.4405229 ## [7,] 0.5895425 0.5816993 0.5647059 0.5163399 0.4339869 0.4535948 0.4836601 Suppose that we compute the SVD \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\text{T}\\). A finite-rank representation of \\(\\mathbf{A}\\) is given by \\(\\mathbf{A}_r = \\mathbf{U}_r \\mathbf{D}_r \\mathbf{V}_r^\\text{T}\\), where \\(\\mathbf{U}_r\\) is the \\(n \\times r\\) matrix comprising the first \\(r\\) columns of \\(\\mathbf{U}\\), \\(\\mathbf{D}_r\\) is the \\(r \\times r\\) matrix comprising the first \\(r\\) rows and columns of \\(\\mathbf{D}\\), and \\(\\mathbf{V}_r\\) is the \\(m \\times r\\) matrix comprising the first \\(r\\) columns of \\(\\mathbf{V}\\). The following shows the resulting greyscale images obtained by plotting \\(\\mathbf{A}_r\\) for \\(r = 1,~2,~5,~10,~20\\) and \\(50\\). Remark. You might wonder how SVD has compressed our image. The image itself takes &gt; format(object.size(A), units = &#39;Kb&#39;) ## [1] &quot;1188.5 Kb&quot; bytes (and there are eight bits in a byte). However, if we consider the \\(r = 20\\) case of Example 3.24, then we need to store the \\(r\\) diagonal elements of \\(\\mathbf{D}_r\\) and the first \\(r\\) columns of \\(\\mathbf{U}_r\\) and \\(\\mathbf{V}_r\\), which we could store in a list &gt; r &lt;- 20 &gt; ind &lt;- 1:r &gt; A_r &lt;- list(diag(D[ind, ind]), U[, ind, drop = FALSE], V[, ind, drop = FALSE]) &gt; format(object.size(A_r), units = &#39;Kb&#39;) ## [1] &quot;123.8 Kb&quot; and takes about 10% of the memory of the original image. Of course, there is computational cost of computing the decomposition, i.e. compressing the image, and then later decompressing the image, which should be taken into account when considering image compression. 3.5.4 QR decomposition The QR decomposition is often used in the background of functions in R. Definition 3.17 (QR decomposition of a square matrix) Any real square matrix \\(\\bf A\\) may be decomposed as \\[\\mathbf{A} = \\mathbf{QR},\\] where \\(\\mathbf{Q}\\) is an orthogonal matrix and \\(\\mathbf{R}\\) is an upper triangular matrix. This is its QR decomposition. In the above, if \\(\\mathbf{A}\\) is non-singular, then the QR decomposition is unique. &gt; qr(A) # computes the QR decomposition of a matrix A Remark. When R computes the QR decomposition, \\(\\bf R\\) is simply the upper triangle of qr()$qr. However, \\(\\bf Q\\) is rather more complicated to obtain, but fortunately qr.Q() does all the calculations for us. 3.5.4.1 Properties \\(|\\text{det}(\\mathbf{A})| = |\\text{det}(\\mathbf{Q})| \\text{det}(\\mathbf{R}) = \\text{det}(\\mathbf{R})\\) since det(\\(\\bf Q\\)) = \\(\\pm 1\\) as \\(\\bf Q\\) is orthogonal. det(\\(\\bf R\\)) = \\(\\prod_{i = 1}^n |r_{ii}|\\), since \\(\\bf R\\) is triangular. \\(\\mathbf{A}^{-1} = (\\mathbf{Q} \\mathbf{R})^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^\\text{T}\\). Example 3.25 (QR decomposition of the Hilbert matrix in R) Use qr() in R to compute the QR decomposition of \\(\\mathbf{H}_3\\), the \\(3 \\times 3\\) Hilbert matrix. Then use qr.Q() and qr.R() to extract \\(\\bf Q\\) and \\(\\bf R\\) from the output of qr() to confirm that \\(\\mathbf{QR} = \\mathbf{H}_3\\). We’ll start with the QR decomposition of \\(\\mathbf{H}_3\\), which we’ll store as qr.H. &gt; qr.H &lt;- qr(H) &gt; qr.H ## $qr ## [,1] [,2] [,3] ## [1,] -1.1666667 -0.6428571 -0.450000000 ## [2,] 0.4285714 -0.1017143 -0.105337032 ## [3,] 0.2857143 0.7292564 0.003901372 ## ## $rank ## [1] 3 ## ## $qraux ## [1] 1.857142857 1.684240553 0.003901372 ## ## $pivot ## [1] 1 2 3 ## ## attr(,&quot;class&quot;) ## [1] &quot;qr&quot; Then qr.Q() and qr.R() will give \\(\\bf Q\\) and \\(\\bf R\\), which we’ll store as Q.H and R.H. &gt; Q.H &lt;- qr.Q(qr.H) &gt; Q.H ## [,1] [,2] [,3] ## [1,] -0.8571429 0.5016049 0.1170411 ## [2,] -0.4285714 -0.5684856 -0.7022469 ## [3,] -0.2857143 -0.6520864 0.7022469 &gt; R.H &lt;- qr.R(qr.H) &gt; R.H ## [,1] [,2] [,3] ## [1,] -1.166667 -0.6428571 -0.450000000 ## [2,] 0.000000 -0.1017143 -0.105337032 ## [3,] 0.000000 0.0000000 0.003901372 And then finally we’ll compute \\(\\bf QR\\) &gt; Q.H %*% R.H ## [,1] [,2] [,3] ## [1,] 1.0000000 0.5000000 0.3333333 ## [2,] 0.5000000 0.3333333 0.2500000 ## [3,] 0.3333333 0.2500000 0.2000000 &gt; all.equal(Q.H %*% R.H, H) ## [1] TRUE which does indeed give \\(\\mathbf{H}_3\\). Example 3.26 (Solving systems of linear equations via the QR decomposition in R) Solve \\(\\boldsymbol{\\Sigma} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\) in R using the QR decomposition and with \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2. Suppose that \\(\\boldsymbol{\\Sigma}\\) has QR decomposition \\(\\boldsymbol{\\Sigma} = \\mathbf{QR}\\). The following calculates the QR decomposition of \\(\\boldsymbol{\\Sigma}\\) as S.qr, and then \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) as S.Q and S.R, respectively. &gt; S.qr &lt;- qr(Sigma) &gt; S.Q &lt;- qr.Q(S.qr) &gt; S.R &lt;- qr.R(S.qr) From Example 3.2 we have that \\(\\mathbf{z}\\) is res1, i.e.  &gt; res1 ## [1] 0.08 -0.38 0.14 Solving \\(\\boldsymbol{\\Sigma} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\) is then equivalent to solving \\(\\mathbf{QR} \\mathbf{z} = \\mathbf{y} - \\boldsymbol{\\mu}\\). So we solve \\(\\mathbf{Q} \\mathbf{x} = \\mathbf{y} - \\boldsymbol{\\mu}\\) for \\(\\mathbf{x}\\) with the following &gt; x &lt;- crossprod(S.Q, y - mu) as \\(\\mathbf{Q}^{-1} = \\mathbf{Q}^\\text{T}\\) because \\(\\mathbf{Q}\\) is orthogonal, and then solve \\(\\mathbf{Rz} = \\mathbf{x}\\) for \\(\\mathbf{z}\\), &gt; z1 &lt;- solve(S.R, x) or, because \\(\\mathbf{R}\\) is upper triangular, &gt; z2 &lt;- backsolve(S.R, x) which are both the same &gt; all.equal(z1, z2) ## [1] TRUE We’ll take the second, z2, and convert it from a one-column matrix to a vector called res7 &gt; res7 &lt;- as.vector(z2) &gt; res7 ## [1] 0.08 -0.38 0.14 and see that this gives the same as before &gt; all.equal(res1, res7) ## [1] TRUE Remark. If we have obtained a QR decomposition in R using qr(), then we can use qr.solve(A, b) to solve \\(\\mathbf{Ax} = \\mathbf{b}\\) for \\(\\mathbf{x}\\). This avoids having to find \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) with qr.Q() and qr.R(), and requires only one function call in R. Theorem 3.3 (QR decomposition of a rectangular matrix) Theorem 3.17 extends to a \\(m \\times n\\) matrix \\(\\bf A\\), with \\(m \\geq n\\), so that \\[\\mathbf{A} = \\mathbf{QR} = \\mathbf{Q} \\begin{bmatrix} \\mathbf{R}_{1} \\\\ \\mathbf{0}\\end{bmatrix} = \\begin{bmatrix} \\mathbf{Q}_{1} &amp; \\mathbf{Q}_{2} \\end{bmatrix} \\begin{bmatrix} \\mathbf{R}_{1}\\\\ \\mathbf{0} \\end{bmatrix} = \\mathbf{Q}_{1} \\mathbf{R}_{1},\\] \\(\\bf Q\\) is an \\(m \\times m\\) unitary matrix, and \\(\\bf R\\) and an \\(m \\times n\\) upper triangular matrix; then \\(\\mathbf{Q}_1\\) is \\(m \\times n\\), \\(\\mathbf{Q}_2\\) is \\(m \\times (m - n)\\), and \\(\\mathbf{Q}_1\\) and \\(\\mathbf{Q}_2\\) both have orthogonal columns, and \\(\\mathbf{R}_1\\) is an \\(n \\times n\\) upper triangular matrix, following by \\((m - n)\\) rows of zeros. Example 3.27 (Linear modelling via QR decomposition) Consider a linear model with response vector \\(\\bf y\\) and design matrix \\(\\bf X\\), where \\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}\\) and \\(\\boldsymbol \\beta\\) is the vector of regression coefficients and \\(\\bf e\\) is the observed residual vector. The least squares estimate of \\(\\boldsymbol \\beta\\), denoted \\(\\hat{\\boldsymbol{\\beta}}\\), satisfies \\(\\mathbf{X}^\\text{T}\\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\text{T} \\mathbf{y}\\). This is achieved in R by forming the QR decomposition of \\(\\mathbf{X}\\), i.e. \\(\\mathbf{X} = \\mathbf{QR}\\). Then \\(\\hat{\\boldsymbol{\\beta}}\\), satisfies \\((\\mathbf{QR})^\\text{T}\\mathbf{QR} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{QR})^\\text{T} \\mathbf{y}\\), which can be re-written as \\(\\mathbf{R}^\\text{T}\\mathbf{Q}^\\text{T}\\mathbf{QR} \\hat{\\boldsymbol{\\beta}} = \\mathbf{R}^\\text{T} \\mathbf{Q}^\\text{T} \\mathbf{y}\\), and, given \\(\\mathbf{Q}\\) is orthogonal, simplifies to \\(\\mathbf{R}^\\text{T}\\mathbf{R} \\hat{\\boldsymbol{\\beta}} = \\mathbf{R}^\\text{T} \\mathbf{Q}^\\text{T} \\mathbf{y}\\). Example 3.28 (MTH2006 cement factory data) Recall the cement factory data from MTH2006 and the linear model \\(Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i\\) where \\(Y_i\\) denotes the output of the cement factory in month \\(i\\) for \\(x_{i1}\\) days at a temperature of \\(x_{i2}\\) degrees Fahrenheit, and where \\(\\varepsilon_i \\sim \\text{N}(0, \\sigma^2)\\) and are i.i.d. Use the QR decomposition to find \\(\\hat{\\boldsymbol{\\beta}}\\) and confirm your answer against that given by lm(). We’ll first input the data &gt; # operating temperatures &gt; temp &lt;- c(35.3, 29.7, 30.8, 58.8, 61.4, 71.3, 74.4, 76.7, 70.7, 57.5, + 46.4, 28.9, 28.1, 39.1, 46.8, 48.5, 59.3, 70, 70, 74.5, 72.1, + 58.1, 44.6, 33.4, 28.6) &gt; # number of operational days &gt; days &lt;- c(20, 20, 23, 20, 21, 22, 11, 23, 21, 20, 20, 21, 21, 19, 23, + 20, 22, 22, 11, 23, 20, 21, 20, 20, 22) &gt; # output from factory &gt; output &lt;- c(10.98, 11.13, 12.51, 8.4, 9.27, 8.73, 6.36, 8.5, 7.82, 9.14, + 8.24, 12.19, 11.88, 9.57, 10.94, 9.58, 10.09, 8.11, 6.83, 8.88, + 7.68, 8.47, 8.86, 10.36, 11.08) and put them into a data.frame called prod (as in MTH2006). &gt; prod &lt;- data.frame(temp = temp, days = days, output = output) Then we’ll fit the linear model with lm(), extract the regression coefficients, and store them as b0. &gt; m0 &lt;- lm(output ~ days + temp, data = prod) &gt; b0 &lt;- coef(m0) Next we’ll store the response data as y and the design matrix as X. &gt; y &lt;- output &gt; X &lt;- cbind(1, days, temp) The QR decomposition of X can be obtained with qr(), which is stored as X.qr, and then \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) of the QR decomposition \\(\\mathbf{X} = \\mathbf{QR}\\) stored as Q.qr and R.qr, respectively. &gt; X.qr &lt;- qr(X) &gt; Q.qr &lt;- qr.Q(X.qr) &gt; R.qr &lt;- qr.R(X.qr) Next we’ll compute \\(\\mathbf{w} = \\mathbf{R}^\\text{T} \\mathbf{Q}^\\text{T} \\mathbf{y}\\), say, then solve \\(\\mathbf{R}^\\text{T} \\mathbf{z} = \\mathbf{w}\\) for \\(\\mathbf{z}\\) and then solve \\(\\mathbf{R} \\hat{\\boldsymbol{\\beta}} = \\mathbf{z}\\) for \\(\\hat{\\boldsymbol{\\beta}}\\). &gt; w &lt;- crossprod(R.qr, crossprod(Q.qr, y)) &gt; z &lt;- forwardsolve(R.qr, w, transpose = TRUE, upper.tri = TRUE) &gt; b1 &lt;- drop(backsolve(R.qr, z)) Finally we’ll check that our regression coefficients calculated through the QR decomposition are the same as those extracted from our call to lm() &gt; all.equal(b0, b1, check.attributes = FALSE) ## [1] TRUE which they are. Remark. Note two things above. 1: We’ve used drop() because backsolve() will by default return a \\((p + 1) \\times 1\\) matrix whereas coef() returns an \\((p + 1)\\)-vector. The two aren’t considered identical by all.equal(), even if their values are the same. Calling drop() will simplify a matrix to a vector if possible, and hence we’re comparing like with like. 2: Setting all.equal(..., check.attributes = FALSE) should check only the supplied objects and not any attributes, such as names. Without this, because b0 has names and b1 doesn’t, all.equal() wouldn’t consider them identical. 3.5.5 Computational costs of matrix decompostions Each of the four matrix decompositions can be used for various purposes, such as calculating the determinant of a matrix, or solving a system of linear equations. Therefore, we might ask ourselves which one we should use. We’ll start by comparing the computational cost of each, i.e. the number of flops each takes. The Cholesky algorithm above requires \\(n^3/3\\) flops (and \\(n\\) square roots). Hence its dominant cost is its \\(n^3/3\\) flops. The dominant cost for the QR decomposition, if computed with householder reflections, is \\(2n^3/3\\), and is therefore roughly twice as expensive as the Cholesky decomposition. The eigen-decomposition and SVD both require \\(k n^3\\) flops, for some value \\(k\\), but \\(k \\gg 1/3\\), as in Cholesky algorithm, making them considerably slower for large \\(n\\). 3.6 Sherman-Morrison formula / Woodbury matrix identity In general, unless we actually need the inverse of a matrix (such as for standard errors of regression coefficients in a linear model), we should solve systems of linear equations. Sometimes, though, we do need – or might just have – an inverse, and want to calculate something related to it. The following are a set of formulae, which go by various names, that can be useful in this situation. 3.6.1 Woodbury’s formula Consider an \\(m \\times m\\) matrix \\(\\bf A\\), with \\(m\\) large, and for which we have the inverse, \\(\\mathbf{A}^{-1}\\). Suppose \\(\\bf A\\) receives a small update of the form \\(\\mathbf{A} + \\mathbf{UV}^\\text{T}\\), for \\(m \\times n\\) matrices \\(\\bf U\\) and \\(\\bf V\\) where \\(n \\ll m\\). Then, by Woodbury’s formula, \\[ (\\mathbf{A} + \\mathbf{UV}^\\text{T})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_n + \\mathbf{V}^\\text{T} \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^\\text{T} \\mathbf{A}^{-1}. \\] Remark. What’s important to note here is that we’re looking to calculate an \\(m \\times m\\) inverse with \\(m\\) large, and so in general this will be an \\(O(m^3)\\) calculation based on the LHS. However, in the above, the RHS only requires that we to invert an \\(n \\times n\\) matrix, at cost \\(O(n^3)\\), which is much less that of the LHS. 3.6.2 Sherman-Morrison-Woodbury formula Woodbury’s formula above generalises to the so-called Sherman-Morrison-Woodbury formula by introducing the \\(n \\times n\\) matrix \\(\\bf C\\), so that \\[ (\\mathbf{A} + \\mathbf{UCV}^\\text{T})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{C}^{-1} + \\mathbf{V}^\\text{T} \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^\\text{T} \\mathbf{A}^{-1}. \\] 3.6.3 Sherman-Morrison formula The Sherman-Morrison formula is the special case of Woodbury’s formula (and hence the Woodbury-Sherman-Morrison formula) in which the update to \\(\\bf A\\) can be considered in terms of \\(m\\)-vectors \\(\\bf u\\) and \\(\\bf v\\), so that \\[ (\\mathbf{A} + \\mathbf{uv}^\\text{T})^{-1} = \\mathbf{A}^{-1} - \\dfrac{\\mathbf{A}^{-1} \\mathbf{u} \\mathbf{v}^\\text{T} \\mathbf{A}^{-1}}{1 + \\mathbf{v}^\\text{T}\\mathbf{A}^{-1}\\mathbf{u}}. \\] Remark. The Sherman-Morrison formula is particularly useful because it requires no matrix inversion. Example 3.29 (Bayesian linear regression) Recall from MTH2006 the normal linear model where \\[ Y_i \\sim N(\\mathbf{x}_i^\\text{T} \\boldsymbol{\\beta}, \\sigma^2) \\] with independent errors \\(\\varepsilon_i = Y_i - \\mathbf{x}_i^\\text{T} \\boldsymbol{\\beta}\\)s, for \\(i = 1, \\ldots, n\\), where \\(\\mathbf{x}_i^\\text{T} = (1, x_{i1}, \\ldots, x_{ip})\\) is the \\(i\\)th row of design matrix \\(\\bf X\\) and where \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^\\text{T}\\). Hence \\(\\mathbf{Y} \\mid \\mathbf{X} \\boldsymbol{\\beta} \\sim MVN_n(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n)\\). In Bayesian linear regression, the elements of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are not fixed, unknown parameters: they are random variables, and we must declare a priori our beliefs about their distribution. The conjugate prior is that \\[ \\boldsymbol{\\beta} \\sim MVN_{p+1}(\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}}, \\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}). \\] Integrating out \\(\\boldsymbol{\\beta}\\) gives \\(\\mathbf{Y} \\sim MVN_n(\\mathbf{X}\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}}, \\sigma^2 \\mathbf{I}_n + \\mathbf{X} \\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1} \\mathbf{X}^\\text{T})\\). Now suppose that we want to evaluate the marginal likelihood for an observation, \\(\\bf y\\), say. Recall (3.1). The Mahalanobis distance then involves the term \\[ (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}})^\\text{T} (\\sigma^2\\mathbf{I}_n + \\mathbf{X} \\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1} \\mathbf{X}^\\text{T})^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}}). \\] The covariance of the marginal distribution, \\(\\sigma^2 \\mathbf{I}_n + \\mathbf{X} \\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1} \\mathbf{X}^\\text{T}\\), is typically dense, expensive to form, and leads to expensive solutions to systems of linear equations. Its inverse, however, can be computed through the Sherman-Morrison formula with \\(\\mathbf{A}^{-1} = \\sigma^{-2} \\mathbf{I}_n\\), \\(\\mathbf{U} = \\mathbf{V} = \\mathbf{X}\\), and \\(\\mathbf{C} =\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}\\). 3.7 Bibliographic notes For more details on matrix decompositions, consider Wood (2015, Appendix B) for a concise overview. For fuller details consider Monahan (2011, chaps. 3, 4 and 6) or Press et al. (2007, chap. 2 and 11). André-Louis Cholesky (15 Oct 1875 – 31 Aug 1918) was a French military officer and mathematician. He worked in geodesy and cartography, and was involved in the surveying of Crete and North Africa before World War I. He is primarily remembered for the development of a matrix decomposition known as the Cholesky decomposition which he used in his surveying work.His discovery was published posthumously by his fellow officer Commandant Benoît in the Bulletin Géodésique.↩︎ The Moore-Penrose pseudoinverse is named after E. H. Moore and Sir Roger Penrose. Moore first worked in abstract algebra, proving in 1893 the classification of the structure of finite fields (also called Galois fields). He then worked on various topics, including the foundations of geometry and algebraic geometry, number theory, and integral equations. Penrose has made contributions to the mathematical physics of general relativity and cosmology. He has received several prizes and awards, including the 1988 Wolf Prize in Physics, which he shared with Stephen Hawking for the Penrose–Hawking singularity theorems, and one half of the 2020 Nobel Prize in Physics “for the discovery that black hole formation is a robust prediction of the general theory of relativity”.↩︎ "],["numerical-calculus.html", "4 Numerical Calculus 4.1 Motivation 4.2 Numerical Differentiation 4.3 Quadrature 4.4 Laplace’s method 4.5 Monte Carlo integration 4.6 Bibliographic notes", " 4 Numerical Calculus 4.1 Motivation Example 4.1 In statistics, we often rely on the Normal distribution with pdf \\[ \\phi(x; \\mu, \\sigma^2) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left[-\\dfrac{(x - \\mu)^2}{2 \\sigma^2}\\right] \\] and cdf \\[ \\Phi(x; \\mu, \\sigma^2) = \\int_{-\\infty}^x \\phi(x; \\mu, \\sigma^2) \\text{d} x. \\] Unfortunately no closed form exists for \\(\\Phi(x; \\mu, \\sigma^2)\\). However, if \\(x\\), \\(\\mu\\) and \\(\\sigma\\) are stored in R as x, mu and sigma, we can still evaluate \\(\\Phi(x; \\mu, \\sigma^2)\\) with pnorm(x, mu, sigma). This is one example of a frequently-occurring situation in which we somehow want to evaluate an intractable integral. This raises the question: are there generic methods that let us evaluate intractable integrals? The answer is often numerical integration. Remark. In this chapter, we’ll consider generic methods for integration, i.e. that work in many scenarios. Sometimes, such as evaluating \\(\\Phi(x; \\mu, \\sigma^2)\\), specific algorithms will give more accurate results. This is what R does for pnorm(). 4.2 Numerical Differentiation In Chapter 5 we will cover optimisation of functions, such as numerically finding maximum likelihood estimates when analytical solutions aren’t available. We’ll see that supplying derivatives can considerably improve estimation, typically in terms of reducing computation time. Here we’ll cover some useful results in terms of differentiation of matrices, which will later prove useful. No knowledge of analytical matrix calculus beyond these results will be needed for MTH3045. The matrix cookbook (Petersen and Pedersen 2012), however, can provide you with a much more thorough set of differentiation rules, should you ever need them. 4.2.1 Differentiation definitions Definition 4.1 (Gradient operator) Consider \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), which we’ll consider a function of vector \\(\\mathbf{x} = (x_1, \\ldots, x_n)^\\text{T}\\). The gradient operator, \\(\\nabla\\), is defined as \\[ \\nabla f(\\mathbf{x}) = \\left(\\begin{array}{c} \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}\\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}\\\\ \\vdots \\\\ \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\\\ \\end{array}\\right). \\] Note that in MTH3045 we will have no cause to consider multivariate functions, i.e. to consider \\(\\mathbf{f} : \\mathbb{R}^n \\to \\mathbb{R}^m\\), for \\(m &gt; 1\\). Definition 4.2 (Hessian matrix) Consider again \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\). The Hessian matrix is the matrix of second derivatives of \\(f\\), whereas the gradient operator considered first derivatives, and is given by \\[ \\nabla^2 f(\\mathbf{x}) = \\left(\\begin{array}{cccc} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\\\ \\end{array}\\right). \\] The Hessian matrix plays an important role in statistics, in particular for estimating parameter uncertainty via the Fisher information, which is covered in MTH3028. In the next chapter, we’ll also see that it’s important for optimisation. Remark. If \\(\\tilde{\\mathbf{x}}\\) is at a minimum of \\(f(\\mathbf{x})\\) then the gradient vector w.r.t. \\(\\mathbf{x}\\) should be all zero, i.e. \\(\\nabla f(\\mathbf{x}) = \\mathbf{0}\\) and, additionally, the Hessian matrix, i.e. \\(\\nabla^2 f(\\mathbf{x})\\) should be positive definite. 4.2.2 Differentiation rules Now consider \\(g : \\mathbb{R}^n \\to \\mathbb{R}\\) a function of \\(\\bf x\\) that, for fixed \\(\\bf A\\), takes the quadratic form \\(g(\\mathbf{x}) = \\mathbf{x}^\\text{T} \\mathbf{Ax}\\) for \\(n \\times n\\) matrix \\(\\mathbf{A}\\) and \\(n\\)-vector \\(\\mathbf{x}\\). Then \\[ \\nabla g(\\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^\\text{T})\\mathbf{x} ~~~\\text{and}~~~ \\nabla^2 g(\\mathbf{x}) = \\mathbf{A} + \\mathbf{A}^\\text{T}.\\] Note that in the case of symmetric \\(\\bf A\\), \\(\\nabla g(\\mathbf{x}) = 2\\mathbf{Ax}\\) and \\(\\nabla^2 g(\\mathbf{x}) = 2\\mathbf{A}\\). Next consider \\(h : \\mathbb{R}^n \\to \\mathbb{R}\\) a function of \\(n\\)-vector \\(\\bf x\\) and \\(p\\)-vector \\(y\\) that, for fixed \\(n \\times n\\) matrix \\(\\bf A\\) and \\(n \\times p\\) matrix \\(\\bf B\\), takes the quadratic form \\(h(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x} + \\mathbf{By})^\\text{T} \\mathbf{A}(\\mathbf{x} + \\mathbf{By})\\). Then \\[ \\dfrac{\\partial h(\\mathbf{x}, \\mathbf{y})}{\\partial \\mathbf{x}} = (\\mathbf{A} + \\mathbf{A}^\\text{T})(\\mathbf{x} + \\mathbf{By})~~~\\text{and}~~~ \\dfrac{\\partial h(\\mathbf{x}, \\mathbf{y})}{\\partial \\mathbf{y}} = \\mathbf{B}^T (\\mathbf{A} + \\mathbf{A}^\\text{T}) (\\mathbf{x} + \\mathbf{By}), \\] and also \\[ \\dfrac{\\partial^2 h(\\mathbf{x}, \\mathbf{y})}{\\partial \\mathbf{x} \\partial \\mathbf{x}^\\text{T}} = \\mathbf{A} + \\mathbf{A}^\\text{T}~~~\\text{and}~~~ \\dfrac{\\partial^2 h(\\mathbf{x}, \\mathbf{y})}{\\partial \\mathbf{y} \\partial \\mathbf{y}^\\text{T}} = \\mathbf{B}^\\text{T}(\\mathbf{A} + \\mathbf{A}^\\text{T})\\mathbf{B}. \\] Note that above we use partial derivative notation, i.e. \\(\\partial\\), as opposed to gradient operator notation, i.e. \\(\\nabla\\), as the derivatives are not w.r.t. all variables. Example 4.2 (Maximum likelihood estimates of regression coefficients in the normal linear model via matrix calculus) Consider the normal linear model \\(\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\\), where \\(\\mathbf{Y} = (Y_1, \\ldots, Y_n)^\\text{T}\\), \\(\\mathbf{X}\\) is an \\(n \\times (p + 1)\\) design matrix, \\(\\boldsymbol{\\beta}\\) is a \\((p + 1)\\)-vector of regression coefficients, and \\(\\boldsymbol{\\varepsilon}= (\\varepsilon_1, \\ldots, \\varepsilon_n)^\\text{T}\\) with independent \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\). The maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\), denoted \\(\\hat{\\boldsymbol{\\beta}}\\), minimises the RSS, i.e. minimises \\((\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^\\text{T}(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\) if we observe \\(\\mathbf{y} = (y_1, \\ldots, y_n)^\\text{T}\\). Differentiating w.r.t. \\(\\boldsymbol{\\beta}\\) gives \\(-\\mathbf{X}^\\text{T} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) - (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^\\text{T} \\mathbf{X}\\), which simplifies to \\(-2\\mathbf{X}^\\text{T} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\) as \\(\\mathbf{X}^\\text{T} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})\\) and \\((\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^\\text{T} \\mathbf{X}\\) are both \\(n\\)-vectors. The derivative is zero at \\(\\hat{\\boldsymbol{\\beta}}\\) and so \\(-2\\mathbf{X}^\\text{T} (\\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}) = 0\\). Therefore \\(\\hat{\\boldsymbol{\\beta}}\\) is the solution of \\(\\mathbf{X}^\\text{T} \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\text{T} \\mathbf{y}\\) or alternatively \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\text{T} \\mathbf{X})^{-1} \\mathbf{X}^\\text{T} \\mathbf{y}\\), as we were given in Topic 3. 4.2.3 Finite-differencing Consider again \\(f(\\mathbf{x})\\), a function of vector \\(\\bf x\\). Definition 4.3 (Partial derivative) Consider \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) for \\(n\\)-vector \\(\\bf x\\). Let, \\(\\mathbf{e}_i\\) be the \\(n\\)-vector comprising entirely zeros, except for its \\(i\\)th element, which is one. Then the partial derivative of \\(f(\\mathbf{x})\\) w.r.t. \\(x_i\\), the \\(i\\)th element of \\(\\bf x\\), is \\[ \\dfrac{\\partial f(\\mathbf{x})}{\\partial x_i} = \\lim_{h \\to 0} \\dfrac{f(\\mathbf{x} + h \\mathbf{e}_i) - f(\\mathbf{x})}{h}. \\] The above definition leads us to the finite-difference partial derivative approximation \\[ \\dfrac{\\partial f(\\mathbf{x})}{\\partial x_i} \\simeq \\dfrac{f(\\mathbf{x} + \\delta \\mathbf{e}_i) - f(\\mathbf{x})}{\\delta}, \\] where \\(\\delta\\) is small. Example 4.3 (Finite-differencing of $\\sin(x)$) Use finite-differencing to approximate the derivative of \\(f(x) = \\sin(x)\\) for \\(x \\in [0, 2\\pi]\\), and compare its accuracy to the true derivative. First, let’s calculate \\(f\\) and its true derivative, i.e. \\(f&#39;(x) = \\cos(x)\\), and store this as partial0 for \\(\\{x_i\\}\\), \\(i = 1, \\ldots, 100\\), a set of equally-spaced points on \\([0, 2\\pi]\\). &gt; n &lt;- 100 &gt; x &lt;- seq(0, 2 * pi, l = 100) &gt; f &lt;- sin(x) &gt; partial0 &lt;- cos(x) Now we’ll set \\(\\delta = 10^{-6}\\) and calculate \\(x_i + \\delta\\), for each \\(x_i\\), i.e. each element of x, so that the finite-difference approximation to the derivative is given by \\([\\sin(x_i + 10^{-6}) - \\sin(x_i)] / 10^{-6}\\), which is calculated below and stored as partial1. &gt; delta1 &lt;- 1e-6 &gt; x1 &lt;- x + delta1 &gt; f1 &lt;- sin(x1) &gt; partial1 &lt;- (f1 - f) / delta1 We’ll then plot the \\(f&#39;(x)\\) against it finite-difference approximation. &gt; matplot(x, cbind(partial0, partial1), type = &#39;l&#39;, xlab = &#39;x&#39;, ylab = &quot;f&#39;(x)&quot;) &gt; legend(&#39;bottomleft&#39;, lty = 1:2, col = 1:2, lwd = 1:2, + legend = c(&#39;True&#39;, &#39;Finite-difference&#39;)) In fact, the true derivative and its finite-difference approximation are so similar that’s it’s difficult to distinguish the two, but they’re both there in the plot! Remark. We might be tempted to choose \\(\\delta\\) as small as possible. Suppose we were to repeat Example 4.3 with \\(\\delta = \\epsilon_m\\), i.e. R’s machine tolerance. The following calculates the finite-difference approximation as partial2 and plots this against the true value of \\(f&#39;(x)\\). &gt; delta2 &lt;- .Machine$double.eps &gt; x2 &lt;- x + delta2 &gt; f2 &lt;- sin(x2) &gt; partial2 &lt;- (f2 - f) / delta2 &gt; matplot(x, cbind(partial0, partial2), type = &#39;l&#39;, xlab = &#39;x&#39;, ylab = &quot;f&#39;(x)&quot;) &gt; legend(&#39;bottomleft&#39;, lty = 1:2, col = 1:2, bg = &#39;white&#39;, + legend = c(&#39;True&#39;, &#39;Finite-difference with machine tolerance&#39;)) Using \\(\\delta = \\epsilon_m\\) gives a terrible approximation to \\(f&#39;(x)\\), which gets worse as \\(x\\) increases. We’ve actually encountered an example calculation error, which was introduced in Chapter 2. Example 4.4 (Finite-differencing of the multivariate Normal log-likelihood) Find \\(\\partial \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) / \\partial y_i\\) analytically, for \\(i = 1, \\ldots, p\\), where \\(f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) is the \\(MVN_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) pdf, for arbitrary \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\). Evaluate this for \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2. Then approximate the same derivative using finite-differencing. The logarithm of the \\(MVN_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) pdf is given by \\[ \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\dfrac{1}{2} \\left[p \\log(2 \\pi) + \\log(|\\boldsymbol{\\Sigma}|) + ({\\bf y} - {\\boldsymbol \\mu})^\\text{T} {\\boldsymbol \\Sigma}^{-1} ({\\bf y} - {\\boldsymbol \\mu}) \\right] \\] which we know, from Example 3.13, we can evaluate with dmvn3(). Using the above properties \\[ \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})}{\\partial \\mathbf{y}} = -{\\boldsymbol \\Sigma}^{-1} ({\\bf y} - {\\boldsymbol \\mu}) \\] since \\(\\boldsymbol{\\Sigma}\\) is symmetric. We can evaluate this for \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\) and \\(\\boldsymbol{\\Sigma}\\) as in Example 3.2 with the following &gt; y &lt;- c(.7, 1.3, 2.6) &gt; mu &lt;- 1:3 &gt; Sigma &lt;- matrix(c(4, 2, 1, 2, 3, 2, 1, 2, 2), 3, 3) &gt; deriv1 &lt;- -solve(Sigma, y - mu) which gives &gt; as.vector(deriv1) ## [1] -0.08 0.38 -0.14 To approximate the derivative by finite-differencing, it makes sense to write a multi-purpose function for finite differencing, which we’ll call fd(). &gt; fd &lt;- function(x, f, delta = 1e-6, ...) { + # Function to evaluate derivative by finite-differencing + # x is a p-vector + # fn is the function for which the derivative is being calculated + # delta is the finite-differencing step, which defaults to 10^{-6} + # returns a vector of length x + f0 &lt;- f(x, ...) + p &lt;- length(x) + f1 &lt;- numeric(p) + for (i in 1:p) { + x1 &lt;- x + x1[i] &lt;- x[i] + delta + f1[i] &lt;- f(x1, ...) + } + (f1 - f0) / delta + } Then we can use this with dmvn3() with the following &gt; deriv2 &lt;- fd(y, dmvn3, mu = mu, Sigma = Sigma) which gives &gt; deriv2 ## [1] -0.0800002 0.3799993 -0.1400008 and is the same as the analytical result &gt; all.equal(deriv1, deriv2) ## [1] &quot;Mean relative difference: 2.832689e-06&quot; once we allow for error in the finite-difference approximation. 4.3 Quadrature Another common requirement in statistics that some integral needs to be evaluated. To start, let’s consider a simple integral of the form \\[ I = \\int_a^b f(x) \\text{d}x. \\] We’ll first take a look at some deterministic approaches to numerically evaluating integrals. In fact, these all boil down to assuming that \\[ I \\simeq \\sum_{i=1}^N w_i f(x_i^*) \\] for some weights \\(w_i\\) and nodes \\(x_i^*\\), \\(i = 1, \\ldots, N\\). Note that here we’re considering the so-called composite approach to approximating an interval, i.e. in which a rule is applied over a collection of sub-intervals. Definition 4.4 (Relative absolute error) The relative absolute error, or sometimes just relative error, of an estimate of some true value is given by \\[ \\left\\vert \\dfrac{\\text{true value} - \\text{estimate}}{\\text{true value}} \\right\\vert. \\] 4.3.1 Midpoint rule Perhaps the first numerical integration scheme we come across is the mid point rule. Put simply, we divide \\([a, b]\\) into \\(N\\) equally-sized intervals, and use the midpoints of these as the nodes, \\(x_i^*\\). This gives \\[ \\int_a^b f(x) \\text{d}x \\simeq h \\sum_{i = 1}^N f(x_i^*), \\] where \\(x_i^* = a + (i - 0.5)(b - a)/N\\) and \\(h = (b - a) / N\\). The error in the approximation is \\(O(h^2)\\). Thus more intervals reduces \\(h\\) and gives a more accurate approximation. Example 4.5 (Midpoint rule) Consider the integral \\(\\int_0^1 \\exp(x) \\text{d}x = \\exp(1) - 1 \\simeq 1.7182818\\). Use R and the midpoint rule to estimate the integral with \\(N = 10\\), \\(100\\) and \\(1000\\). Then compare the relative absolute error of each. We’ll start by calculating the true value of the integral, which we’ll stored as true. &gt; true &lt;- exp(1) - 1 Then we’ll store the values of \\(N\\) that we’re testing as N_vals. &gt; N_vals &lt;- 10^c(1:3) The following then creates a vector, midpoint, in which to store the integral approximations, and calculates the approximations with a for loop. Inside the loop the integration nodes (i.e. the midpoints) and \\(h\\) are calculated. &gt; midpoint &lt;- numeric(length(N_vals)) &gt; for (i in 1:length(N_vals)) { + N &lt;- N_vals[i] + nodes &lt;- (1:N - .5) / N + h &lt;- 1 / N + midpoint[i] &lt;- h * sum(exp(nodes)) + } &gt; midpoint ## [1] 1.717566 1.718275 1.718282 The relative absolute error for each is then given in the vector rel_err_mp below. &gt; rel_err_mp &lt;- abs((true - midpoint) / true) &gt; rel_err_mp ## [1] 4.165452e-04 4.166655e-06 4.166667e-08 We clearly see that the absolute error reduces by two factors of ten for each factor of ten increase in \\(N\\), which is consistent with the above comment of \\(O(h^2)\\) error, where here \\(h = 1/N\\). 4.3.2 Simpson’s rule The midpoint rule works simply by approximating \\(f(x)\\) over a sub-interval of \\([a, b]\\) by a horizontal line. The trapezium rule (which we’ll overlook) assumes a straight line. Simpson’s rule is derived from a quadratic approximation and given by \\[\\begin{equation} \\int_a^b f(x) \\text{d}x \\simeq \\dfrac{h}{6} \\left(f(a) + 4 \\sum_{i = 1}^N f(x_{1i}^*) + 2\\sum_{i = 1}^{N - 1} f(x_{2i}^*) + f(b)\\right), \\tag{4.1} \\end{equation}\\] where \\(x_{1i}^* = a + h(2i - 1) / 2\\), \\(x_{2i}^* = a + ih\\) and \\(h = (b - a)/N\\). Note that Simpson’s rule requires \\(N + 1\\) more evaluations of \\(f\\) than the midpoint rule; however, a benefit of those extra evaluations is that its error reduces to \\(O(h^4)\\). Example 4.6 (Simpson's rule) Now use R and Simpson’s rule to approximate the integral \\(\\int_0^1 \\exp(x) \\text{d}x = \\exp(1) - 1\\) with \\(N = 10\\), \\(100\\) and \\(1000\\), compare the relative absolute error for each, and against those of the midpoint rule in Example 4.5. We already have true and N_vals from Example 4.5, and we can use a similar for loop to approximate the integral using Simpson’s rule. The main difference is that we create two sets of nodes, nodes1 and nodes2, which correspond to the \\(x_{1i}\\)s and \\(x_{2i}\\)s in Equation (4.1), respectively. The integral approximations are stored as simpson &gt; simpson &lt;- numeric(length(N_vals)) &gt; N_vals &lt;- 10^c(1:3) &gt; for (i in 1:length(N_vals)) { + N &lt;- N_vals[i] + h &lt;- 1 / N + simpson[i] &lt;- 1 + exp(1) + nodes1 &lt;- h * (2*c(1:N) - 1) / 2 + simpson[i] &lt;- simpson[i] + 4 * sum(exp(nodes1)) + nodes2 &lt;- h * c(1:(N - 1)) + simpson[i] &lt;- simpson[i] + 2 * sum(exp(nodes2)) + simpson[i] &lt;- h * simpson[i] / 6 + } &gt; print(simpson, digits = 12) ## [1] 1.71828188810 1.71828182847 1.71828182846 and we print this to 11 decimal places so we can see where the approximations changes with \\(N\\). Finally we calculate the relative absolute errors, rel_err_simp, &gt; rel_err_simp &lt;- abs((true - simpson) / true) &gt; rel_err_simp ## [1] 3.471189e-08 3.472270e-12 6.461239e-16 and we see a dramatic improvement in the accuracy of approximation that Simpson’s rule brings, with relative absolute errors of the same order of magnitude as those form the midpoint rule using \\(N = 1000\\) achieved with \\(N=10\\) for Simpson’s rule. Note, though, that for given \\(N\\), Simpson’s rule requires \\(N + 1\\) more evaluations of \\(f()\\). 4.3.3 Gaussian quadrature We’ve seen that Simpson’s rule can considerably improve on the midpoint rule for approximating integrals. However, we might still consider both restrictive in that they consider an equally-spaced set of nodes. Definition 4.5 (Gauss-Legendre quadrature rule) Consider \\(g(x)\\), a polynomial of degree \\(2N - 1\\), and a fixed weight function \\(w(x)\\). Then, the Gauss-Legendre quadrature rule states that \\[\\int_a^b w(x) g(x) \\text{d}x = \\sum_{i = 1}^N w_i g(x_i),\\] where, for \\(i = 1, \\ldots, N\\), \\(w_i\\) and \\(x_i\\) depend on \\(w(x)\\), \\(a\\) and \\(b\\), but not \\(g(x)\\). The Gauss-Legendre quadrature rule is the motivation for Gaussian quadrature, whereby we assume that the integral we’re interested in can be well-approximated by a polynomial. This results in the approximation \\[\\int_a^b f(x) \\text{d}x \\simeq \\sum_{i = 1}^N w_i f(x_i)\\] for a fixed set of \\(x\\) values, \\(x_i\\) with corresponding weights \\(w_i\\), for \\(i = 1, \\ldots, N\\). There are many rules for choosing the weights, \\(w_i\\), but (perhaps fortunately) we won’t go into them in detail in MTH3045. Instead, we’ll just consider the function pracma::gaussLegrendre() (for which you’ll need to install the pracma package), where pracma::gaussLegrendre(N, a, b) produces N nodes and corresponding weights on the interval \\([\\)a,b\\(]\\), with \\(N =\\) N, \\(a =\\) a and \\(b =\\) b. The following produces nodes and weights for \\(N = 10\\) on \\([0, 1]\\) &gt; gq &lt;- pracma::gaussLegendre(10, 0, 1) &gt; gq ## $x ## [1] 0.01304674 0.06746832 0.16029522 0.28330230 0.42556283 0.57443717 ## [7] 0.71669770 0.83970478 0.93253168 0.98695326 ## ## $w ## [1] 0.03333567 0.07472567 0.10954318 0.13463336 0.14776211 0.14776211 ## [7] 0.13463336 0.10954318 0.07472567 0.03333567 Figure 4.1: Node and weights for Gauss-Legendre quadrature with \\(N = 10\\) for integral on [0, 1]. which we can see in Figure 4.1, and shows that nodes are spread further apart towards the middle of the \\([0, 1]\\) range, but given more weight. Note that pracma::gaussLegrendre() is named so because it implements Gauss-Legendre quadrature, i.e. Gaussian quadrature with Legendre polynomials7. Example 4.7 (Gaussian quadrature) Now use R and Gauss-Legendre quadrature to approximate the integral \\(\\int_0^1 \\exp(x) \\text{d}x\\) with \\(N = 10\\). Explore what value of \\(N\\) gives a comparable estimate to that of the midpoint rule with \\(N = 100\\) based on relative absolute error. We can re-use true from Example 4.5 and then we’ll consider \\(N=10\\), 4 and 3, which we’ll call N_vals, and store the resulting integral approximations in gauss. &gt; N_vals &lt;- c(10, 4, 3) &gt; gauss &lt;- numeric(length(N_vals)) &gt; for (i in 1:length(N_vals)) { + N &lt;- N_vals[i] + xw &lt;- pracma::gaussLegendre(N, 0, 1) + gauss[i] &lt;- sum(xw$w * exp(xw$x)) + } &gt; gauss ## [1] 1.718282 1.718282 1.718281 The relative absolute errors, rel_err_gauss, &gt; rel_err_gauss &lt;- abs((true - gauss) / true) &gt; rel_err_gauss ## [1] 2.584496e-16 5.429651e-10 4.795992e-07 show that, having considered \\(N = 3, 4, 10\\), choosing \\(N = 3\\) for Gaussian quadrature gives closest relative absolute error to that of the midpoint rule with \\(N = 100\\), which really is quite impressive. It is important to note, though, that \\(f(x) = \\exp(x)\\) is a very smooth function. For wiggler functions, larger \\(N\\) is likely to be needed, and improvements in performance, such as Gaussian quadrature over the midpoint rule, might be significantly less. Example 4.8 (Poisson marginal approximation using Gaussian quadrature) Consider a single random variable \\(Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)\\), where we can characterise our prior beliefs about \\(\\lambda\\) as \\(\\lambda \\sim \\text{N}(\\mu, \\sigma^2)\\). Use Gaussian quadrature with \\(N = 9\\) to estimate the marginal pdf of \\(Y\\) if \\(\\mu = 10\\) and \\(\\sigma = 3\\). The marginal distribution of \\(Y\\) is given by \\[ f(y) = \\int_{-\\infty}^\\infty f(y \\mid \\lambda) f(\\lambda) \\text{d}\\lambda. \\] Remark. The three sigma rule is a heuristic rule of thumb that 99.7% of values lie within three standard deviations of the mean. Hence for the \\(\\text{N}(10, 3^2)\\) distribution we should expect 99.7% of values to lie within \\(10 \\pm 3 \\times3\\). Hence we’ll take this as our range for the Gaussian quadrature nodes. &gt; mu &lt;- 10 &gt; sigma &lt;- 3 &gt; N &lt;- 9 # no. of nodes &gt; xw &lt;- pracma::gaussLegendre( + N, + mu - 3 * sigma, # left-hand end + mu + 3 * sigma # right-hand end + ) &gt; xw ## $x ## [1] 1.286558 2.475720 4.479657 7.081719 10.000000 12.918281 15.520343 ## [8] 17.524280 18.713442 ## ## $w ## [1] 0.7314695 1.6258334 2.3454963 2.8111237 2.9721542 2.8111237 2.3454963 ## [8] 1.6258334 0.7314695 which are stored as xw$x with corresponding weights xw$w, \\(w_1, \\ldots, w_N\\). Next we want a set of values at which to evaluate \\(f(y)\\), and for this we’ll choose \\(0, 1, \\ldots, 30\\), which we can create in R with &gt; y_vals &lt;- 0:30 Then we can estimate \\(f(y)\\) as \\[ \\hat f(y) \\simeq \\sum_{i = 1}^N w_i f(y \\mid \\lambda_i^*) f(\\lambda_i^*). \\] The following code gives \\(\\hat f(y)\\) as fhat for \\(y\\) in the set of values y_vals. &gt; m &lt;- length(y_vals) &gt; fhat &lt;- numeric(m) &gt; for (i in 1:m) { + fhat[i] &lt;- sum(xw$w * dpois(y_vals[i], xw$x) * + dnorm(xw$x, mu, sigma)) + } Finally, we’ll plot \\(\\hat f(y)\\) against the pdf of the Poisson(\\(10\\)) distribution &gt; matplot(y_vals, cbind(fhat, dpois(y_vals, mu)), lwd = 1, + col = 1:2, lty = 1, type = &#39;l&#39;, xlab = &#39;y&#39;, ylab = &#39;f(y)&#39;) &gt; legend(&#39;topright&#39;, c(&quot;f(y)&quot;, &quot;f(y | 10)&quot;), + lty = 1, col = 1:2) which demonstrates that \\(f(y)\\) is broader than \\(f(y \\mid 10)\\), which is to be expected given that \\(f(y)\\) integrates out the variability in \\(\\lambda\\) given by the \\(\\text{N}(10, 3^2)\\) distribution. 4.3.4 One-dimensional numerical integration in R Unsurprisingly, R has a function for one-dimensional numerical integration. It’s called integrate(). It uses a method that builds on Gaussian quadrature, but we won’t go into its details. Use of integrate(), however, is fairly straightforward. Example 4.9 (Integration with integrate()) Evaluate the integral \\(\\int_0^1 \\exp(x) \\text{d}x = \\exp(1) - 1\\) using R’s integrate() function with \\(N = 10\\) and report its relative absolute error. We can use the following code, where the first argument to integrate() is the function we’re integrating, the second and third are the lower and upper ranges of the definite integral, and subdivisions is the maximum number of nodes to use in the approximation, which defaults to 100. &gt; true &lt;- exp(1) - 1 &gt; estimate &lt;- integrate(function(x) exp(x), 0, 1, subdivisions = 10) &gt; rel_err &lt;- abs(true - estimate$value) / true Note above that the absolute error is similarly tiny to that of Gaussian quadrature above. The values themselves, being so close to the machine tolerance, are incomparable. But we can be sure that the approximation is incredibly accurate. 4.3.5 Multi-dimensional quadrature Now suppose that we want to integrate a multi-variable function over some finite range. For simplicity, we’ll just consider the case of function of two variables, \\(f(x, y)\\). Then, by the earlier one-dimensional Gaussian quadrature we have that \\[ \\int f(x, y_j) \\text{d} x \\simeq \\sum_{i = 1}^M w_{x,i} f(x_i, y_j) \\] from which it follows, by another application of Gaussian quadrature, that \\[ \\int \\int f(x, y) \\text{d} x \\text{d} y \\simeq \\sum_{i=1}^{M} w_{x,i} \\sum_{j=1}^N w_{y,j} f(x_i, y_j). \\] Note that the weight sequences \\(w_{x, i}\\), \\(i = 1, \\ldots, M\\), and \\(w_{y, j}\\), \\(j = 1, \\ldots, N\\), can be specified separately, i.e. according to different quadrature rules. More generally, we therefore have that \\[ \\int \\ldots \\int f(x_1, \\ldots, x_d) \\text{d} x_1 \\ldots, \\text{d} x_d = \\int f(\\mathbf{x}) \\text{d} \\mathbf{x} \\simeq \\sum_{i_1=1}^{N_1} \\ldots \\sum_{i_d=1}^{N_d} w_{x_1,i_1} \\ldots w_{x_d, i_d} f(x_{1, i_1}, \\ldots, x_{d, i_d}). \\] Remark. In practice, multi-dimensional quadrature is only feasible for small numbers of dimensions. For example, consider a \\(d\\)-variable function \\(f\\), such that each dimension has \\(N\\) nodes. This will require \\(N^d\\) evaluations of \\(f\\). Some useful numbers to draw upon are \\(10^6\\), and \\(3^{15} \\simeq 14348907\\). Example 4.10 (Two-variable numerical integration using the midpoint rule) Use the midpoint rule to approximate \\[ I = \\int_0^1 \\int_0^1 \\exp(x_1 + x_2) \\text{d}x_1 \\text{d}x_2 \\] with 10 nodes for each variable, and estimate its relative absolute error. The following sets \\(d\\) and finds the integral’s true value, true. &gt; d &lt;- 2 &gt; true &lt;- (exp(1) - 1)^d We want \\(N = 10\\) nodes per variable &gt; N &lt;- 10 and then we’ll use the following nodes for \\(x_1\\) and \\(x_2\\) on \\([0, 1]\\) &gt; x1 &lt;- x2 &lt;- (1:N - .5) / N The approximation to the integral is given by \\[ \\hat I = \\sum_{i = 1}^N\\sum_{j = 1}^N w_{ij}f(x_{1i}, x_{2j}) \\] where \\(w_{ij} = N^{-d}\\) for \\(i, j = 1, \\ldots, N\\). This can can be evaluated in R with &gt; midpoint &lt;- 0 &gt; w &lt;- 1/(N^d) &gt; for (i in 1:N) for (j in 1:N) + midpoint &lt;- midpoint + w * exp(x1[i] + x2[j]) where midpoint calculates \\(\\hat I\\) above. The following give the true integral, alongside its midpoint-rule based estimate, and the absolute relative error of the estimate, rel.err &gt; c(true = true, midpoint = midpoint, rel.err = abs(midpoint - true) / true) ## true midpoint rel.err ## 2.9524924420 2.9500332614 0.0008329168 We see that the estimate is reasonably accurate, but we have had to evaluate \\(f(x_1, x_2)\\) 100 times. Example 4.11 (Two-variable numerical integration using Gaussian quadrature) Use Gaussian quadrature to approximate \\(I\\) from Example 4.10 with 4 nodes for each variable, and estimate its relative absolute error. We can take d and true from Example 4.10. Then we calculate \\(\\hat I\\) as above, but using the integration nodes and weights of Gaussian quadrature. &gt; N &lt;- 4 &gt; xw1 &lt;- xw2 &lt;- pracma::gaussLegendre(N, 0, 1) &gt; gq &lt;- 0 &gt; for (i in 1:N) for (j in 1:N) + gq &lt;- gq + xw1$w[i] * xw2$w[j] * exp(xw1$x[i] + xw2$x[j]) &gt; gq ## [1] 2.952492 We again see below that Gaussian quadrature gives an incredibly accurate estimate &gt; c(true = true, guass_quad = gq, rel.err = abs(gq - true) / true) ## true guass_quad rel.err ## 2.952492e+00 2.952492e+00 1.085930e-09 yet now we’ve only had to evaluate \\(f(x_1, x_2)\\) 16 times. Example 4.12 (Five-variable numerical integration using Gaussian quadrature) Use the Gaussian quadrature to approximate \\[ I = \\int_0^1 \\ldots \\int_0^1 \\exp(\\sum_{i = 1}^5 x_i) \\text{d}x_1 \\ldots \\text{d}x_5 \\] with 4 nodes for each variable, and estimate its relative absolute error. We can proceed as in Example 4.11 by storing \\(d\\) as d and \\(I\\) as true. &gt; d &lt;- 5 &gt; true &lt;- (exp(1) - 1)^d Now we’ll need a new tactic, because forming xw1, xw2, …, xw5 will be rather laborious. Having the same nodes and weights for each variable, i.e. for \\(x_1\\), \\(x_2\\), , \\(x_5\\), simplifies the following code a bit. We’ll start by setting \\(n\\) and getting the quadrature nodes and weights, which will be repeated for each variable. &gt; N &lt;- 4 &gt; xw &lt;- pracma::gaussLegendre(N, 0, 1) Then we want to put together all the combinations of the nodes, X, and weights, W, that will be used. &gt; xxww &lt;- lapply(1:d, function(i) xw) &gt; X &lt;- expand.grid(lapply(xxww, &#39;[[&#39;, 1)) &gt; W &lt;- expand.grid(lapply(xxww, &#39;[[&#39;, 2)) We then want multiply all the combinations of weights, for which would could use apply(..., 1, prod), but instead we can use rowSums() in the following way &gt; w &lt;- exp(rowSums(log(W))) We then calculate \\(\\hat I\\) and store this as gq &gt; gq &lt;- sum(w * exp(rowSums(X))) and see that we still get an excellent approximation to \\(I\\) &gt; c(true = true, gauss_quad = gq, rel.err = abs(gq - true) / true) ## true gauss_quad rel.err ## 1.497863e+01 1.497863e+01 2.714825e-09 but have now evaluated \\(\\exp(\\sum_{j = 1}^5 x_i)\\) 1024 times. 4.4 Laplace’s method 4.4.1 An aside on Taylor series in one dimension In MTH1002 you met Taylor series expansions. The expansion is at the centre of many useful statistical approximations. Theorem 4.1 (Taylor series in one dimension) Consider a function \\(f(x)\\) in the region of a point \\(x_0\\) that is infinitely differentiable. Then \\[ f(x) = f(x_0) + \\dfrac{f&#39;(x_0)}{1!}(x - x_0) + \\dfrac{f&#39;&#39;(x_0)}{2!}(x - x_0)^2 + \\dfrac{f^{(3)}(x_0)}{3!}(x - x_0)^3 + \\ldots \\] where \\(f&#39;(x)\\) and \\(f&#39;&#39;(x)\\) denote the first and second derivatives of \\(f(x)\\) w.r.t. \\(x\\), respectively, and \\(f^{(n)}(x)\\) denotes the \\(n\\)th derivative, for \\(n = 3, 4, 5, \\ldots\\). From a statistical perspective we’re often just looking to approximate \\(f(x)\\) up to second-order terms, and hence we may work with the truncated expansion \\[\\begin{equation} f(x) \\simeq f(x_0) + (x - x_0)f&#39;(x_0) + \\dfrac{1}{2}(x - x_0)^2 f&#39;&#39;(x_0). \\tag{4.2} \\end{equation}\\] 4.4.2 Definition Definition 4.6 (Laplace's method in one dimension) Consider the integral \\[\\begin{equation} I_n = \\int_{-\\infty}^{\\infty} \\text{e}^{-nf(x)} \\text{d}x, \\tag{4.3} \\end{equation}\\] where \\(f(x)\\) is a convex function with a minimum at \\(x = \\tilde x\\), so that \\(f&#39;(\\tilde x) = 0\\). The integral can be approximated, using Laplace’s method, as \\[\\begin{equation} I_n \\simeq \\text{e}^{-n[f(\\tilde x)]} \\sqrt{\\dfrac{2 \\pi}{nf&#39;&#39;(\\tilde x)}}, \\tag{4.4} \\end{equation}\\] which is also sometimes referred to as the Laplace approximation8. Remark. Laplace’s method can be derived from the truncated Taylor expansion of Equation (4.2). If we expand \\(f(x)\\) about its global maximum, which we’ll denote by \\(\\tilde x\\), then \\[ f(x) \\simeq f(\\tilde x) + \\dfrac{1}{2}(x - \\tilde x)^2 f&#39;&#39;(\\tilde x), \\] since \\(f&#39;(\\tilde x) = 0\\). Substituting this in we get \\[\\begin{align*} I_n &amp;\\simeq \\int_{-\\infty}^{\\infty} \\exp\\left\\{-n\\left[f(\\tilde x) + \\dfrac{1}{2}(x - \\tilde x)^2 f&#39;&#39;(\\tilde x)\\right]\\right\\} \\text{d}x\\\\ &amp;= \\text{e}^{-n[f(\\tilde x)]} \\int_{-\\infty}^{\\infty} \\exp\\left\\{-\\dfrac{n(x - \\tilde x)^2 f&#39;&#39;(\\tilde x)}{2}\\right\\} \\text{d}x. \\end{align*}\\] We recognise \\(-n(x - \\tilde x)^2 f&#39;&#39;(\\tilde x) / 2\\) as the exponential part of the Normal(\\(\\tilde x\\), \\(1/nf&#39;&#39;(\\tilde x)\\)) distribution, for which the pdf, \\(\\sqrt{nf&#39;&#39;(\\tilde x) / (2 \\pi)} \\exp\\{- n(x - \\tilde x)^2 f&#39;&#39;(\\tilde x) / 2\\}\\), integrates to unity, and so \\[ \\int_{-\\infty}^\\infty \\exp\\left\\{-\\dfrac{n(x - \\tilde x)^2 f&#39;&#39;(\\tilde x) }{2} \\right\\} \\text{d} x = \\sqrt{\\dfrac{2 \\pi}{nf&#39;&#39;(\\tilde x)}}. \\] Substituting this into \\(I_n\\) above we get \\(I_n \\simeq \\text{e}^{-n[f(\\tilde x)]} \\sqrt{(2 \\pi) / nf&#39;&#39;(\\tilde x)}\\), as stated in Equation (4.4). Knowledge of the derivation of Laplace’s method is beyond the scope of MTH3045. Example 4.13 (Poisson marginal approximation using Laplace's method) Recall Example 4.8. Use Laplace’s method to approximate \\(f(y)\\). Given Equation (4.3) we have \\[ f(y, \\lambda) = \\lambda - y \\log(\\lambda) + \\dfrac{1}{2 \\sigma^2}(\\lambda - \\mu)^2 + \\text{constant}, \\] with \\(n = 1\\), which is based on swapping \\(x\\) for \\(\\lambda\\). Then \\[ f&#39;(y, \\lambda) = 1 - \\dfrac{y}{\\lambda} + \\dfrac{1}{\\sigma^2}(\\lambda - \\mu) \\] and \\[ f&#39;&#39;(y, \\lambda) = \\dfrac{y}{\\lambda^2} + \\dfrac{1}{\\sigma^2}. \\] We’ll first set sigsq. &gt; sigsq &lt;- sigma^2 We can’t find \\(\\tilde \\lambda\\) analytically, and so we’ll find it numerically instead. The following function, tilde_lambda(y, mu, sigsq), gives \\(\\tilde \\lambda\\) given \\(y =\\), y, \\(\\mu =\\) mu and \\(\\sigma^2 =\\) sigsq. &gt; f &lt;- function(lambda, y, mu, sigsq) + - dpois(y, lambda, log = TRUE) - dnorm(lambda, mu, sqrt(sigsq), log = TRUE) &gt; tilde_lambda &lt;- function(y, mu, sigsq) { + optimize(f, c(.1, 20), y = y, mu = mu, sigsq = sigsq)$minimum + } We’ll re-use mu, sigsq and y_vals from Example 4.8. We then want to find \\(\\tilde \\lambda\\) for each \\(y\\) in y_vals, which we’ll store as the vector tilde_lambdas. &gt; tilde_lambdas &lt;- sapply(y_vals, tilde_lambda, mu = mu, sigsq = sigsq) &gt; tilde_lambdas ## [1] 1.000000 3.541378 4.771984 5.720154 6.520803 7.226814 7.865452 ## [8] 8.452984 8.999993 9.513868 10.000009 10.462438 10.904333 11.328202 ## [15] 11.736102 12.129703 12.510409 12.879438 13.237752 13.586233 13.925720 ## [22] 14.256831 14.580111 14.896197 15.205447 15.508331 15.805227 16.096470 ## [29] 16.382372 16.663214 16.939295 We then want to evaluate the approximation to \\(I_n\\) given by Equation (4.4). We’ll start with a function to evaluate \\(f&#39;&#39;(\\tilde \\lambda)\\), which we’ll use for each \\(\\tilde \\lambda\\) in tilde_lambdas. &gt; f2 &lt;- function(lambda, y, mu, sigsq) + y / lambda^2 + 1 / sigsq &gt; f2_lambdas &lt;- f2(tilde_lambdas, y_vals, mu, sigsq) Then we’ll approximate \\(I_n\\) and store this as fhat2. &gt; fhat2 &lt;- sqrt(2 * pi / f2_lambdas) * exp(-f(tilde_lambdas, y_vals, mu, sigsq)) Finally we’ll plot the Laplace approximation to \\(f(y)\\) against \\(y\\) for y_vals, i.e. fhat2, alongside the estimate obtained by Gaussian quadrature in Example 4.8 &gt; matplot(y_vals, cbind(fhat, fhat2), + col = 1:2, lty = 1, type = &#39;l&#39;, xlab = &#39;y&#39;, ylab = &#39;f(y)&#39;) &gt; legend(&#39;topright&#39;, c(&quot;Laplace&#39;s method&quot;, &quot;Gaussian quadrature&quot;), + lty = 1, col = 1:2) and see that the two approaches to approximate \\(f(y)\\) give very similar results. 4.4.3 Laplace’s method for multiple dimensions Above we considered Taylor series for a function of one variable. Laplace’s method extends to higher dimensions, although the one-variable case will only be assessed in MTH3045. Consider the Taylor series for functions of multiple variables, denote \\(f(\\mathbf{x})\\), where \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\). The infinite series notation becomes a bit cumbersome, so we’ll skip to the second-order approximation. \\[ f(\\mathbf{x}) \\simeq f(\\mathbf{x}_0) + (\\mathbf{x} - \\mathbf{x}_0)^\\text{T} \\nabla f(\\mathbf{x}_0) + \\dfrac{1}{2} (\\mathbf{x} - \\mathbf{x}_0)^\\text{T} \\left[\\nabla^2 f(\\mathbf{x}_0)\\right] (\\mathbf{x} - \\mathbf{x}_0). \\] Then consider the integral \\[ I_n = \\int \\text{e}^{-nf(\\mathbf{x})} \\text{d} \\mathbf{x}. \\] Laplace’s method gives that \\[ I_n \\simeq \\left(\\dfrac{2 \\pi}{n}\\right)^{d/2} \\dfrac{\\text{e}^{-nf(\\tilde{\\mathbf{x}})}}{|\\mathbf{H}|^{1/2}}, \\] where \\(\\mathbf{H}\\) is the Hessian matrix of \\(f(\\mathbf{x})\\) evaluated at \\(\\mathbf{x} = \\tilde{\\mathbf{x}}\\), i.e. \\(\\nabla^2 f(\\tilde{\\mathbf{x}})\\). The above approximation results from integrating out the \\(MVN_p(\\tilde{\\mathbf{x}}, \\mathbf{H}^{-1})\\) distribution. This is effectively a multivariate extension to integrating out the \\(N(\\tilde x, 1 / nf&#39;&#39;(\\tilde x))\\), which led to Equation (4.4). 4.5 Monte Carlo integration So far we have considered deterministic approaches to numerical integration; these will always give the same answer. It is worth, however, considering a stochastic approach to integration known as Monte Carlo integration. Now consider \\(\\mathbf{X} = (X_1, \\ldots, X_d)\\) on some finite region \\(\\mathcal{X}\\) with volume \\(V(\\mathcal{X})\\), where, additionally \\(\\mathbf{X}\\) is uniformly distributed on \\(\\mathcal{X}\\). Then \\[\\begin{equation} I_{\\mathcal{X}} = \\int_{\\mathbf{x} \\in \\mathcal{X}} f(\\mathbf{x}) \\text{d} \\mathbf{x} = \\text{E}[f(\\mathbf{X})] V(\\mathcal{X}). \\tag{4.5} \\end{equation}\\] The standard Monte Carlo estimate for \\(I_{\\mathcal{X}}\\) is given by \\[ \\hat{I}_{\\mathcal{X}} \\simeq \\dfrac{V(\\mathcal{X})}{N} \\sum_{i = 1}^N f(\\tilde{\\mathbf{x}}_i) \\] where \\(\\tilde{\\mathbf{x}}_i\\), for \\(i = 1, \\ldots, N\\), are drawn uniformly from \\(\\mathcal{X}\\). We can immediately see that Monte Carlo integration is a powerful tool as, provided we can uniformly generate points on \\(\\mathcal{X}\\), \\(\\tilde{\\mathbf{x}}\\), and evaluate \\(f(\\tilde{\\mathbf{x}})\\), then we can approximate \\(I\\). A natural next question is the accuracy of this approximation and, as \\(\\hat{I}_{\\mathcal{X}}\\) is unbiased its variance \\[ \\text{Var}(\\hat{I}_{\\mathcal{X}}) = \\dfrac{V(\\mathcal{X})^2}{N} \\text{Var}[f(\\mathbf{X})] \\] is key. As \\(\\text{Var}(\\hat{I}_{\\mathcal{X}})\\) decreases linearly with \\(1/N\\), convergence is rather slow. The following example confirms this. Example 4.14 (One-dimensional Monte Carlo integration) Recall the integral \\(\\int_0^1 \\exp(x) \\text{d}x = \\exp(1) - 1 \\simeq 1.7182818\\) from Example 4.5. Use R and Monte Carlo integration to approximate the integral with \\(N = 100\\), \\(1000\\) and \\(10000\\), using the mean of \\(m = 100\\) replicates for each value of \\(N\\) as your approximation. Then compare the relative absolute error for each value of \\(N\\). We’ll start with a function, mc(f, N), which we’ll use to approximate the integral for given \\(N\\) &gt; mc &lt;- function(f, N, a = 0, b = 1, ...) { + x &lt;- runif(N, a, b) + V &lt;- b - a + V * mean(f(x, ...)) + } and will quickly test for \\(N = 100\\) &gt; mc(function(x) exp(x), 100) ## [1] 1.832858 Note above that as the vector x contains random variable, you may get a slightly different result; hence the replicates. Now we’ll perform \\(m = 100\\) replicates for each Monte Carlo sample size \\(N\\). &gt; estimates &lt;- numeric(3) &gt; N_vals &lt;- 10^c(2:4) &gt; m &lt;- 100 &gt; for (i in 1:length(N_vals)) + estimates[i] &lt;- mean(replicate(m, mc(function(x) exp(x), N_vals[i]))) &gt; estimates ## [1] 1.715251 1.719208 1.718545 Finally, we’ll compare these to the true integral, true below, by calculating the relative absolute error, rel_err &gt; true &lt;- exp(1) - 1 &gt; rel_err &lt;- abs(true - estimates) / true &gt; rel_err ## [1] 0.0017637093 0.0005390542 0.0001529152 and see that the higher values of \\(N\\) give more accurate approximations, once we allow for variability in Monte Carlo samples. Example 4.15 (Multi-dimensional Monte Carlo integration) Recall Example 4.12, i.e. \\[ I = \\int_0^1 \\ldots \\int_0^1 \\exp(\\sum_{i = 1}^5 x_i) \\text{d}x_1 \\ldots \\text{d}x_5. \\] Use Monte Carlo integration with Monte Carlo samples of size \\(N = 10, 100, 10^3, 10^4\\) and \\(10^5\\) to estimate \\(I\\) and estimate the relative absolute error for each \\(N\\). The following code approximates the integral for the different values of \\(N\\). ## true monte_carlo rel.err ## [1,] 14.97863 17.62581 0.176730779 ## [2,] 14.97863 13.99063 0.065960649 ## [3,] 14.97863 14.91521 0.004233631 ## [4,] 14.97863 14.92255 0.003743597 ## [5,] 14.97863 15.00143 0.001522209 &gt; d &lt;- 5 &gt; true &lt;- (exp(1) - 1)^d &gt; NN &lt;- 10^c(1:5) &gt; f &lt;- numeric(length(NN)) &gt; for (i in 1:length(NN)) + f[i] &lt;- mean(exp(rowSums(matrix(runif(d * NN[i]), NN[i])))) &gt; cbind(true = true, monte_carlo = f, rel.err = abs(f - true) / true) We see that the relative absolute error decreases as \\(N\\) increases, even with just one replicate. The above integral, \\(I_{\\mathcal{X}}\\) in Equation (4.5), can be seen as a special case of \\[ I_{g(\\mathcal{X})} = \\int_{\\mathbf{x} \\in \\mathcal{X}} f(\\mathbf{x}) g(\\mathbf{x}) \\text{d} \\mathbf{x}, \\] for a pdf \\(g(\\mathbf{x})\\), which can be estimated by Monte Carlo as \\[\\begin{equation} \\hat{I}_{g(\\mathcal{X})} \\simeq \\dfrac{1}{N} \\sum_{i = 1}^N f(\\tilde{\\mathbf{x}}_i), \\tag{4.6} \\end{equation}\\] where \\(\\tilde{\\mathbf{x}}_i\\) are draws from the pdf \\(g(\\mathbf{x})\\). Example 4.16 (Poisson marginal approximation using Monte Carlo integration) Recall Example 4.8. Use Monte Carlo integration with \\(N = 1000\\) to approximate \\(f(y)\\). We need to draw a sample of size \\(N = 1000\\) from \\(f(\\lambda)\\), which corresponds to \\(g(\\mathbf{x})\\) in Equation (4.6). The following code does this, simply by calling rnorm(). We’ll call the sample lambda. &gt; N &lt;- 1e3 &gt; lambda &lt;- rnorm(N, 10, 3) By definition, \\(\\lambda &gt; 0\\). However, &gt; range(lambda) ## [1] -0.5806389 19.1004557 shows that we have some \\(\\lambda &lt; 0\\). We’ll simply set these to zero, and proceed drawing from \\(f(y \\mid \\mu)\\) with rpois(), storing the sample as y. &gt; y &lt;- rpois(N, pmax(0, lambda)) We can estimate \\(f(y)\\) from the proportion of y taking each of y_vals from Example 4.8, which we’ll store as fhat3. &gt; fhat3 &lt;- sapply(y_vals, function(z) mean(y == z)) Finally, we’ll plot the resulting estimate alongside those of Gaussian quadrature and Laplace’s method. &gt; matplot(y_vals, cbind(fhat, fhat2, fhat3), + lty = 1, type = &#39;l&#39;, xlab = &#39;y&#39;, ylab = &#39;f(y)&#39;) &gt; legend(&#39;topright&#39;, c(&quot;Laplace&#39;s method&quot;, &quot;Gaussian quadrature&quot;, &quot;Monte Carlo&quot;), + lty = 1) Remark. The larger the Monte Carlo sample size, the more accurate the approximation. The following repeats Example 4.16 for \\(N=10^4\\), \\(10^5\\) and \\(10^6\\), comparing the results against \\(N = 10^3\\). Although we don’t have the true \\(f(y)\\) against which to compare the Monte Carlo estimates, we are seeing in the Figure below approximations that increase with accuracy as \\(N\\) increases. &gt; NN &lt;- 10^c(3:6) &gt; fhat4 &lt;- sapply(NN, function(N) { + lambda &lt;- rnorm(N, 10, 3) + y &lt;- rpois(N, pmax(0, lambda)) + sapply(y_vals, function(z) mean(y == z)) + } + ) &gt; matplot(y_vals, fhat4, + lty = 1, type = &#39;l&#39;, xlab = &#39;y&#39;, ylab = &#39;f(y)&#39;) &gt; legend(&#39;topright&#39;, paste(&#39;N = 10^&#39;, 3:6, sep = &#39;&#39;), lty = 1) 4.6 Bibliographic notes For details on numerical differentiation see Wood (2015, sec. 5.5.2) or Nocedal and Wright (2006, chap. 8). For details on quadrature see Monahan (2011, chap. 10) or Press et al. (2007, chap. 4). For details on Laplace’s method see Davison (2003, sec. 11.3.1), Wood (2015, sec. 5.3.1) or Monahan (2011, sec. 12.6). For details on Monte Carlo integration see Monahan (2011, chap. 12) or Davison (2003, sec. 3.3). Wikipedia has a useful pages on Gaussian quadrature and on Legendre polynomials, should you want to read more on them.↩︎ Pierre-Simon Laplace (23 March 1749 – 5 March 1827) was a “French mathematician, astronomer, and physicist who was best known for his investigations into the stability of the solar system. Laplace successfully accounted for all the observed deviations of the planets from their theoretical orbits by applying Sir Isaac Newton’s theory of gravitation to the solar system, and he developed a conceptual view of evolutionary change in the structure of the solar system. He also demonstrated the usefulness of probability for interpreting scientific data”, according to Britannica.com. He developed the Laplace transform, in addition to Laplace’s method, and, with Abraham de Moivre, is responsible for the de Moivre–Laplace theorem, which approximates the binomial distribution with a normal distribution.↩︎ "],["optimisation.html", "5 Optimisation 5.1 Root finding 5.2 One-dimensional optimisation in R 5.3 Newton’s method in one-dimension 5.4 Newton’s multi-dimensional method 5.5 Global optimisation 5.6 Bibliographic notes", " 5 Optimisation When fitting a statistical model we often use maximum likelihood. For this we have some likelihood function \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\), data \\(\\mathbf{y} = (y_1, \\ldots, y_n)\\) and unknown but fixed parameters \\(\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_p)\\). We are then required to find \\[ \\hat{\\boldsymbol{\\theta}} = \\arg\\,\\max_{\\boldsymbol{\\theta}} f(\\mathbf{y} \\mid \\boldsymbol{\\theta}). \\] Sometimes, it will be possible to find \\(\\hat{\\boldsymbol{\\theta}}\\) analytically, but sometimes not. The normal linear model is one example of the former, whereas the gamma distribution is an example of the latter. Example 5.1 (Maximum likelihood estimation with the gamma distribution) Consider an independent sample of data \\(\\mathbf{y} = (y_1, \\ldots, y_n)\\), and suppose that these are modelling as Gamma\\((\\alpha, \\beta)\\) realisations, i.e. with pdf \\[ f(y \\mid \\alpha, \\beta) = \\dfrac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha -1}e^{-\\beta y} \\quad {\\text{ for }}y&gt;0 \\] where \\(\\alpha ,\\beta &gt;0\\) and \\(\\Gamma()\\) is the gamma function. The log-likelihood is then \\[ \\log f(\\mathbf{y} \\mid \\alpha, \\beta) = n \\alpha \\log \\beta - n\\log \\Gamma(\\alpha) + (\\alpha - 1) \\sum_{i = 1}^n \\log y_i - \\beta \\sum_{i = 1}^n y_i. \\] We can write its derivatives w.r.t. \\((\\alpha, \\beta)\\) in the vector \\[ \\begin{pmatrix} \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\alpha, \\beta)}{\\partial \\alpha}\\\\[2ex] \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\alpha, \\beta)}{\\partial \\beta} \\end{pmatrix} = \\begin{pmatrix} n \\log \\beta + \\sum_{i = 1}^n \\log y_i\\\\[2ex] n \\alpha / \\beta - \\sum_{i = 1}^n y_i \\end{pmatrix}. \\] Unfortunately, we cannot analytically find both the maximum likelihood estimates, \\((\\hat \\alpha, \\hat \\beta)\\), i.e. we cannot find \\(\\alpha\\) and \\(\\beta\\) simultaneously that satisfy that \\(\\partial \\log f(\\mathbf{y} \\mid \\alpha, \\beta) / \\partial \\alpha\\) and \\(\\partial \\log f(\\mathbf{y} \\mid \\alpha, \\beta) / \\partial \\beta\\) are both zero. Fortunately, we can still find \\((\\hat \\alpha, \\hat \\beta)\\), but just not analytically. When we cannot find \\(\\hat{\\boldsymbol{\\theta}}\\) analytically, our next option is to find it numerically: that is, to adopt some kind of iterative process that we expect will ultimately result in the \\(\\hat{\\boldsymbol{\\theta}}\\) that we want, as opposed to an estimate that we don’t want. Although in maximum likelihood estimation interest lies in finding \\(\\boldsymbol{\\theta}\\) that maximises \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\), it is much more common in mathematics to want to minimise a function. Fortunately, maximising \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) is equivalent to minimising \\(-f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\), i.e. \\(f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\) negated. Therefore, \\[ \\hat{\\boldsymbol{\\theta}} = \\arg\\,\\min_{\\boldsymbol{\\theta}} \\left\\{-f(\\mathbf{y} \\mid \\boldsymbol{\\theta})\\right\\}. \\] So that we can better follow the literature on numerical optimisation, we’ll just consider finding minima. 5.1 Root finding We’ll start this chapter with a brief aside on root finding, because our main concern will be finding values that maximise (or minimise) functions. Consider some function \\(f(x)\\) for \\(x \\in \\mathbb{R}\\) and wanting to find the value of \\(x\\), \\(\\tilde x\\) say, such that \\(f(\\tilde x) = 0\\). Sometimes we can analytically find \\(\\tilde x\\), but sometimes not. We’ll just consider the latter case where we’ll need to find \\(\\tilde x\\) numerically, such as through some iterative process. We won’t go into the details of root-finding algorithms; instead we’ll just look at R’s function uniroot(). This is R’s go-to function for root finding. This chapter will just demonstrate its use by example. Example 5.2 (Root-finding in R) Use uniroot() in R to find the root of \\[ f(x) = (x + 3)(x - 1)^2 \\] i.e. to find \\(\\tilde x\\), where \\(f(\\tilde{x}) = 0\\), given that \\(\\tilde x \\in [-4, 4/3]\\). We’ll start by writing a function to evaluate \\(f()\\), which we’ll call f. &gt; f &lt;- function(x) (x + 3) * (x - 1)^2 Then we’ll call uniroot() &gt; uniroot(f, c(-4, 4.3)) ## $root ## [1] -2.999997 ## ## $f.root ## [1] 4.501378e-05 ## ## $iter ## [1] 7 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 We see that its output includes various details. Most important are root, its estimate of \\(\\tilde x\\), which is \\(\\tilde x \\simeq -2.9999972\\), and f.root, the value of \\(f()\\) at the root, i.e. \\(f(\\tilde x)\\), which is \\(f(\\tilde x) \\simeq 4.5013782\\times 10^{-5}\\) and is sufficiently close to zero that we should be confident that we’ve reached a root. Remark. We can ask uniroot() to extend the search range for the root through its argument extendInt. Options are 'no', 'yes', 'downX' and 'upX', which correspond to not extending the range (the default) or allowing it to be extended to allow upward and downward crossings, just downward or just upward, respectively. (If we want to extend the search interval for the root, extendInt = 'yes' is usually the best option. Otherwise, we need to think about how \\(f()\\) behaves at the roots, i.e. whether it’s increasing or decreasing. See the help file for uniroot() for more details.) If we return to the above example and consider the search range \\([-2, -1]\\) instead, then by issuing &gt; uniroot(f, c(-2, -1), extendInt = &#39;yes&#39;) ## $root ## [1] -2.999991 ## ## $f.root ## [1] 0.0001449862 ## ## $iter ## [1] 12 ## ## $init.it ## [1] 6 ## ## $estim.prec ## [1] 6.103516e-05 we do still find the root, even though it’s outside of our specified range. 5.2 One-dimensional optimisation in R We’ll only briefly look at how we can perform one-dimensional optimisation in R, which is through its optimize() function. As described by its help file, optimize() uses ‘a combination of golden section search and successive parabolic interpolation, and was designed for use with continuous functions’. We can instead use optimize() by calling optim(..., method = 'Brent'). The two are equivalent. By default optim() uses the Nelder-Mead polytope method, which we’ll cover in Section 5.4.7, which doesn’t usually work well in one dimension. The only reason I can see for using optim(..., method = 'Brent') over optimize() is that optim() is R’s preferred numerical optimisation function, and hence users my benefit from familiarity with its output, as opposed to that of optimize(). Example 5.3 (Numerical maximum likelihood estimation) Consider a sample of data \\(y_1, \\ldots, y_n\\) as independent realisations from the \\(\\text{Exp}(\\lambda)\\) distribution with pdf \\[ f(y \\mid \\lambda) = \\lambda \\text{exp}(-\\lambda y) \\hspace{1cm} \\text{for}~y &gt; 0 \\] where \\(\\lambda &gt; 0\\) is an unknown parameter that we want to estimate. Its mle is \\(1 / \\bar y\\), where \\(\\bar y = n^{-1} \\sum_{i = 1}^n y_i\\). Confirm this numerically in R using optimze() by assuming that the sample of data \\[ 0.4, 0.5, 0.8, 1.8, 2.1, 3.7, 8.2, 10.6, 11.6, 12.8 \\] are independent \\(\\text{Exp}(\\lambda)\\) realisations. By default optimize() will find the minimum, so we want to write a function that will evaluate the exponential distribution’s log-likelihood \\[ \\log f(\\mathbf{y} \\mid \\lambda) = n \\log \\lambda - \\lambda \\sum_{i = 1}^n y_i \\] and then negate it. We’ll call this negloglik(lambda, y). &gt; negloglik &lt;- function(lambda, y) { + # Function to evaluate Exp(lambda) neg. log likelihood + # lambda is a scalar + # y can be scalar or vector + # returns a scalar + -n * log(lambda) + lambda * sum(y) + } We then pass this on to optimize() with our sample of data, which we’ll call y. &gt; y &lt;- c(0.4, 0.5, 0.8, 1.8, 2.1, 3.7, 8.2, 10.6, 11.6, 12.8) &gt; optimize(negloglik, lower = .1, upper = 10, y = y) ## $minimum ## [1] 0.1904839 ## ## $objective ## [1] 26.58228 We see that R’s numerical maximum likelihood estimate of \\(\\lambda\\) is 0.1904839, and the true value is \\(1 / 5.25 \\simeq 0.1904762\\); so the two agree to five decimal places. Remark. We can ask optimize() to be more precise through its tol argument, which has default tol = .Machine$double.eps^0.25. Smaller values of tol will give more accurate numerical estimates. Remark. Calling optimise() is equivalent to calling optimize(), for those that don’t like American spellings of English words. 5.3 Newton’s method in one-dimension Recall Theorem 4.1. The second-order approximation, if we swap from \\(x\\) to \\(\\theta\\), \\[ f(\\theta) \\simeq f(\\theta_0) + (\\theta - \\theta_0)f&#39;(\\theta_0) + \\dfrac{1}{2}(\\theta - \\theta_0)^2 f&#39;&#39;(\\theta_0) \\] can be re-written as \\[ f(\\theta + \\delta) \\simeq f(\\theta) + \\delta f&#39;(\\theta) + \\dfrac{1}{2} \\delta^2 f&#39;&#39;(\\theta) \\] for small \\(\\delta\\) if we consider values near \\(\\theta\\) for some twice-differentiable function \\(f()\\). If we’re trying to find \\(\\theta^* = \\theta + \\delta\\) that minimises \\(f(\\theta + \\delta)\\) iteratively, then we want \\(\\theta^*\\) to be an improvement on \\(\\theta\\), i.e. \\(f(\\theta + \\delta) &lt; f(\\theta)\\). The best value of \\(\\theta + \\delta t\\) then therefore minimises \\(f(\\theta) + \\delta f&#39;(\\theta) + \\delta^2 f&#39;&#39;(\\theta) / 2\\), and if we differentiate this w.r.t. \\(\\delta\\) we get \\[ f&#39;(\\theta) = - \\delta f&#39;&#39;(\\theta) \\] so that \\[ \\delta = - \\dfrac{f&#39;(\\theta)}{f&#39;&#39;(\\theta)}. \\] The above result is the basis for Newton’s method whereby, if we assume we have a value of \\(\\theta\\) at iteration \\(i\\), then we update this at the next iteration so that \\[ \\theta_{i + 1} = \\theta_i - \\dfrac{f&#39;(\\theta_i)}{f&#39;&#39;(\\theta_i)}. \\] Remark. For the one dimensional case of Newton’s method, we will refer to \\(p_i = - f&#39;(\\theta_i) / f&#39;&#39;(\\theta_i)\\) as the Newton step. Example 5.4 (Poisson maximum likelihood) Let \\(Y_i\\), \\(i = 1, \\ldots, n\\), denote the numbers of cars passing the front of the Laver building between 9 and 10am on weekdays during term time. Assume that these are independent from one day to the next, and that \\(Y_i \\sim \\text{Poisson}(\\mu)\\), for some unknown \\(\\mu\\). The likelihood for a sample of data is given by \\[ f(\\mathbf{y} \\mid \\mu) = \\prod_{i = 1}^n \\dfrac{\\mu^{y_i}\\text{e}^{-\\mu}}{y_i!} \\] and for which we can write the log-likelihood as \\[ \\log f(\\mathbf{y} \\mid \\mu) = -n \\mu + \\sum_{i = 1}^n y_i \\log(\\mu) - \\sum_{i = 1}^n \\log(y_i!). \\] Of course, we can solve this analytically, which gives a maximum likelihood estimate of \\(\\hat \\mu = \\bar y = \\sum_{i = 1}^n y_i / n\\). Given the counts below \\[ 20, 21, 23, 25, 26, 26, 30, 37, 38, 41, \\] we find that \\(\\hat \\mu = 28.7\\). Use five iterations of Newton’s method to find \\(\\hat \\mu\\) iteratively, starting with \\(\\mu_0 = 28\\), and comment on how many iterations are required to find \\(\\hat \\mu\\) to two decimal places. We’ll start by reading in the data. &gt; y &lt;- c(20, 21, 23, 25, 26, 26, 30, 37, 38, 41) Then we’ll find the first and second derivatives of \\(-\\log f(\\mathbf{y} \\mid \\mu)\\) w.r.t. \\(\\mu\\), which are \\(n - (\\sum_{i = 1}^n y_i) / \\mu\\) and \\((\\sum_{i = 1}^n y_i) / \\mu^2\\), respectively. Then we’ll write functions in R, which we’ll call d1 and d2, to evaluate these analytical derivatives. &gt; d1 &lt;- function(mu, y) { + # Function to first derivative w.r.t. lambda of + # Exp(lambda) neg. log likelihood + # lambda is a scalar + # y can be scalar or vector + # returns a scalar + length(y) - sum(y) / mu + } &gt; &gt; d2 &lt;- function(mu, y) { + # Function to second derivative w.r.t. lambda of + # Exp(lambda) neg. log likelihood + # lambda is a scalar + # y can be scalar or vector + # returns a scalar + sum(y) / mu^2 + } Then we’ll iterate estimates of \\(\\mu\\) using Newton’s method. &gt; mu_i &lt;- numeric(6) &gt; mu_i[1] &lt;- 28 &gt; for (i in 1:(length(mu_i) - 1)) + mu_i[i + 1] &lt;- mu_i[i] - d1(mu_i[i], y) / d2(mu_i[i], y) &gt; mu_i ## [1] 28.00000 28.68293 28.69999 28.70000 28.70000 28.70000 Finally, we’ll compare our estimate from Newton’s method with the true maximum likelihood estimate &gt; mu_its &lt;- min(which(round(abs(mu_i - mu_hat), 2) == 0)) - 1 and find that after 2 iterations our iterative estimate matches the true value of \\(\\hat \\mu\\) to three decimal places. Let’s quickly look at how our iterations have gone. We’ll superimpose them on top of a plot of the negative log-likelihood as vertical, dotted red lines, with the true value of \\(\\hat \\mu\\) shown in green. &gt; mu_seq &lt;- seq(mu_hat - 1, mu_hat + 1, l = 1e2) &gt; f_seq &lt;- sapply(mu_seq, function(z) -sum(log(dpois(y, z)))) &gt; plot(mu_seq, f_seq, type = &quot;l&quot;, + xlab = expression(mu), ylab = expression(-log(f(bold(y) * &quot; | &quot; *mu)))) &gt; abline(v = mu_hat, col = 3, lwd = 2) &gt; abline(v = mu_i[seq_len(mu_its)], col = 2, lty = 2, lwd = 2) We see that Newton’s method has quickly homed in on the true value of \\(\\hat \\mu\\). Although this example is simple, and is one in which Newton’s method will typically perform well, Newton’s method is incredibly powerful, and will often perform well in a wide range of scenarios. Next we’ll consider multidimensional \\(\\boldsymbol{\\theta}\\). 5.4 Newton’s multi-dimensional method 5.4.1 Taylor’s theorem (multivariate) The above extension to Taylor’s theorem in the univariate case applies to the multivariate case, after swapping \\(\\mathbf{x}\\) for \\(\\boldsymbol{\\theta}\\), so that \\[ f(\\mathbf{\\boldsymbol{\\theta}}) \\simeq f(\\boldsymbol{\\theta}_0) + \\left[\\nabla f(\\boldsymbol{\\theta}_0)\\right]^\\text{T} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) + \\dfrac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)^\\text{T} \\left[\\nabla^2 f(\\boldsymbol{\\theta}_0)\\right] (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0) \\] can be re-written as \\[\\begin{equation} f(\\boldsymbol{\\theta} + \\boldsymbol{\\Delta}) \\simeq f(\\boldsymbol{\\theta}) + \\left[\\nabla f(\\boldsymbol{\\theta})\\right]^\\text{T} \\boldsymbol{\\Delta} + \\dfrac{1}{2} \\boldsymbol{\\Delta}^\\text{T} \\left[\\nabla^2 f(\\boldsymbol{\\theta})\\right] \\boldsymbol{\\Delta}. \\tag{5.1} \\end{equation}\\] As similar argument to that above, i.e. finding \\(\\boldsymbol{\\theta} + \\boldsymbol{\\Delta}\\) that minimises equation (5.1), gives \\[ \\boldsymbol{\\theta}_{i + 1} = \\boldsymbol{\\theta}_i - \\left[\\nabla^2 f(\\boldsymbol{\\theta}_i)\\right]^{-1} \\nabla f(\\boldsymbol{\\theta}_i) \\] for which we require that \\(\\nabla^2 f(\\boldsymbol{\\theta}_i)\\) is positive semi-definite in order to ensure that \\(f(\\boldsymbol{\\theta}_{i + 1}) \\leq f(\\boldsymbol{\\theta}_i)\\). Remark. For the multi-dimensional case of Newton’s method, we will refer to \\(\\mathbf{p}_i = - \\left[\\nabla^2 f(\\boldsymbol{\\theta}_i)\\right]^{-1} \\nabla f(\\boldsymbol{\\theta}_i)\\) as the Newton step. Remark. Sometimes it may turn out that \\(\\nabla^2 f(\\boldsymbol{\\theta}_i)\\) is not positive semi-definite. Fortunately, this does not prohibit use of Newton’s method because we can perturb \\(\\nabla^2 f(\\boldsymbol{\\theta}_i)\\) so that it is positive semi-definite, which will then guarantee that \\(f(\\boldsymbol{\\theta}_{i + 1}) \\leq f(\\boldsymbol{\\theta}_i)\\). There are various options for perturbation, but a common choice is to use \\(\\nabla^2 f(\\boldsymbol{\\theta}_i) + \\gamma \\mathbf{I}_p\\), where \\(\\mathbf{I}_p\\) is the \\(p \\times p\\) identity matrix, and we choose \\(\\gamma\\) very small, and sequentially increase its value until \\(\\nabla^2 f(\\boldsymbol{\\theta}_i) + \\gamma \\mathbf{I}_p\\) is positive semi-definite. For example, we might proceed through \\(\\gamma = 10^{-12}, 10^{-10}, \\ldots\\). Example 5.5 (Weibull maximum likelihood: Newton's method) The Weibull distribution is sometimes used to model wind speeds. For a wind speed \\(y\\) its pdf is given by \\[ f(y \\mid \\lambda,k) = \\dfrac{k}{\\lambda}\\left(\\dfrac{y}{\\lambda}\\right)^{k-1}e^{-(y/\\lambda)^{k}} \\quad \\text{for}~y &gt; 0 \\] and where \\(\\lambda, k &gt; 0\\) are its parameters. (Note that this is the scale parameterisation of the Weibull distribution.) For observed wind speeds \\(y_1, \\ldots, y_n\\) its corresponding log-likelihood is therefore \\[ \\log f(\\mathbf{y} \\mid \\lambda, k) = n \\log k - nk \\log \\lambda + (k - 1) \\sum_{i = 1}^n \\log y_i - \\sum_{i = 1}^n \\left(\\frac{y_i}{\\lambda}\\right)^k. \\] To implement Newton’s method, we need to find the first and second derivatives of \\(\\log f(\\mathbf{y} \\mid \\lambda, k)\\) w.r.t. \\(\\lambda\\) and \\(k\\). The first derivatives are \\[ \\begin{pmatrix} \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial \\lambda}\\\\[2ex] \\dfrac{\\partial \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial k} \\end{pmatrix} = \\begin{pmatrix} \\dfrac{k}{\\lambda}\\left(\\sum_{i = 1}^n \\left(\\dfrac{y_i}{\\lambda}\\right)^k - n\\right)\\\\[2ex] \\dfrac{n}{k} - n \\log \\lambda + \\sum_{i = 1}^n \\log y_i - \\sum_{i = 1}^n \\left[\\left(\\dfrac{y_i}{\\lambda}\\right)^k \\log \\left(\\dfrac{y_i}{\\lambda}\\right)\\right] \\end{pmatrix} \\] and the second derivatives are stored in the matrix \\[ \\begin{pmatrix} \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial \\lambda^2} &amp; \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial \\lambda \\partial k}\\\\[2ex] \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial k \\partial \\lambda} &amp; \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial k^2} \\end{pmatrix} \\] where \\[\\begin{align*} \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial \\lambda^2} &amp;= \\dfrac{k}{\\lambda^2}\\left(n - (1 + k) \\sum_{i = 1}^n \\left(\\dfrac{y_i}{\\lambda}\\right)^k\\right) \\\\ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial \\lambda \\partial k} &amp;= \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial k \\partial \\lambda} = \\dfrac{1}{\\lambda} \\left(\\sum_{i = 1}^n \\left(\\dfrac{y_i}{\\lambda}\\right)^k - n + k \\sum_{i = 1}^n \\left[\\left(\\dfrac{y_i}{\\lambda}\\right)^k \\log\\left(\\dfrac{y_i}{\\lambda}\\right)\\right]\\right) \\\\ \\dfrac{\\partial^2 \\log f(\\mathbf{y} \\mid \\lambda, k)}{\\partial k^2} &amp;= -\\dfrac{n}{k^2} - \\sum_{i = 1}^n \\left(\\dfrac{y_i}{\\lambda}\\right)^k \\left[\\log\\left(\\dfrac{y_i}{\\lambda}\\right)\\right]^2. \\end{align*}\\] Consider the following wind speed measurements (in m/s) for the month of March. &gt; y0 &lt;- c(3.52, 1.95, 0.62, 0.02, 5.13, 0.02, 0.01, 0.34, 0.43, 15.5, + 4.99, 6.01, 0.28, 1.83, 0.14, 0.97, 0.22, 0.02, 1.87, 0.13, 0.01, + 4.81, 0.37, 8.61, 3.48, 1.81, 37.21, 1.85, 0.04, 2.32, 1.06) Use five iterations of Newton’s method to estimate \\(\\hat \\lambda\\) and \\(\\hat k\\), assuming the above wind speeds are independent from one day to the next and follow a Weibull distribution. We’ll start by plotting the log-likelihood surface. We wouldn’t normally do this, but it can be useful to quickly judge whether the log-likelihood surface is well-behaved, such as being unimodal and approximately quadratic about its maximum. Figure 5.1: Log-likelihood surface of Weibull distribution model for wind speed data with different \\(\\lambda\\) and \\(k\\) values. Then we’ll write functions weib_d1 and weib_d2 to evaluate the first and second derivatives of the Weibull distribution’s log-likelihood w.r.t. \\(\\lambda\\) and \\(k\\). We’ll create these as functions of pars, the vector of parameters, y the vector of data, and mult, a multiplier of the final log-likelihood, which defaults to 1. Introducing mult makes it much simpler when we later need the negative log-likelihood, as we don’t have to write separate functions. Often when calculating derivatives w.r.t. multiple parameters, we find that calculations are repeated. It is worth avoiding repetition, and instead storing the results of any calculations that are used multiple times as objects. We’ll do this in the next few lines of code, storing the re-used objects as z1, z2, etc. &gt; weib_d1 &lt;- function(pars, y, mult = 1) { + # Function to evaluate first derivative of Weibull log-likelihood + # pars is a vector + # y can be scalar or vector + # mult is a scalar defaulting to 1; so -1 returns neg. gradient + # returns a vector + n &lt;- length(y) + z1 &lt;- y / pars[1] + z2 &lt;- z1^pars[2] + out &lt;- numeric(2) + out[1] &lt;- (sum(z2) - n) * pars[2] / pars[1] # derivative w.r.t. lambda + out[2] &lt;- n * (1 / pars[2] - log(pars[1])) + + sum(log(y)) - sum(z2 * log(z1)) # w.r.t k + mult * out + } &gt; &gt; weib_d2 &lt;- function(pars, y, mult = 1) { + # Function to evaluate second derivative of Weibull log-likelihood + # pars is a vector + # y can be scalar or vector + # mult is a scalar defaulting to 1; so -1 returns neg. Hessian + # returns a matrix + n &lt;- length(y) + z1 &lt;- y / pars[1] + z2 &lt;- z1^pars[2] + z3 &lt;- sum(z2) + z4 &lt;- log(z1) + out &lt;- matrix(0, 2, 2) + out[1, 1] &lt;- (pars[2] / pars[1]^2) * (n - (1 + pars[2]) * z3) # w.r.t. (lambda^2) + out[1, 2] &lt;- out[2, 1] &lt;- (1 / pars[1]) * ((z3 - n) + + pars[2] * sum(z2 * z4)) # w.r.t. (lambda, k) + out[2, 2] &lt;- -n/pars[2]^2 - sum(z2 * z4^2) # w.r.t. k^2 + mult * out + } Next we’ll perform five iterations of Newton’s method, ensuring that mult = -1 so that we negate the log-likelihood in order for minima to be sought, although we see that reasonable convergence is achieved after two or three iterations. &gt; iterations &lt;- 5 &gt; xx &lt;- matrix(0, iterations + 1, 2) &gt; dimnames(xx) &lt;- list(paste(&#39;iter&#39;, 0:iterations), c(&#39;lambda&#39;, &#39;k&#39;)) &gt; xx[1, ] &lt;- c(1.6, 0.6) &gt; for (i in 2:(iterations + 1)) { + gi &lt;- weib_d1(xx[i - 1, ], y0, -1) + Hi &lt;- weib_d2(xx[i - 1, ], y0, -1) + xx[i, ] &lt;- xx[i - 1,] - solve(Hi, gi) + } &gt; xx ## lambda k ## iter 0 1.600000 0.6000000 ## iter 1 1.712945 0.5328618 ## iter 2 1.866832 0.5375491 ## iter 3 1.889573 0.5375304 ## iter 4 1.890069 0.5375279 ## iter 5 1.890069 0.5375279 Finally we can plot the course of the iterations, and see visually that Newton’s method quickly homes in on the negative log-likelihood surface’s minimum. Figure 5.2: Five iterations of Newton’s method to find Weibull maximum likelihood estimates. 5.4.2 Newton’s method in R Above we put together some simple code that implemented Newton’s method. However, there are various ways of performing Newton’s method in R. Here we just take a look at nlminb(), which is described as ‘Unconstrained and box-constrained optimization using PORT routines’. We can use our functions weib_d1 and weib_d2 from earlier for the first and second derivatives of the negative log-likelihood w.r.t. \\(\\lambda\\) and \\(k\\). We now just need a function to evaluate the negative log-likelihood itself. We’ll call this weib_d0. Note, though, that it’s important to ensure that invalid parameters, i.e. \\(\\lambda \\leq 0\\) and/or \\(k \\leq 0\\) are avoided. Below we achieve this by setting the likelihood to be extremely low (\\(10^{-8}\\)) for such parts of parameter space. &gt; weib_d0 &lt;- function(pars, y, mult = 1) { + # Function to evaluate Weibull log-likelihood + # pars is a vector + # y can be scalar or vector + # mult is a scalar defaulting to 1; so -1 returns neg. log likelihood + # returns a scalar + n &lt;- length(y) + if (min(pars) &lt;= 0) { + out &lt;- -1e8 + } else { + out &lt;- n * (log(pars[2]) - pars[2] * log(pars[1])) + + (pars[2] - 1) * sum(log(y)) - sum((y / pars[1])^pars[2]) + } + mult * out + } &gt; nlminb(c(1.6, 0.6), weib_d0, weib_d1, weib_d2, y = y0, mult = -1) ## $par ## [1] 1.8900689 0.5375279 ## ## $objective ## [1] 54.95316 ## ## $convergence ## [1] 0 ## ## $iterations ## [1] 5 ## ## $evaluations ## function gradient ## 6 6 ## ## $message ## [1] &quot;relative convergence (4)&quot; We see that nlminb’s output is a list comprising the parameter estimates (par), the final value of the negative log-likelihood (objective) whether the algorithm has converged (convergence, where 0 indicates successful convergence), the number of iterations before convergence was achieved (iterations), how many times the function and gradient were evaluated (evaluations), and then message provides further details on the type of convergence achieved. 5.4.3 Gradient descent If we consider small \\(\\boldsymbol{\\Delta}\\) in (5.1) then we get the first order approximation \\[\\begin{equation*} f(\\boldsymbol{\\theta} + \\boldsymbol{\\Delta}) \\simeq f(\\boldsymbol{\\theta}) + \\left[\\nabla f(\\boldsymbol{\\theta})\\right]^\\text{T} \\boldsymbol{\\Delta}, \\tag{5.2} \\end{equation*}\\] which is appropriate for small \\(\\boldsymbol{\\Delta}\\). The concept behind gradient descent is simple: we want to minimise \\(\\left[\\nabla f(\\boldsymbol{\\theta})\\right]^\\text{T} \\boldsymbol{\\Delta}\\), which requires that we follow the direction of \\(-\\nabla f(\\boldsymbol{\\theta})\\). To allow for different magnitudes of gradient, we will choose \\[ \\boldsymbol{\\Delta} = - \\dfrac{\\nabla f(\\boldsymbol{\\theta})}{||\\nabla f(\\boldsymbol{\\theta})||}. \\] Now that we know the direction in which we want to head, we need to know how far in that direction we should go. For this we’ll consider some \\(\\alpha &gt; 0\\), so that \\[\\begin{align*} f(\\boldsymbol{\\theta} + \\boldsymbol{\\Delta}) &amp;\\simeq f(\\boldsymbol{\\theta}) - \\alpha \\dfrac{\\left[\\nabla f(\\boldsymbol{\\theta})\\right]^\\text{T} \\left[\\nabla f(\\boldsymbol{\\theta})\\right]}{||\\nabla f(\\boldsymbol{\\theta})||},\\\\ &amp;= f(\\boldsymbol{\\theta}) - \\alpha ||\\nabla f(\\boldsymbol{\\theta})||, \\tag{5.3} \\end{align*}\\] which means that \\(\\boldsymbol{\\Delta} = - \\nabla f(\\boldsymbol{\\theta}) / ||\\nabla f(\\boldsymbol{\\theta})||\\) brings a decrease in \\(f(\\boldsymbol{\\theta} + \\boldsymbol{\\Delta})\\) that’s proportional to \\(||\\nabla f(\\boldsymbol{\\theta})||\\) for \\(\\alpha &gt; 0\\), and is the fastest possible rate at which \\(f(\\boldsymbol{\\theta} + \\boldsymbol{\\Delta})\\) can decrease. Example 5.6 (Weibull maximum likelihood: gradient descent) Repeat Example 5.5 using gradient descent with \\(\\alpha = 0.5\\) and \\(\\alpha = 0.1\\), using 30 iterations for each. Comment on how these compare to each other, and to Newton’s method. &gt; alpha_seq &lt;- c(.5, .1) &gt; iterations &lt;- 30 &gt; for (j in 1:length(alpha_seq)) { + xx2 &lt;- matrix(0, iterations + 1, 2) + dimnames(xx2) &lt;- list(paste(&#39;iter&#39;, 0:iterations), c(&#39;lambda&#39;, &#39;k&#39;)) + xx2[1, ] &lt;- c(1.6, 0.6) + for (i in 2:(iterations + 1)) { + gi &lt;- weib_d1(xx2[i - 1, ], y0, mult = -1) + gi &lt;- gi / crossprod(gi)[1, 1] + xx2[i, ] &lt;- xx2[i - 1,] - alpha_seq[j] * gi + } + print(list(paste(&#39;alpha&#39;, alpha_seq[j], sep = &#39; = &#39;), tail(xx2, 5))) + } ## [[1]] ## [1] &quot;alpha = 0.5&quot; ## ## [[2]] ## lambda k ## iter 26 2.010166 -0.03896600 ## iter 27 2.010166 -0.03960079 ## iter 28 2.010166 -0.04024614 ## iter 29 2.010166 -0.04090225 ## iter 30 2.010166 -0.04156929 ## ## [[1]] ## [1] &quot;alpha = 0.1&quot; ## ## [[2]] ## lambda k ## iter 26 1.771195 0.5611662 ## iter 27 1.772881 0.5440336 ## iter 28 1.780112 0.5015718 ## iter 29 1.780158 0.5164447 ## iter 30 1.781202 0.5447801 Above we give the final five iterations for each value of \\(\\alpha\\), and see that convergence to \\(\\hat \\lambda\\) and \\(\\hat k\\) has not been achieved after 30 iterations for \\(\\alpha = 0.5\\) and for \\(\\alpha = 0.1\\), whereas Newton’s method had essentially converged after four or five iterations. Worse still, if we allowed more iterations, we’d see that both eventually diverge away from \\(\\hat \\lambda\\) and \\(\\hat k\\), as opposed to converging. The undesirable course of these iterations can be seen in the following plot. Figure 5.3: Iterations of the gradient descent algorithm with \\(\\alpha = 0.5\\) and \\(\\alpha = 0.1\\). 5.4.4 Line search Above we see that, for fixed \\(\\alpha\\), gradient descent has diverged, i.e. not homed in on the minimum of \\(f()\\). This often happens with gradient descent. A solution, which also applies to Newton’s method, is to use a line search. Consider Newton’s method and a search direction of \\(\\mathbf{p}_i = - \\left[\\nabla^2 f(\\boldsymbol{\\theta}_i)\\right]^{-1} \\nabla f(\\boldsymbol{\\theta}_i)\\). We want \\(f(\\boldsymbol{\\theta}_i + \\mathbf{p}_i) &lt; f(\\boldsymbol{\\theta}_i)\\) in order for \\(\\boldsymbol{\\theta}_i + \\mathbf{p}_i\\) to be an improvement on \\(\\boldsymbol{\\theta}_i\\). If we employ a line search, we instead consider \\(\\boldsymbol{\\theta}_i + \\alpha \\mathbf{p}_i\\) for some \\(\\alpha &gt; 0\\) and ideally want \\(\\alpha\\) to minimise \\(f(\\boldsymbol{\\theta}_i + \\alpha \\mathbf{p}_i)\\). In practice this can be done informally, through the following process. Choose an initial value for \\(\\alpha\\), \\(\\alpha_0\\), say, and set \\(j = 0\\). Evaluate \\(f(\\boldsymbol{\\theta}_i + \\alpha_j \\mathbf{p}_i)\\). Set \\(j = j + 1\\). Set \\(\\alpha_j = \\rho \\alpha_{j - 1}\\), for \\(0 &lt; \\rho &lt; 1\\). Evaluate \\(f(\\boldsymbol{\\theta}_i + \\alpha_j \\mathbf{p}_i)\\). If \\(f(\\boldsymbol{\\theta}_i + \\alpha_j \\mathbf{p}_i) &lt; f(\\boldsymbol{\\theta}_i + \\alpha_{j - 1} \\mathbf{p}_i)\\), repeat steps 3 to 6 until \\(f(\\boldsymbol{\\theta}_i + \\alpha_j \\mathbf{p}_i) \\geq f(\\boldsymbol{\\theta}_i + \\alpha_{j - 1} \\mathbf{p}_i)\\). Choose \\(\\alpha = \\alpha_{j - 1}\\). We can implement this in R. &gt; line_search &lt;- function(theta, p, f, alpha0 = 1, rho = .5, ...) { + best &lt;- f(theta, ...) + cond &lt;- TRUE + while (cond &amp; alpha0 &gt; .Machine$double.eps) { + prop &lt;- f(theta + alpha0 * p, ...) + cond &lt;- prop &gt;= best + if (!cond) + best &lt;- prop + alpha0 &lt;- alpha0 * rho + } + alpha &lt;- alpha0 / rho + alpha + } Remark. Notice the use of the ... argument here, which passes any extra arguments given to line_search() on to f(), and hence avoids the need to include f()’s arguments in line_search(). This is useful because it makes line_search() applicable to any f(). Example 5.7 (Weibull maximum likelihood: gradient descent with line search) Repeat Example 5.5 using gradient descent but with line search and 200 iterations. The following code repeats Example 5.6 with the addition of line search. &gt; iterations &lt;- 200 &gt; xx2 &lt;- matrix(0, iterations + 1, 2) &gt; dimnames(xx2) = list(paste(&#39;iter&#39;, 0:iterations), c(&#39;lambda&#39;, &#39;k&#39;)) &gt; xx2[1, ] &lt;- c(1.6, 0.6) &gt; for (i in 2:(iterations + 1)) { + gi &lt;- weib_d1(xx2[i - 1, ], y0, mult = -1) + gi &lt;- gi / crossprod(gi)[1, 1] + alpha_i &lt;- line_search(xx2[i - 1, ], -gi, weib_d0, y = y0, mult = -1) + xx2[i, ] &lt;- xx2[i - 1,] - alpha_i * gi + } &gt; tail(xx2, 5) ## lambda k ## iter 196 1.888392 0.5374135 ## iter 197 1.888417 0.5374937 ## iter 198 1.888463 0.5374345 ## iter 199 1.888489 0.5374891 ## iter 200 1.888556 0.5374245 We see that line search does at least bring us convergence of the parameter estimates, but that it’s also very slow, as the following plot shows. Figure 5.4: 200 iterations of the gradient descent algorithm using line search. Remark. Here we’ve adopted an informal approach to line search. A more formal approach is to choose \\(\\alpha\\) so that it satisfies the Wolfe conditions. A step length \\(\\alpha_k\\) is said to satisfy the Wolfe conditions, restricted to the direction \\(\\mathbf{p}_i\\), if the following two inequalities hold: \\[\\begin{align*} \\textbf{i)} &amp; \\quad f(\\boldsymbol{\\theta}_i + \\alpha_i \\mathbf{p}_i) \\leq f(\\boldsymbol{\\theta}_i) + c_1 \\alpha_i \\mathbf{p}_i^{\\mathrm{T}} \\nabla f(\\boldsymbol{\\theta}_i), \\\\[6pt] \\textbf{ii)} &amp; \\quad -\\mathbf{p}_i^{\\mathrm{T}} \\nabla f(\\boldsymbol{\\theta}_i + \\alpha_i \\mathbf{p}_i) \\leq -c_2\\mathbf{p}_i^{\\mathrm{T}} \\nabla f(\\boldsymbol{\\theta}_i), \\end{align*}\\] with \\(0 &lt; c_1 &lt; c_2 &lt; 1\\). \\(c_1\\) is usually chosen to be quite small while \\(c_2\\) is much larger; Nocedal and Wright (2006, sec. 6.1) give example values of \\(c_1 = 10^{-4}\\) and \\(c_2 = 0.9\\) for Newton or quasi-Newton methods. 5.4.5 Quasi-Newton methods Between Newton’s method and steepest descent lie quasi-Newton methods. These essentially employ Newton’s method, but with some approximation to the Hessian matrix. Instead of the search direction used in Newton’s method, \\(\\mathbf{p}_i = - \\left[\\nabla^2 f(\\boldsymbol{\\theta}_i)\\right]^{-1} \\nabla f(\\boldsymbol{\\theta}_i)\\), consider the search direction \\(\\tilde{\\mathbf{p}}_i = - \\mathbf{H}_i^{-1} \\nabla f(\\boldsymbol{\\theta}_i)\\), where \\(\\mathbf{H}_i\\) is an approximation to the Hessian matrix \\(\\nabla^2 f(\\boldsymbol{\\theta}_i)\\) at the \\(i\\)th iteration. We might, for example, want to avoid explicitly calculating \\(\\nabla^2 f(\\boldsymbol{\\theta}_i)\\) because it’s a matrix that’s sufficiently more difficult to calculate than the gradient (e.g. mathematically, or just in terms of time), so that using an approximation to the Hessian matrix (provided it is an adequate approximation) gives a more efficient approach to optimisation than using the Hessian matrix itself. We should typically expect quasi-Newton methods to converge slower than Newton’s method, but provided that convergence isn’t too much slower or less reliable, then we may prefer this over analytically forming the Hessian matrix. In MTH3045 we shall consider the so-called BFGS (shorthand for Broyden–Fletcher–Goldfarb–Shanno) quasi-Newton algorithm. Put simply, at iteration \\(i\\), the BFGS algorithm assumes that \\[ \\nabla^2 f(\\boldsymbol{\\theta}_{i}) \\simeq \\mathbf{H}_{i} = \\mathbf{H}_{i - 1} + \\dfrac{\\mathbf{y}_i \\mathbf{y}_i^{\\mathrm{T}}}{\\mathbf{y}_i^{\\mathrm{T}} \\mathbf{s}_i} - \\dfrac{(\\mathbf{H}_{i - 1})^{-1} \\mathbf{s}_i \\mathbf{s}_i^{\\mathrm{T}} (\\mathbf{H}_{i - 1})^{-T}}{\\mathbf{s}_i^{\\mathrm{T}} (\\mathbf{H}_{i - 1})^{-1} \\mathbf{s}_i}, \\] where \\(\\mathbf{s}_i = \\boldsymbol{\\theta}_{i} - \\boldsymbol{\\theta}_{i - 1}\\) and \\(\\mathbf{y}_i = \\nabla f(\\boldsymbol{\\theta}_{i}) - \\nabla f(\\boldsymbol{\\theta}_{i - 1})\\). Hence the BFGS algorithm uses differences in the gradients of successive iterations to approximate the Hessian matrix. We now note that we use \\(\\mathbf{H}_i\\) in \\(\\mathbf{p}_i = -[\\mathbf{H}_i]^{-1} \\nabla f(\\boldsymbol{\\theta}_i)\\). We can avoid solving this system of linear equations by instead directly obtaining \\([\\mathbf{H}_i]^{-1}\\) through \\[ [\\mathbf{H}_{i}]^{-1} = \\left(\\mathbf{I}_p - \\dfrac{\\mathbf{s}_i \\mathbf{y}_i^\\text{T}}{\\mathbf{y}_i^\\text{T}\\mathbf{s}_i}\\right) [\\mathbf{H}_{i - 1}]^{-1} \\left(\\mathbf{I}_p - \\dfrac{\\mathbf{y}_i \\mathbf{s}_i^\\text{T}}{\\mathbf{s}_i^\\text{T}\\mathbf{y}_i}\\right) + \\dfrac{\\mathbf{s}_i \\mathbf{s}_i^\\text{T}}{\\mathbf{y}_i^\\text{T} \\mathbf{y}_i}. \\] The following R function updates \\([\\mathbf{H}_{i - 1}]^{-1}\\) to \\([\\mathbf{H}_{i}]^{-1}\\) given \\(\\boldsymbol{\\theta}_i\\), \\(\\boldsymbol{\\theta}_{i - 1}\\), \\(\\nabla f(\\boldsymbol{\\theta}_{i - 1})\\) and \\(\\nabla f(\\boldsymbol{\\theta}_i)\\), which are the arguments x0, x1, g0 and g1, respectively. &gt; iH1 &lt;- function(x0, x1, g0, g1, iH0) { + # Function to update Hessian matrix + # x0 and x1 are p-vectors of second to last and last estimates, respectively + # g0 and g1 are p-vectors of second to last and last gradients, respectively + # iH0 is previous estimate of p x p Hessian matrix + # returns a p x p matrix + s0 &lt;- x1 - x0 + y0 &lt;- g1 - g0 + denom &lt;- sum(y0 * s0) + I &lt;- diag(rep(1, 2)) + pre &lt;- I - tcrossprod(s0, y0) / denom + post &lt;- I - tcrossprod(y0, s0) / denom + last &lt;- tcrossprod(s0) / denom + pre %*% iH0 %*% post + last + } Example 5.8 (Weibull maximum likelihood: BFGS) Repeat Example 5.5 using the BFGS method. Comment on how it compares to Newton’s method. The following code implements five iterations of the BFGS method. &gt; iterations &lt;- 5 &gt; xx &lt;- matrix(0, iterations + 1, 2) &gt; dimnames(xx) &lt;- list(paste(&#39;iter&#39;, 0:iterations), c(&#39;lambda&#39;, &#39;k&#39;)) &gt; xx[1, ] &lt;- c(1.6, 0.6) &gt; g &lt;- iH &lt;- list() &gt; for (i in 2:(iterations + 1)) { + g[[i]] &lt;- weib_d1(xx[i - 1, ], y0, mult = -1) + if (sqrt(sum(g[[i]]^2)) &lt; 1e-6) + break + if (i == 2) { + iH[[i]] &lt;- diag(1, 2) + } else { + iH[[i]] &lt;- iH1(xx[i - 2, ], xx[i - 1, ], g[[i - 1]], g[[i]], iH[[i - 1]]) + } + search_dir &lt;- -(iH[[i]] %*% g[[i]]) + alpha &lt;- line_search(xx[i - 1, ], search_dir, weib_d0, y = y0, mult = -1) + xx[i, ] &lt;- xx[i - 1,] + alpha * search_dir + } Our estimates at each iteration are &gt; xx ## lambda k ## iter 0 1.600000 0.6000000 ## iter 1 1.615241 0.4736904 ## iter 2 2.097571 0.5295654 ## iter 3 1.918661 0.5356343 ## iter 4 1.881726 0.5375145 ## iter 5 1.890167 0.5374986 and we see that we need two more iterations than Newton’s method to reach convergence to three decimal places. Finally, we’ll plot the course of the iterations Figure 5.5: Five iterations of the MFGS quasi-Newton method. and see that we’ve taken a slightly less direct route to the minimum than Newton’s method. 5.4.6 Quasi-Newton methods in R There are various options for performing quasi-Newton methods in R. For these, we just need to supply the function to be minimised and its gradient. The first option is to use nlminb() again: if we don’t supply a function to evaluate the Hessian, then nlminb() uses a quasi-Newton approach. The alternative – and possibly preferred – option is to use optim() with option method = 'BFGS'. The following two lines of code repeat the above example that used Newton’s method in R, using nlminb() and then optim(). &gt; nlminb(c(1.6, 0.6), weib_d0, weib_d1, y = y0, mult = -1) ## $par ## [1] 1.8900689 0.5375279 ## ## $objective ## [1] 54.95316 ## ## $convergence ## [1] 0 ## ## $iterations ## [1] 7 ## ## $evaluations ## function gradient ## 9 8 ## ## $message ## [1] &quot;relative convergence (4)&quot; &gt; optim(c(1.6, 0.6), weib_d0, weib_d1, y = y0, mult = -1, method = &#39;BFGS&#39;) ## $par ## [1] 1.8900632 0.5375283 ## ## $value ## [1] 54.95316 ## ## $counts ## function gradient ## 14 6 ## ## $convergence ## [1] 0 ## ## $message ## NULL We see that nlminb() and optim() have essentially given the same value of par, i.e. for \\(\\hat \\lambda\\) and \\(\\hat k\\), which is reassuring. Note that nlminb() has used fewer function evaluations than optim(). We won’t go into the details of the cause of this, but it is worth noting that the functions use different stopping criteria, and slightly different variants of the BFGS algorithm. Note also that nlminb() has used three more function evaluations with the BFGS method than with Newton’s method, which is typically the case, and reflects the improved convergence achieved by using the actual Hessian matrix with Newton’s method, as opposed to the approximation that’s used with the BFGS approach. 5.4.7 Nelder-Mead polytope method So far we have considered derivative-based optimisation algorithms. When we cannot analytically calculate derivatives, we can use finite-difference approximations. However, sometimes we may want to find the minimum point of a surface that is not particularly smooth. Then derivative information may not be helpful. Instead, we might want an algorithm that explores a surface differently. The Nelder-Mead polytope algorithm is one such approach (Nelder and Mead (1965)). In fact, it is R’s default if we use optim(), i.e. if we don’t supply method = '...'. For the Nelder-Mead algorithm, consider \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^p\\). The algorithm starts with \\(p + 1\\) test points, \\(\\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_{p + 1}\\), which we call vertices, and then proceeds as follows. Compute the order statistics of \\(f(\\boldsymbol{\\theta}_1), \\ldots, f(\\boldsymbol{\\theta}_{p + 1})\\) vertices, i.e. find the order \\[ f(\\boldsymbol{\\theta}^{(1)}) \\leq f(\\boldsymbol{\\theta}^{(2)}) \\leq \\ldots \\leq f(\\boldsymbol{\\theta}^{(p + 1)}) \\] and check whether the termination criteria have been met (which are given later). If not, proceed to Step 2. Calculate the centroid, \\(\\boldsymbol{\\theta}_o\\), of \\(\\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_p\\), i.e. omitting \\(\\boldsymbol{\\theta}_{p + 1}\\), because \\(\\boldsymbol{\\theta}_{p + 1}\\) is the worst vertex. Reflection. Compute the reflected point \\(\\boldsymbol{\\theta}_r = \\boldsymbol{\\theta}_o + \\alpha (\\boldsymbol{\\theta}_o - \\boldsymbol{\\theta}_{p + 1})\\). If \\(f(\\boldsymbol{\\theta}_1) \\leq f(\\boldsymbol{\\theta}_r) &lt; f(\\boldsymbol{\\theta}_p)\\), replace \\(\\boldsymbol{\\theta}_{p + 1}\\) with \\(\\boldsymbol{\\theta}_r\\) and return to Step 1. Otherwise, proceed to Step 4. Expansion. If \\(f(\\boldsymbol{\\theta}_r) &lt; f(\\boldsymbol{\\theta}_1)\\), i.e. is the best point so far, compute the expanded point \\(\\boldsymbol{\\theta}_e = \\boldsymbol{\\theta}_o + \\gamma (\\boldsymbol{\\theta}_r - \\boldsymbol{\\theta}_o)\\) for \\(\\gamma &gt; 1\\). If \\(f(\\boldsymbol{\\theta}_e) &lt; f(\\boldsymbol{\\theta}_r)\\), replace \\(\\boldsymbol{\\theta}_{p + 1}\\) with \\(\\boldsymbol{\\theta}_r\\) and return to Step 1. Otherwise, replace \\(\\boldsymbol{\\theta}_{p + 1}\\) with \\(\\boldsymbol{\\theta}_r\\) and return to Step 1. Contraction. Now \\(f(\\boldsymbol{\\theta}_r) \\geq f(\\boldsymbol{\\theta}_p)\\). Compute the contracted point \\(\\boldsymbol{\\theta}_c = \\boldsymbol{\\theta}_o + \\rho (\\boldsymbol{\\theta}_{p + 1} - \\boldsymbol{\\theta}_o)\\) for \\(0 &lt; \\rho \\leq 0.5\\). If \\(f(\\boldsymbol{\\theta}_c) &lt; f(\\boldsymbol{\\theta}_{p + 1})\\) then replace \\(\\boldsymbol{\\theta}_{p + 1}\\) with \\(\\boldsymbol{\\theta}_c\\) and return to Step 1. Otherwise proceed to Step 6. Shrink. For \\(i = 2, \\ldots, p + 1\\) set \\(\\boldsymbol{\\theta}_i = \\boldsymbol{\\theta}_1 + \\sigma (\\boldsymbol{\\theta}_i - \\boldsymbol{\\theta}_1)\\) and return to Step 1. Often the values \\(\\alpha = 1\\), \\(\\gamma = 2\\), \\(\\rho = 0.5\\) and \\(\\sigma = 0.5\\) are used. In Step 1, termination is defined in terms of tolerances. The main criterion is for \\(f(\\boldsymbol{\\theta}_{p + 1}) - f(\\boldsymbol{\\theta}_1)\\) to be sufficiently small, so that \\(f(\\boldsymbol{\\theta}_i)\\) for \\(i = 1, \\ldots, p + 1\\) are close together for all \\(\\theta_i\\), and hence we are hoping that all the \\(\\boldsymbol{\\theta}_i\\) values are in the region of the true minimum. We won’t code the Nelder-Mead algorithm in R; instead we’ll just optim() and look what it does, by requesting a trace with control = list(trace = TRUE). Example 5.9 (Weibull maximum likelihood: Nelder-Mead) Use the Nelder-Mead method and R’s optim() function to find the maximumum likelihood estimates of the Weibull distribution for the data of Example 5.5. We’ve got everything we need for this example, i.e. the data, which we stored earlier as y0, and a function to evaluate the Weibull distribution’s log-likelihood, weib_d0(). So we just pass these to optim(), ensuring that mult = -1, so that we find the negative log-likelihood’s minimum. &gt; fit_nelder &lt;- optim(c(1.6, 0.6), weib_d0, y = y0, mult = -1, control = list(trace = TRUE)) ## Nelder-Mead direct search function minimizer ## function value for initial parameters = 55.677933 ## Scaled convergence tolerance is 8.29666e-07 ## Stepsize computed as 0.160000 ## BUILD 3 60.970799 55.438511 ## LO-REDUCTION 5 55.677933 55.000530 ## REFLECTION 7 55.438511 54.983053 ## HI-REDUCTION 9 55.040918 54.983053 ## HI-REDUCTION 11 55.000530 54.969138 ## REFLECTION 13 54.983053 54.957102 ## HI-REDUCTION 15 54.969138 54.957102 ## HI-REDUCTION 17 54.958325 54.955086 ## HI-REDUCTION 19 54.957102 54.954109 ## HI-REDUCTION 21 54.955086 54.953597 ## HI-REDUCTION 23 54.954109 54.953417 ## LO-REDUCTION 25 54.953597 54.953331 ## HI-REDUCTION 27 54.953417 54.953219 ## HI-REDUCTION 29 54.953331 54.953189 ## LO-REDUCTION 31 54.953219 54.953168 ## HI-REDUCTION 33 54.953189 54.953165 ## HI-REDUCTION 35 54.953168 54.953162 ## HI-REDUCTION 37 54.953165 54.953160 ## HI-REDUCTION 39 54.953162 54.953159 ## HI-REDUCTION 41 54.953160 54.953159 ## Exiting from Nelder Mead minimizer ## 43 function evaluations used The above output is telling us what optim() is doing as it’s going along. Specifically, LO-REDUCTION corresponds to a contraction, HI-REDUCTION to an expansion and REFLECTION to a reflection. On this occasion, there was no need to shrink the simplex, which is identified as SHRINK. &gt; fit_nelder ## $par ## [1] 1.8900970 0.5374706 ## ## $value ## [1] 54.95316 ## ## $counts ## function gradient ## 43 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL What optim() returns is essentially the same as we saw before for the BFGS method, i.e. roughly the same parameter estimates and function minimum, except that now there are no gradient evaluations. Remark. The Nelder-Mead method is usually great if you’re quickly looking to find a function’s minimum, provided it’s a relatively low-dimensional function, and the function is fairly quick to evaluate. This is probably why it’s R’s default. However, it can be very slow for high-dimensional optimisation problems, and is also typically less reliable than gradient-based methods, except for strangely-behaved surfaces. 5.5 Global optimisation So far we have considered optimisation algorithms that usually home in on local minima, given starting points. For multimodal surfaces, this can be undesirable. Here we consider approaches to global optimisation, which are designed to find the overall minimum. 5.5.1 Stochastic optimisation The optimisation algorithms that have been introduced so far have been deterministic: given a set of starting values, they will always return the same final value. Stochastic optimisation algorithms go from one point to another probabilistically, and so will go through different sets of parameters. We should expect them to converge to the same final value, though. 5.5.2 Simulated annealing Simulated annealing gains its name from the physical annealing process, which is a heat treatment that alters the physical and sometimes chemical properties of a material to increase its ductility and reduce its hardness, making it more workable. You can of course read more about it on Wikipedia9. Consider a current parameter value \\(\\boldsymbol{\\theta}\\) and some function we seek to minimise \\(f()\\). For example, \\(f()\\) might be a negated log-likelihood. The key to simulated annealing is that a random point is proposed, \\(\\boldsymbol{\\theta}^*\\), which is drawn from some proposal density \\(q(\\boldsymbol{\\theta}^* \\mid \\boldsymbol{\\theta})\\), and so depends on the current value \\(\\boldsymbol{\\theta}\\). The proposal density \\(q()\\) is chosen to be symmetric, but otherwise its choice is arbitrary. The main aspect of simulated annealing is to work with the function \\[ \\pi_T(\\boldsymbol{\\theta}) = \\exp\\{-f(\\boldsymbol{\\theta}) / T\\} \\] for some temperature \\(T\\). We note that as \\(T \\searrow 0\\), \\(\\pi_T(\\boldsymbol{\\theta}) \\to \\exp\\{-f(\\boldsymbol{\\theta})\\}\\) Put algorithmically, simulated annealing works as follows. Propose \\(\\boldsymbol{\\theta}^*\\) from \\(q(\\boldsymbol{\\theta}^* \\mid \\boldsymbol{\\theta})\\). Generate \\(U \\sim \\text{Uniform}[0, 1]\\). Calculate \\[\\begin{align*} \\alpha(\\boldsymbol{\\theta}^* \\mid \\boldsymbol{\\theta}) &amp;= \\min\\left\\{\\dfrac{\\exp\\left[-f(\\boldsymbol{\\theta}^*) / T\\right]}{\\exp\\left[-f(\\boldsymbol{\\theta}) / T\\right]}, 1\\right\\}\\\\ &amp;= \\min\\left(\\exp\\left\\{-\\left[f(\\boldsymbol{\\theta}^*) - f(\\boldsymbol{\\theta})\\right] / T\\right\\}, 1\\right). \\end{align*}\\] Accept \\(\\boldsymbol{\\theta}^*\\) if \\(\\alpha(\\boldsymbol{\\theta}^* \\mid \\boldsymbol{\\theta}) &gt; U\\); otherwise keep \\(\\boldsymbol{\\theta}\\). Decrease \\(T\\). It is worth noting that steps 1 to 4 implement a special case of the Metropolis-Hastings algorithm for symmetric \\(q()\\). This algorithm is heavily used in statistics, especially Bayesian statistics, to sample from posterior densities that do not have or have unwieldy closed forms, typically as part of Markov chain Monte Carlo (MCMC) sampling. Remark. R’s default is to use a Gaussian distribution for \\(q()\\) and the temperature at iteration \\(i\\), \\(T_i\\), is chosen according to \\(T_i = T_1 / \\log\\{t_{\\max} \\lfloor(i-1) / t_{\\max} \\rfloor + \\exp(1)\\}\\) with \\(T_1 = t_{\\max} = 10\\) the default values. (#exm:weib:sann) (Weibull maximum likelihood: Simulated annealing) Write a function to update the simulated annealing temperature according to R’s rule and another function to generate Gaussian proposals with standard deviation 0.1. Then use simulated annealing to repeat Example 5.5 with \\(N = 1000\\) iterations and plot \\(\\lambda_i\\) and \\(k_i\\) at each iteration using initial temperatures of \\(T_1 = 10\\), 1 and 0.1. The following function, update_T(), updates the temperature according to R’s rule. &gt; update_T &lt;- function(i, t0 = 10, t1 = 10) { + # Function to update simulated annealing temperature + # i is an integer giving the current iteration + # t0 is a scalar giving the initial temperature + # t1 is a integer giving how many iterations of each temperature to use + # returns a scalar + t0 / log(((i - 1) %/% t1) * t1 + exp(1)) + } Then the following function, q_fn(), generates Gaussian proposals with standard deviation 0.1. &gt; q_fn &lt;- function(x) { + # Function to generate Gaussian proposals with standard deviation 0.1 + # x is the Gaussian mean as either a scalar or vector + # returns a scalar or vector, as x + rnorm(length(x), x, .1) + } The following function can perform simulated annealing. You won’t be asked to write such a function for MTH3045, but it’s illuminating to see how such a function can be written. &gt; sa &lt;- function(p0, h, N, q, T1, ...) { + # Function to perform simulated annealing + # p0 p-vector of initial parameters + # h() function to be minimised + # N number of iterations + # q proposal function + # T1 initial temperature + # ... arguments to pass to h() + # returns p x N matrix of parameter estimates at each iteration + out &lt;- matrix(0, N, length(p0)) # matrix to store estimates at each iteration + out[1, ] &lt;- p0 # fill first row with initial parameter estimates + for (i in 2:N) { # N iterations + T &lt;- update_T(i, T1) # update temperature + U &lt;- runif(1) # generate U + out[i, ] &lt;- out[i - 1,] # carry over last parameter estimate, by default + proposal &lt;- q(out[i - 1,]) # generate proposal + if (min(proposal) &gt;= 0) { # ensure proposal valid + h0 &lt;- h(out[i - 1, ], ...) # evaluate h for current theta + h1 &lt;- h(proposal, ...) # evaluate h for proposed theta + alpha &lt;- min(exp(- (h1 - h0) / T), 1) # calculate M-H ratio + if (alpha &gt;= U) # accept if ratio sufficiently high + out[i, ] &lt;- proposal # swap last with proposal + } + } + out # return all parameter estimates + } Then we’ll specify out initial temperatures as T_vals, and loop over these with sa(), plotting the resulting parameter estimates for each temperature and each iteration. &gt; # values to use for initial temperature &gt; T_vals &lt;- c(10, 1, .1) &gt; # loop over values, and plot &gt; for (j in 1:length(T_vals)) { + T1 &lt;- T_vals[j] + sa_result &lt;- sa(c(1.6, 0.6), weib_d0, 1e3, q_fn, T1, y = y0, mult = -1) + if (j == 1) { + plot(sa_result, col = j, pch = 20, xlab = &#39;lambda&#39;, ylab = &#39;k&#39;) + } else { + points(sa_result, col = j, pch = 20) + } + } &gt; legend(&#39;bottomright&#39;, pch = 20, col = 1:length(T_vals), + legend = paste(&quot;t_0 =&quot;, T_vals), bg = &#39;white&#39;) Figure 5.6: Iterations of simulated annealing for different temperatures. Note that lower initial temperatures bring smaller clouds of parameter estimates. 5.5.3 Simulated annealing in R Simulated annealing is built in to R’s optim() function and requires method = 'SANN'. Example 5.10 (Weibull maximum likelihood: Simulated annealing with \\texttt{optim()}) Repeat Example ?? using R’s optim() function to perform simulated annealing. Report the best value of the objective function every 100 iterations. We simply need to issue the following. &gt; optim(c(1.6, 0.6), weib_d0, y = y0, mult = -1, method = &#39;SANN&#39;, + control = list(trace = 1, REPORT = 10, maxit = 1e3)) ## sann objective function values ## initial value 55.677933 ## iter 100 value 55.201564 ## iter 200 value 55.201564 ## iter 300 value 55.129664 ## iter 400 value 55.129664 ## iter 500 value 55.057112 ## iter 600 value 55.057112 ## iter 700 value 54.966591 ## iter 800 value 54.966591 ## iter 900 value 54.953947 ## iter 999 value 54.953947 ## final value 54.953947 ## sann stopped after 999 iterations ## $par ## [1] 1.8687326 0.5383946 ## ## $value ## [1] 54.95395 ## ## $counts ## function gradient ## 1000 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL We now see that the parameter estimates are some way off those from the deterministic methods. If we allowed simulated annealing more iterations, it would gradually get closer to the true minimum. Note that the control$REPORT argument specifies the frequency in terms of how often the temperature changes; so R reports the status of the optimiser each (control$tmax * control$REPORT)th iteration, noting that control$tmax defaults to 10. Remark. It’s sometimes a good tactic to use simulated annealing to get close to the minimum, and then to employ one of the previously discussed deterministic optimisation methods to get a more accurate estimate. This is especially useful if we’re unsure whether we’re starting off with sensible initial parameter estimates. 5.6 Bibliographic notes By far the best resource for reading up on numerical optimisation is Nocedal and Wright (2006). In particular, Chapter 3 covers Newton’s method and line search; Chapter 6 covers quasi-Newton methods; and Chapter 8 covers derivative-free optimisation, including the Nelder-Method method in Section 9.5. Optimisation is also covered in Monahan (2011, chap. 8) and in Wood (2015, sec. 5.1). Simulated annealing is covered in Press et al. (2007, sec. 10.12). Root-finding is covered in Monahan (2011, sec. 8.3) and Press et al. (2007, chap. 9). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
